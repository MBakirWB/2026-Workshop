{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1po8aF76bGdV"
      },
      "source": [
        "# W&B End-to-End Workshop: Aquatic Species Classification (AQUA20)\n",
        "\n",
        "## From First Baseline to Production-Ready Model with Full MLOps\n",
        "\n",
        "---\n",
        "\n",
        "### Workshop Story\n",
        "\n",
        "**Scenario:** You're part of a marine biology AI research team building an image classifier to identify aquatic species from underwater photographs. The goal is to help marine researchers automatically catalog and monitor marine biodiversity.\n",
        "\n",
        "**Your journey:**\n",
        "1. **Train a baseline model** with proper experiment tracking and visual diagnostics\n",
        "2. **Package the dataset and model** as versioned artifacts with rich metadata\n",
        "3. **Register the model** and promote it through stages (candidate â†’ staging â†’ production)\n",
        "4. **Create CI/CD-style automation** for evaluation and promotion workflows\n",
        "\n",
        "---\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "| Section | Topic | Key Skills |\n",
        "|---------|-------|------------|\n",
        "| 1 | Setup | Environment configuration, W&B login, config objects |\n",
        "| 2 | Data & Artifacts | HuggingFace datasets, **raw artifact upload**, **split artifacts with lineage** |\n",
        "| 3 | Experiment Tracking | Run initialization, logging profiles, step alignment, resume |\n",
        "| 4 | Model Training | PyTorch training, mixed precision, checkpoint logging |\n",
        "| 5 | Visual Logging | Images, tables, confusion matrices, PR curves |\n",
        "| 6 | Model Artifacts | Model artifact with lineage, use_artifact consumer workflow |\n",
        "| 7 | Registry | Collections, linking, promotion workflow |\n",
        "| 8 | CI/CD Automation | Validation jobs, gating, programmatic promotion |\n",
        "| 9 | Offline Mode | WANDB_MODE, syncing runs |\n",
        "| 10 | Wrap-up | Recap, next steps |\n",
        "\n",
        "**Key Change:** Artifacts are introduced in Section 2 - the very first thing you do after loading data is log it as an artifact!\n",
        "\n",
        "---\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Basic Python and PyTorch knowledge\n",
        "- Google Colab with T4 GPU runtime\n",
        "- W&B account (free at wandb.ai)\n",
        "\n",
        "**Runtime:** ~30-45 minutes with fast_run=True, ~60-90 minutes with full training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smfMb_XbbGdW"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. Setup\n",
        "\n",
        "Let's install dependencies, authenticate with W&B, and configure our experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2j3keQQbGdW"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install wandb datasets transformers timm -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyuupFAHce2q"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqdSLo5BbGdW"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import random\n",
        "import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import GradScaler\n",
        "import wandb\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Workshop utilities (handles ML boilerplate)\n",
        "from workshop_utils import (\n",
        "    CLASS_NAMES, NUM_CLASSES, DEVICE,\n",
        "    set_seed, get_transforms,\n",
        "    create_model, count_parameters,\n",
        "    train_one_epoch, evaluate,\n",
        "    generate_run_name, update_best_metric,\n",
        "    AquaticDataset,\n",
        "    # Visualization helpers\n",
        "    create_prediction_images, create_predictions_table,\n",
        ")\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"W&B: {wandb.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NNN1uWabGdW"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# W&B CONFIGURATION - UPDATE THESE FOR YOUR ENVIRONMENT\n",
        "# ============================================================================\n",
        "\n",
        "WANDB_ENTITY = \"m-bakir\"              # Your W&B username or team\n",
        "WANDB_PROJECT = \"SIE-Workshop-2026\"    # Project name\n",
        "# WANDB_ORG = \"Moe\"                     # Organization (for registry)\n",
        "\n",
        "# Authenticate with W&B\n",
        "WANDB_HOST = \"https://mbakir.wandb.io/\" #@param\n",
        "wandb.login(host= WANDB_HOST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5-fMsLAbGdW"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MASTER CONFIGURATION\n",
        "# ============================================================================\n",
        "# Single config object for all hyperparameters and run metadata\n",
        "# This is the \"source of truth\" for reproducibility\n",
        "\n",
        "# ============================================================================\n",
        "# WORKSHOP: Random Model Assignment\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "# TRAINING CONFIGURATION\n",
        "# ============================================================================\n",
        "# Random model assignment - creates diverse runs for W&B comparison!\n",
        "WORKSHOP_MODELS = [\"resnet50\", \"efficientnet_b0\"]\n",
        "ASSIGNED_MODEL = random.choice(WORKSHOP_MODELS)\n",
        "print(f\"Your assigned model: {ASSIGNED_MODEL}\")\n",
        "\n",
        "# Training config - these get logged to W&B automatically\n",
        "CONFIG = {\n",
        "    \"model_name\": ASSIGNED_MODEL,\n",
        "    \"num_classes\": NUM_CLASSES,\n",
        "    \"epochs\": 3,              # Quick training for workshop\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"image_size\": 224,\n",
        "    \"max_samples\": 1000,      # Subset for fast iteration\n",
        "    \"use_amp\": True,\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "# Set reproducibility\n",
        "set_seed(CONFIG[\"seed\"])\n",
        "print(f\"\\nConfig: {CONFIG}\")\n",
        "\n",
        "# Device is already set by workshop_utils\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy9fJN_pbGdW"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Data Preparation\n",
        "\n",
        "The AQUA20 dataset has been **pre-prepared and logged as W&B artifacts**. This is a common pattern in production ML:\n",
        "- Data engineering teams prepare and version datasets\n",
        "- ML engineers consume versioned datasets for training\n",
        "\n",
        "**Available artifacts:**\n",
        "- `aqua-train:v0` - Training split (~6,500 images)\n",
        "- `aqua-val:v0` - Validation split (~800 images)  \n",
        "- `aqua-test:v0` - Test split (~800 images)\n",
        "\n",
        "The `workshop_utils.py` file handles all ML boilerplate (transforms, dataset class, etc.).\n",
        "\n",
        "**Key W&B concept:** The artifact download happens in the training run using `use_artifact()` - this creates **lineage** showing exactly which dataset version was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJzD7T-cbGdX"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# W&B ARTIFACT PATHS (Pre-prepared datasets)\n",
        "# ============================================================================\n",
        "ARTIFACT_PROJECT = f\"{WANDB_ENTITY}/{WANDB_PROJECT}\"\n",
        "TRAIN_ARTIFACT = f\"{ARTIFACT_PROJECT}/aqua-train:v0\"\n",
        "VAL_ARTIFACT = f\"{ARTIFACT_PROJECT}/aqua-val:v0\"\n",
        "TEST_ARTIFACT = f\"{ARTIFACT_PROJECT}/aqua-test:v0\"\n",
        "\n",
        "print(f\"Artifacts: {TRAIN_ARTIFACT}, {VAL_ARTIFACT}, {TEST_ARTIFACT}\")\n",
        "print(f\"Classes: {NUM_CLASSES} marine species\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id9m0-v9bGdX"
      },
      "outputs": [],
      "source": [
        "# Transforms are provided by workshop_utils.get_transforms()\n",
        "# Includes underwater-optimized augmentations: color jitter, rotation, etc.\n",
        "print(f\"Image transforms ready (size: {CONFIG['image_size']}x{CONFIG['image_size']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ8OvFBKbGdX"
      },
      "outputs": [],
      "source": [
        "# AquaticDataset is provided by workshop_utils\n",
        "# Loads images from class folders in downloaded artifacts\n",
        "print(\"AquaticDataset class ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7geOwXwqbGdX"
      },
      "outputs": [],
      "source": [
        "# Section 2 complete - transforms and Dataset class are ready\n",
        "# \n",
        "# The actual data loading happens in the TRAINING RUN (Section 4):\n",
        "# 1. wandb.init() starts the training run\n",
        "# 2. use_artifact() downloads train + val datasets (creates lineage!)\n",
        "# 3. Datasets and DataLoaders are created\n",
        "# 4. Training proceeds with metrics logged to W&B\n",
        "#\n",
        "# This pattern ensures your training run shows exactly which \n",
        "# dataset versions were used - critical for reproducibility!\n",
        "\n",
        "print(\"âœ… Section 2 complete!\")\n",
        "print(\"   - Transforms defined\")\n",
        "print(\"   - AquaticDataset class ready\")\n",
        "print(\"   - Data will be downloaded in training run via use_artifact()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pj9JvEWbGdX"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DATA EXPLORATION (Pre-prepared in W&B)\n",
        "# ============================================================================\n",
        "# An EDA table has been prepared for you in the W&B project!\n",
        "# \n",
        "# Go to: https://wandb.ai/m-bakir/SIE-Workshop-2026\n",
        "# Look for the \"dataset-eda-exploration\" run\n",
        "#\n",
        "# The table includes:\n",
        "# - Sample images from each class\n",
        "# - Image statistics (brightness, contrast, color channels)\n",
        "# - Class distribution visualization\n",
        "#\n",
        "# Try these explorations:\n",
        "# 1. Group by \"class\" to see samples per species\n",
        "# 2. Sort by \"brightness\" to find dark/light images\n",
        "# 3. Filter by \"blue_ratio\" to see underwater color cast\n",
        "# 4. Compare \"contrast\" across different marine species\n",
        "\n",
        "print(\"ðŸ“Š Data exploration table available in W&B!\")\n",
        "print(\"   Project: m-bakir/SIE-Workshop-2026\")\n",
        "print(\"   Run: dataset-eda-exploration\")\n",
        "print(\"\\n   Skip to Section 4 to start training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cuwYT0obGdX"
      },
      "source": [
        "### Why This Matters: Data Sanity Checks TODO Remove?\n",
        "\n",
        "Before training any model, always verify:\n",
        "1. **Images load correctly** - No corrupted files\n",
        "2. **Labels are valid** - Within expected range\n",
        "3. **Class distribution** - Check for imbalance (common in marine datasets)\n",
        "4. **Visual inspection** - Do underwater images look right?\n",
        "\n",
        "Logging this to W&B creates a permanent record of your data exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeMH7w5dbGdX"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Experiment Tracking Fundamentals\n",
        "\n",
        "Now let's set up proper experiment tracking with W&B best practices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "japhdR8wbGdX"
      },
      "outputs": [],
      "source": [
        "# generate_run_name() creates descriptive names from config\n",
        "# Format: {model}-lr{learning_rate}-bs{batch_size}-ep{epochs}\n",
        "print(f\"Example run name: {generate_run_name(CONFIG)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwueIRgebGdX"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "\n",
        "# Best Practice 3: Step Alignment\n",
        "def log_training_step(metrics: Dict[str, float], step: int, run):\n",
        "    \"\"\"Log training metrics with proper step alignment.\"\"\"\n",
        "    wandb.log(metrics, step=step)\n",
        "\n",
        "# Best Practice 4: Summary vs History\n",
        "def update_summary(run, key: str, value: float, mode: str = \"max\"):\n",
        "    \"\"\"Update run summary with best value.\"\"\"\n",
        "    current_best = run.summary.get(key)\n",
        "\n",
        "    if current_best is None:\n",
        "        run.summary[key] = value\n",
        "    elif mode == \"max\" and value > current_best:\n",
        "        run.summary[key] = value\n",
        "    elif mode == \"min\" and value < current_best:\n",
        "        run.summary[key] = value\n",
        "\n",
        "print(\"âœ… Experiment tracking utilities defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZrWCHqtbGdX"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Model Training\n",
        "\n",
        "Train a baseline model with proper experiment tracking.\n",
        "\n",
        "**W&B Features in this section:**\n",
        "- `tags` and `group` - Organize runs for filtering and comparison\n",
        "- `notes` - Quick description visible in run overview  \n",
        "- `define_metric()` - Set epoch as x-axis for cleaner charts\n",
        "- `use_artifact()` - Download data with automatic lineage tracking\n",
        "- `wandb.alert()` - Get notified when validation improves\n",
        "- **System Metrics** - GPU/CPU usage logged automatically (check System tab in W&B UI!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7O4UEYsbGdX"
      },
      "outputs": [],
      "source": [
        "# Create model (uses timm library under the hood)\n",
        "model = create_model(CONFIG[\"model_name\"], NUM_CLASSES, pretrained=True)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "total_params, trainable_params = count_parameters(model)\n",
        "print(f\"Model: {CONFIG['model_name']}\")\n",
        "print(f\"  Parameters: {total_params:,} ({trainable_params:,} trainable)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpamVc2SbGdX"
      },
      "outputs": [],
      "source": [
        "# train_one_epoch() and evaluate() are imported from workshop_utils\n",
        "# They handle mixed precision training and W&B logging automatically\n",
        "print(\"Training utilities ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8tS50AmbGdY"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MAIN TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "run_name = generate_run_name(CONFIG)\n",
        "\n",
        "run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    name=run_name,\n",
        "    job_type=\"training\",\n",
        "    # GROUP: Links related runs together (all resnet50 runs grouped)\n",
        "    group=CONFIG[\"model_name\"],\n",
        "    # TAGS: Filterable labels - find runs by model, dataset, experiment type\n",
        "    tags=[\n",
        "        \"aqua20\",                      # Dataset\n",
        "        \"baseline\",                    # Experiment type\n",
        "        CONFIG[\"model_name\"],          # Model architecture\n",
        "        \"workshop-uk-2026\",            # Workshop identifier\n",
        "        f\"epochs-{CONFIG['epochs']}\",  # Hyperparameter tag\n",
        "    ],\n",
        "    # NOTES: Quick description (visible in run overview)\n",
        "    notes=f\"Workshop training: {CONFIG['model_name']} on AQUA20. \"\n",
        "          f\"Epochs: {CONFIG['epochs']}, LR: {CONFIG['learning_rate']}, BS: {CONFIG['batch_size']}\",\n",
        "    config=CONFIG\n",
        ")\n",
        "\n",
        "# DEFINE_METRIC: Set \"epoch\" as x-axis for cleaner charts\n",
        "wandb.define_metric(\"epoch\")\n",
        "wandb.define_metric(\"train/*\", step_metric=\"epoch\")\n",
        "wandb.define_metric(\"val/*\", step_metric=\"epoch\")\n",
        "wandb.define_metric(\"learning_rate\", step_metric=\"epoch\")\n",
        "\n",
        "print(f\"\\nStarting training run: {run_name}\")\n",
        "print(f\"   View at: {run.url}\")\n",
        "print(f\"   Tags: {run.tags}\")\n",
        "\n",
        "# ============================================================================\n",
        "# DOWNLOAD DATASET ARTIFACTS (Creates Lineage!)\n",
        "# ============================================================================\n",
        "# Using use_artifact() within this run creates lineage showing this training\n",
        "# depends on these specific dataset versions - critical for reproducibility!\n",
        "\n",
        "print(\"\\nðŸ“¦ Downloading dataset artifacts...\")\n",
        "\n",
        "# Download training data\n",
        "train_artifact = run.use_artifact(TRAIN_ARTIFACT, type='dataset')\n",
        "train_dir = train_artifact.download()\n",
        "print(f\"   âœ… Training data: {train_artifact.name}:{train_artifact.version}\")\n",
        "\n",
        "# Download validation data\n",
        "val_artifact = run.use_artifact(VAL_ARTIFACT, type='dataset')\n",
        "val_dir = val_artifact.download()\n",
        "print(f\"   âœ… Validation data: {val_artifact.name}:{val_artifact.version}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE DATASETS AND DATALOADERS\n",
        "# ============================================================================\n",
        "print(\"\\nðŸ“Š Creating datasets...\")\n",
        "\n",
        "train_dataset = AquaticDataset(\n",
        "    train_dir, \n",
        "    transform=get_transforms(CONFIG[\"image_size\"], is_training=True), \n",
        "    class_names=CLASS_NAMES,\n",
        "    max_samples=CONFIG.get(\"max_samples\")  # Limit for fast_run mode\n",
        ")\n",
        "\n",
        "val_dataset = AquaticDataset(\n",
        "    val_dir,\n",
        "    transform=get_transforms(CONFIG[\"image_size\"], is_training=False),\n",
        "    class_names=CLASS_NAMES\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"   Train: {len(train_dataset)} samples, {len(train_loader)} batches\")\n",
        "print(f\"   Val: {len(val_dataset)} samples, {len(val_loader)} batches\")\n",
        "\n",
        "# Download test data for final evaluation\n",
        "test_artifact = run.use_artifact(TEST_ARTIFACT, type='dataset')\n",
        "test_dir = test_artifact.download()\n",
        "print(f\"   Test data: {test_artifact.name}:{test_artifact.version}\")\n",
        "\n",
        "test_dataset = AquaticDataset(\n",
        "    test_dir,\n",
        "    transform=get_transforms(CONFIG[\"image_size\"], is_training=False),\n",
        "    class_names=CLASS_NAMES\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"   Test: {len(test_dataset)} samples, {len(test_loader)} batches\")\n",
        "\n",
        "# ============================================================================\n",
        "# SETUP TRAINING COMPONENTS\n",
        "# ============================================================================\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=CONFIG[\"learning_rate\"],\n",
        "    weight_decay=CONFIG[\"weight_decay\"]\n",
        ")\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=CONFIG[\"epochs\"]\n",
        ")\n",
        "scaler = GradScaler(enabled=CONFIG[\"use_amp\"])\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_model_path = None\n",
        "\n",
        "for epoch in range(CONFIG[\"epochs\"]):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model, train_loader, criterion, optimizer, scaler, DEVICE,\n",
        "        epoch, log_interval=1, run=run  # Log every batch\n",
        "    )\n",
        "\n",
        "    val_loss, val_acc, val_preds, val_labels, val_probs = evaluate(\n",
        "        model, val_loader, criterion, DEVICE, desc=f\"Epoch {epoch+1} [Val]\"\n",
        "    )\n",
        "\n",
        "    scheduler.step()\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "    epoch_metrics = {\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train/loss\": train_loss,\n",
        "        \"train/accuracy\": train_acc,\n",
        "        \"val/loss\": val_loss,\n",
        "        \"val/accuracy\": val_acc,\n",
        "        \"learning_rate\": current_lr,\n",
        "    }\n",
        "    wandb.log(epoch_metrics)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Results:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "    print(f\"  Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_path = f\"best_model_epoch{epoch+1}.pth\"\n",
        "        torch.save({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"val_acc\": val_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"config\": CONFIG\n",
        "        }, best_model_path)\n",
        "        print(f\"  New best model saved! Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # ALERT: Send notification when new best model is found\n",
        "        wandb.alert(\n",
        "            title=\"New Best Model!\",\n",
        "            text=f\"Validation accuracy improved to {val_acc:.2f}% at epoch {epoch+1}\",\n",
        "            level=wandb.AlertLevel.INFO\n",
        "        )\n",
        "\n",
        "        run.summary[\"best_val_accuracy\"] = val_acc\n",
        "        run.summary[\"best_val_loss\"] = val_loss\n",
        "        run.summary[\"best_epoch\"] = epoch + 1\n",
        "\n",
        "    # ========================================================================\n",
        "    # LOG MODEL CHECKPOINT AS ARTIFACT (creates versioned history)\n",
        "    # ========================================================================\n",
        "    checkpoint_artifact = wandb.Artifact(\n",
        "        name=f\"model-{CONFIG['model_name']}\",\n",
        "        type=\"model\",\n",
        "        metadata={\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"train_loss\": train_loss,\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Save checkpoint to temp file\n",
        "    checkpoint_path = f\"checkpoint_epoch{epoch+1}.pth\"\n",
        "    torch.save({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"config\": CONFIG\n",
        "    }, checkpoint_path)\n",
        "    checkpoint_artifact.add_file(checkpoint_path)\n",
        "    \n",
        "    # Aliases for easy reference\n",
        "    aliases = [f\"epoch_{epoch+1}\"]\n",
        "    if val_acc >= best_val_acc:\n",
        "        aliases.append(\"best\")\n",
        "    if epoch == CONFIG[\"epochs\"] - 1:\n",
        "        aliases.append(\"latest\")\n",
        "    \n",
        "    run.log_artifact(checkpoint_artifact, aliases=aliases)\n",
        "    print(f\"  Logged checkpoint artifact (aliases: {aliases})\")\n",
        "\n",
        "print(f\"\\n\")\n",
        "print(f\"Training complete!\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-c6AUV3bGdY"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Visual Logging (Media)\n",
        "\n",
        "Log rich visual diagnostics: underwater images, predictions, confusion matrices, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlK3IzY3bGdY"
      },
      "outputs": [],
      "source": [
        "# Final evaluation on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "test_loss, test_acc, test_preds, test_labels, test_probs = evaluate(\n",
        "    model, test_loader, criterion, DEVICE, desc=\"Test Evaluation\"\n",
        ")\n",
        "\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"  Test Loss: {test_loss:.4f}\")\n",
        "print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "wandb.log({\n",
        "    \"test/loss\": test_loss,\n",
        "    \"test/accuracy\": test_acc\n",
        "})\n",
        "\n",
        "run.summary[\"test_accuracy\"] = test_acc\n",
        "run.summary[\"test_loss\"] = test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8Z3zNwWbGdY"
      },
      "outputs": [],
      "source": [
        "# Log prediction samples with images and confidence scores\n",
        "# create_prediction_images() handles the boilerplate\n",
        "\n",
        "prediction_images = create_prediction_images(\n",
        "    test_dataset, test_preds, test_probs, CLASS_NAMES, n_samples=16\n",
        ")\n",
        "\n",
        "# Log to W&B - images appear in Media tab\n",
        "wandb.log({\"predictions/samples\": prediction_images})\n",
        "\n",
        "print(f\"Logged {len(prediction_images)} prediction samples to W&B\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrdc-XvdbGdY"
      },
      "outputs": [],
      "source": [
        "# Create W&B Table with predictions for detailed analysis\n",
        "# Includes per-class confidence scores for histogram visualization\n",
        "\n",
        "predictions_table = create_predictions_table(\n",
        "    test_dataset, test_preds, test_probs, CLASS_NAMES, n_samples=100\n",
        ")\n",
        "\n",
        "# Log to W&B - table appears in Tables tab\n",
        "wandb.log({\"predictions/analysis_table\": predictions_table})\n",
        "\n",
        "print(f\"Logged predictions table with {len(CLASS_NAMES)} class score columns\")\n",
        "print(\"  In W&B UI try:\")\n",
        "print(\"  - Group by 'truth' to see recall per class\")\n",
        "print(\"  - Group by 'guess' to see precision per class\")\n",
        "print(\"  - Filter: row['truth'] != row['guess'] to find errors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCuEgY99bGdY"
      },
      "outputs": [],
      "source": [
        "# Log per-class metrics\n",
        "# Get unique classes that appear in the data (handles fast_run with subset)\n",
        "unique_classes = sorted(set(test_labels) | set(test_preds))\n",
        "\n",
        "precision, recall, f1, support = precision_recall_fscore_support(\n",
        "    test_labels, test_preds, labels=unique_classes, average=None, zero_division=0\n",
        ")\n",
        "\n",
        "metrics_table = wandb.Table(\n",
        "    columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"]\n",
        ")\n",
        "\n",
        "for i, class_idx in enumerate(unique_classes):\n",
        "    class_name = CLASS_NAMES[class_idx] if class_idx < len(CLASS_NAMES) else f\"Class_{class_idx}\"\n",
        "    metrics_table.add_data(\n",
        "        class_name,\n",
        "        round(precision[i], 4),\n",
        "        round(recall[i], 4),\n",
        "        round(f1[i], 4),\n",
        "        int(support[i])\n",
        "    )\n",
        "\n",
        "wandb.log({\"evaluation/per_class_metrics\": metrics_table})\n",
        "\n",
        "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "    test_labels, test_preds, average='macro'\n",
        ")\n",
        "\n",
        "wandb.log({\n",
        "    \"evaluation/precision_macro\": precision_macro,\n",
        "    \"evaluation/recall_macro\": recall_macro,\n",
        "    \"evaluation/f1_macro\": f1_macro\n",
        "})\n",
        "\n",
        "run.summary[\"precision_macro\"] = precision_macro\n",
        "run.summary[\"recall_macro\"] = recall_macro\n",
        "run.summary[\"f1_macro\"] = f1_macro\n",
        "\n",
        "print(\"\\nPer-class metrics logged.\")\n",
        "print(f\"Macro F1-Score: {f1_macro:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF6Z9u6UbGdY"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Model Artifacts with Lineage\n",
        "\n",
        "We've already logged our **data artifacts** in Section 2:\n",
        "- âœ… Raw dataset artifact (`aqua20-raw`)\n",
        "- âœ… Split artifacts with lineage (`aqua-train`, `aqua-val`, `aqua-test`)\n",
        "\n",
        "Now it's time to complete the lineage by **logging the trained model as an artifact** that references the training data!\n",
        "\n",
        "### Complete Lineage Chain\n",
        "\n",
        "```\n",
        "Raw Dataset â†’ Train/Val/Test Splits â†’ Model\n",
        "```\n",
        "\n",
        "By using `use_artifact()` to consume the split artifacts when logging our model, we create a complete audit trail showing exactly which data was used to train this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j20RWM9UbGdY"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MODEL ARTIFACT - Complete the Lineage Chain\n",
        "# ============================================================================\n",
        "# Start a run that consumes the split artifacts and logs the trained model.\n",
        "# This creates the final lineage: Splits â†’ Model\n",
        "\n",
        "model_artifact_run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    name=\"aqua20-model-artifact-logging\",\n",
        "    job_type=\"model-logging\",\n",
        "    tags=[\"aqua20\", \"model-artifact\", \"baseline\"],\n",
        "    notes=\"Log trained model artifact with lineage to training data splits\"\n",
        ")\n",
        "\n",
        "# Reference the SAME artifacts used during training (creates proper lineage)\n",
        "train_artifact_ref = model_artifact_run.use_artifact(TRAIN_ARTIFACT, type='dataset')\n",
        "val_artifact_ref = model_artifact_run.use_artifact(VAL_ARTIFACT, type='dataset')\n",
        "\n",
        "print(f\"ðŸ“¦ Model trained using:\")\n",
        "print(f\"   Train artifact: {train_artifact_ref.name}:{train_artifact_ref.version}\")\n",
        "print(f\"   Val artifact: {val_artifact_ref.name}:{val_artifact_ref.version}\")\n",
        "\n",
        "# Prepare model info\n",
        "model_info = {\n",
        "    \"architecture\": CONFIG[\"model_name\"],\n",
        "    \"num_classes\": NUM_CLASSES,\n",
        "    \"class_names\": CLASS_NAMES,\n",
        "    \"input_size\": CONFIG[\"image_size\"],\n",
        "    \"pretrained\": True,\n",
        "    \"total_params\": total_params,\n",
        "    \"trainable_params\": trainable_params,\n",
        "    \"training_config\": {\n",
        "        \"epochs\": CONFIG[\"epochs\"],\n",
        "        \"batch_size\": CONFIG[\"batch_size\"],\n",
        "        \"learning_rate\": CONFIG[\"learning_rate\"],\n",
        "        \"optimizer\": \"AdamW\"\n",
        "    },\n",
        "    \"metrics\": {\n",
        "        \"best_val_accuracy\": best_val_acc,\n",
        "        \"test_accuracy\": test_acc,\n",
        "        \"test_loss\": test_loss,\n",
        "        \"f1_macro\": f1_macro\n",
        "    },\n",
        "    \"data_artifacts\": {\n",
        "        \"train\": f\"{train_artifact_ref.name}:{train_artifact_ref.version}\",\n",
        "        \"val\": f\"{val_artifact_ref.name}:{val_artifact_ref.version}\"\n",
        "    },\n",
        "    \"created_at\": datetime.now().isoformat(),\n",
        "    \"training_run_id\": model_artifact_run.id\n",
        "}\n",
        "\n",
        "with open(\"artifacts/model_info.json\", \"w\") as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "\n",
        "import shutil\n",
        "shutil.copy(best_model_path, \"artifacts/model.pth\")\n",
        "\n",
        "# Create and log the model artifact\n",
        "model_artifact = wandb.Artifact(\n",
        "    name=\"aqua20-species-classifier\",\n",
        "    type=\"model\",\n",
        "    description=f\"Aquatic Species classifier ({CONFIG['model_name']}) trained on AQUA20 dataset\",\n",
        "    metadata={\n",
        "        \"architecture\": CONFIG[\"model_name\"],\n",
        "        \"num_classes\": NUM_CLASSES,\n",
        "        \"input_size\": CONFIG[\"image_size\"],\n",
        "        \"final_val_accuracy\": best_val_acc,\n",
        "        \"final_test_accuracy\": test_acc,\n",
        "        \"f1_macro\": f1_macro,\n",
        "        \"framework\": \"pytorch\",\n",
        "        \"domain\": \"marine_biology\",\n",
        "        \"train_artifact\": f\"{train_artifact_ref.name}:{train_artifact_ref.version}\",\n",
        "        \"val_artifact\": f\"{val_artifact_ref.name}:{val_artifact_ref.version}\"\n",
        "    }\n",
        ")\n",
        "\n",
        "model_artifact.add_file(\"artifacts/model.pth\")\n",
        "model_artifact.add_file(\"artifacts/model_info.json\")\n",
        "\n",
        "model_artifact_run.log_artifact(model_artifact, aliases=[\"latest\", \"candidate\", \"baseline\"], tags=[\"baseline-model\", \"aqua20\", \"marine-biology\"])\n",
        "\n",
        "print(\"\\nâœ… Model artifact logged with lineage!\")\n",
        "print(f\"   Name: aqua20-species-classifier\")\n",
        "print(f\"   Type: model\")\n",
        "print(f\"   Test Accuracy: {test_acc:.2f}%\")\n",
        "print(f\"   Aliases: latest, candidate, baseline\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# COMPLETE ARTIFACT LINEAGE SUMMARY\n",
        "# ============================================================================\n",
        "# At this point, we've created the following artifact lineage:\n",
        "#\n",
        "# Section 2 (Data):\n",
        "#   1. Raw Dataset Artifact (aqua20-raw)\n",
        "#      â†“ use_artifact() in data-splitting run\n",
        "#   2. Split Artifacts (aqua-train, aqua-val, aqua-test)\n",
        "#\n",
        "# Section 6 (Model Artifacts):\n",
        "#      â†“ use_artifact() in model-artifact-logging run\n",
        "#   3. Model Artifact (aqua20-species-classifier)\n",
        "#\n",
        "# Check the W&B UI â†’ Artifacts â†’ Lineage tab to visualize this!\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"COMPLETE ARTIFACT LINEAGE\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                    â”‚  aqua20-raw  â”‚\n",
        "                    â”‚   (source-of-truth)  â”‚\n",
        "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                               â”‚\n",
        "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "              â–¼                â–¼                â–¼\n",
        "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "       â”‚ train-splitâ”‚  â”‚ val-split  â”‚  â”‚ test-split â”‚\n",
        "       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "              â”‚               â”‚\n",
        "              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                      â–¼\n",
        "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "          â”‚ aqua20-species-     â”‚\n",
        "          â”‚ classifier (model)  â”‚\n",
        "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\"\"\")\n",
        "print(\"âœ… View lineage at: W&B UI â†’ Artifacts â†’ Select any artifact â†’ Lineage tab\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rLLd46KbGdY"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DEMONSTRATE use_artifact: LOAD MODEL FROM ARTIFACT\n",
        "# ============================================================================\n",
        "# This shows how another run can consume your model artifact.\n",
        "# This is useful for evaluation, deployment, or further fine-tuning.\n",
        "\n",
        "eval_run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    name=\"aqua20-model-evaluation-from-artifact\",\n",
        "    job_type=\"evaluation\",\n",
        "    tags=[\"aqua20\", \"evaluation\", \"artifact-consumption\"],\n",
        "    notes=\"Evaluation job that loads model from artifact\"\n",
        ")\n",
        "\n",
        "# Load the model artifact we just created\n",
        "artifact_path = f\"{WANDB_ENTITY}/{WANDB_PROJECT}/aqua20-species-classifier:candidate\"\n",
        "loaded_model_artifact = eval_run.use_artifact(artifact_path, type=\"model\")\n",
        "\n",
        "print(f\"\\nðŸ“¦ Using artifact: {artifact_path}\")\n",
        "print(f\"   Version: {loaded_model_artifact.version}\")\n",
        "print(f\"   Metadata: {loaded_model_artifact.metadata}\")\n",
        "\n",
        "# Download the artifact files\n",
        "artifact_dir = loaded_model_artifact.download()\n",
        "print(f\"   Downloaded to: {artifact_dir}\")\n",
        "\n",
        "# Load the model from the artifact\n",
        "model_path = os.path.join(artifact_dir, \"model.pth\")\n",
        "checkpoint = torch.load(model_path, map_location=DEVICE)\n",
        "\n",
        "eval_model = create_model(\n",
        "    CONFIG[\"model_name\"],\n",
        "    CONFIG[\"num_classes\"],\n",
        "    pretrained=False\n",
        ")\n",
        "eval_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "eval_model = eval_model.to(DEVICE)\n",
        "eval_model.eval()\n",
        "\n",
        "print(\"\\nâœ… Model loaded from artifact successfully!\")\n",
        "\n",
        "# Quick evaluation\n",
        "eval_loss, eval_acc, _, _, _ = evaluate(\n",
        "    eval_model, test_loader, criterion, DEVICE, desc=\"Artifact Model Eval\"\n",
        ")\n",
        "\n",
        "wandb.log({\n",
        "    \"evaluation/loss\": eval_loss,\n",
        "    \"evaluation/accuracy\": eval_acc,\n",
        "    \"artifact_version\": loaded_model_artifact.version\n",
        "})\n",
        "\n",
        "print(f\"\\nEvaluation from artifact:\")\n",
        "print(f\"  Accuracy: {eval_acc:.2f}%\")\n",
        "print(f\"  Loss: {eval_loss:.4f}\")\n",
        "\n",
        "wandb.finish()\n",
        "print(\"\\nâœ… Evaluation run finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ02qA_ZbGdY"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# KEY ARTIFACT CONCEPTS RECAP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                    ARTIFACT KEY CONCEPTS                         â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘                                                                  â•‘\n",
        "â•‘  1. PRODUCER WORKFLOW (log_artifact)                             â•‘\n",
        "â•‘     - Create artifact with wandb.Artifact()                      â•‘\n",
        "â•‘     - Add files with artifact.add_file()                         â•‘\n",
        "â•‘     - Log with run.log_artifact()                                â•‘\n",
        "â•‘                                                                  â•‘\n",
        "â•‘  2. CONSUMER WORKFLOW (use_artifact)                             â•‘\n",
        "â•‘     - Fetch with run.use_artifact(\"entity/project/name:alias\")   â•‘\n",
        "â•‘     - Download with artifact.download()                          â•‘\n",
        "â•‘     - Creates INPUT lineage automatically!                       â•‘\n",
        "â•‘                                                                  â•‘\n",
        "â•‘  3. LINEAGE                                                      â•‘\n",
        "â•‘     - use_artifact() â†’ creates INPUT edge                        â•‘\n",
        "â•‘     - log_artifact() â†’ creates OUTPUT edge                       â•‘\n",
        "â•‘     - Automatically tracked in W&B UI                            â•‘\n",
        "â•‘                                                                  â•‘\n",
        "â•‘  4. ALIASES vs VERSIONS                                          â•‘\n",
        "â•‘     - Aliases are mutable pointers (latest, production)          â•‘\n",
        "â•‘     - Versions are immutable (v0, v1, v2...)                     â•‘\n",
        "â•‘                                                                  â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Model Registry: Promote to Production\n",
        "\n",
        "The **Model Registry** is where you promote validated models for deployment.\n",
        "\n",
        "**Key concepts:**\n",
        "- **Artifact aliases** = training state (epoch_1, best, latest)\n",
        "- **Registry aliases** = deployment state (staging, production)\n",
        "\n",
        "**Workflow:**\n",
        "1. Training creates model artifacts with checkpoints\n",
        "2. Link the best artifact to a **Registered Model** (collection)\n",
        "3. Promote within registry: staging â†’ production\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 1: LINK BEST MODEL TO REGISTRY\n",
        "# ============================================================================\n",
        "# Take the best checkpoint from training and link it to a Registered Model\n",
        "\n",
        "registry_run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    name=\"registry-promotion\",\n",
        "    job_type=\"registry-promotion\",\n",
        "    tags=[\"registry\", \"promotion\"],\n",
        ")\n",
        "\n",
        "# Get the best model artifact from training\n",
        "best_model_path = f\"{WANDB_ENTITY}/{WANDB_PROJECT}/model-{CONFIG['model_name']}:best\"\n",
        "best_artifact = registry_run.use_artifact(best_model_path, type=\"model\")\n",
        "\n",
        "print(f\"Best model artifact: {best_artifact.name}:{best_artifact.version}\")\n",
        "print(f\"  Metadata: {best_artifact.metadata}\")\n",
        "\n",
        "# Link to a Registered Model collection in the Registry\n",
        "# This creates a new collection if it doesn't exist\n",
        "REGISTRY_NAME = \"aqua-classifier\"  # Collection name in Registry\n",
        "\n",
        "registry_run.link_artifact(\n",
        "    artifact=best_artifact,\n",
        "    target_path=f\"wandb-registry-model/{REGISTRY_NAME}\",\n",
        "    aliases=[\"staging\"]  # Start in staging\n",
        ")\n",
        "\n",
        "print(f\"\\nLinked to Registry: {REGISTRY_NAME}\")\n",
        "print(f\"  Alias: staging\")\n",
        "print(f\"  Check Model Registry in W&B UI!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 2: PROMOTE TO PRODUCTION\n",
        "# ============================================================================\n",
        "# After validating in staging, promote to production\n",
        "\n",
        "# Use the API to add 'production' alias\n",
        "api = wandb.Api()\n",
        "\n",
        "# Get the staged model from registry\n",
        "registry_path = f\"{WANDB_ENTITY}/wandb-registry-model/{REGISTRY_NAME}:staging\"\n",
        "\n",
        "try:\n",
        "    staged_artifact = api.artifact(registry_path)\n",
        "    \n",
        "    # Validation check before production\n",
        "    val_acc = staged_artifact.metadata.get(\"val_accuracy\", 0)\n",
        "    MIN_ACC_FOR_PRODUCTION = 50.0\n",
        "    \n",
        "    print(f\"Staged model: {staged_artifact.name}\")\n",
        "    print(f\"  Validation accuracy: {val_acc:.2f}%\")\n",
        "    print(f\"  Min required: {MIN_ACC_FOR_PRODUCTION}%\")\n",
        "    \n",
        "    if val_acc >= MIN_ACC_FOR_PRODUCTION:\n",
        "        # Add production alias\n",
        "        staged_artifact.aliases.append(\"production\")\n",
        "        staged_artifact.save()\n",
        "        \n",
        "        print(f\"\\n  PROMOTED TO PRODUCTION!\")\n",
        "        print(f\"  Aliases: {staged_artifact.aliases}\")\n",
        "    else:\n",
        "        print(f\"\\n  FAILED: Accuracy below threshold\")\n",
        "        \n",
        "except wandb.errors.CommError as e:\n",
        "    print(f\"Error: Could not find staged model. Run Step 1 first.\")\n",
        "    print(f\"  {e}\")\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Registry workflow complete!\")\n",
        "print(\"Check Model Registry in W&B UI to see:\")\n",
        "print(\"  - aqua-classifier collection\")\n",
        "print(\"  - staging and production aliases\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QErW_40GbGdc"
      },
      "source": [
        "---\n",
        "\n",
        "## 9. Online vs Offline Runs and Syncing\n",
        "\n",
        "Understand different run modes and how to sync offline runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq8K2vfdbGdc"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ONLINE vs OFFLINE MODE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\"\"\n",
        "W&B Run Modes:\n",
        "\n",
        "1. ONLINE (default)\n",
        "   - Metrics streamed to W&B servers in real-time\n",
        "   - Run visible in UI immediately\n",
        "   - Requires network connection\n",
        "\n",
        "2. OFFLINE\n",
        "   - All data saved locally to ./wandb/ directory\n",
        "   - No network requests during training\n",
        "   - Must sync later to view in UI\n",
        "   - Use when: no internet, restricted networks, HPC clusters\n",
        "\n",
        "3. DISABLED\n",
        "   - No tracking at all\n",
        "   - Use for quick local testing\n",
        "\n",
        "How to set mode:\n",
        "  - In code: wandb.init(mode=\"offline\")\n",
        "  - Environment variable: export WANDB_MODE=offline\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88TWnW-hbGdc"
      },
      "outputs": [],
      "source": [
        "# Demonstrate offline mode\n",
        "print(\"Creating an OFFLINE run...\")\n",
        "\n",
        "offline_run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    name=\"aqua20-offline-demo-run\",\n",
        "    mode=\"offline\",\n",
        "    job_type=\"offline-demo\",\n",
        "    tags=[\"offline\", \"demo\", \"aqua20\"]\n",
        ")\n",
        "\n",
        "print(f\"Run directory: {offline_run.dir}\")\n",
        "print(f\"Run mode: {offline_run.mode}\")\n",
        "\n",
        "for i in range(10):\n",
        "    wandb.log({\"offline_metric\": i * 2, \"step\": i})\n",
        "\n",
        "offline_run_dir = offline_run.dir\n",
        "wandb.finish()\n",
        "\n",
        "print(f\"\\nâœ… Offline run completed\")\n",
        "print(f\"   Data saved to: {offline_run_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P_fcsZCbGdc"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "To sync offline runs to W&B:\n",
        "\n",
        "1. Sync a specific run (point to the .wandb file):\n",
        "   wandb sync wandb/offline-run-YYYYMMDD_HHMMSS-runid/run-runid.wandb\n",
        "\n",
        "2. Sync all offline runs in a directory:\n",
        "   wandb sync --sync-all wandb/\n",
        "\n",
        "3. Check what needs syncing:\n",
        "   wandb sync --view\n",
        "\"\"\")\n",
        "\n",
        "# Show the actual sync command for this run\n",
        "import glob\n",
        "if 'offline_run_dir' in dir():\n",
        "    wandb_files = glob.glob(f'{offline_run_dir}/*.wandb')\n",
        "    if wandb_files:\n",
        "        print(f\"To sync this demo run:\")\n",
        "        print(f\"  wandb sync {wandb_files[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl31gpsibGdc"
      },
      "source": [
        "---\n",
        "\n",
        "## 10. Wrap-up\n",
        "\n",
        "Let's recap what we built and discuss next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "462LSy4ObGdc"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘            AQUA20 WORKSHOP COMPLETE! ðŸ ðŸ¦ˆðŸ‹                      â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "What we built:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "1. âœ… Data Pipeline\n",
        "   - Loaded AQUA20 Aquatic Species dataset from HuggingFace\n",
        "   - Created train/val/test splits\n",
        "   - Logged data sanity checks to W&B\n",
        "\n",
        "2. âœ… Experiment Tracking\n",
        "   - Used proper run naming conventions\n",
        "   - Configured logging profiles\n",
        "   - Maintained step alignment\n",
        "   - Tracked summary vs history metrics\n",
        "\n",
        "3. âœ… Model Training\n",
        "   - Trained EfficientNet-B0 with mixed precision\n",
        "   - Logged training curves and checkpoints\n",
        "   - Saved best model based on validation accuracy\n",
        "\n",
        "4. âœ… Visual Logging\n",
        "   - Logged underwater species prediction samples\n",
        "   - Created W&B Tables for analysis\n",
        "   - Generated confusion matrix\n",
        "   - Logged per-class metrics\n",
        "\n",
        "5. âœ… Artifacts\n",
        "   - Created dataset artifact with metadata\n",
        "   - Created model artifact with aliases\n",
        "   - Demonstrated use_artifact workflow\n",
        "\n",
        "6. âœ… Registry & Promotion\n",
        "   - Promoted model: candidate â†’ staging â†’ production\n",
        "   - Added gated promotion with quality checks\n",
        "   - Created audit trail with timestamps\n",
        "\n",
        "7. âœ… CI/CD Automation\n",
        "   - Implemented training job (Step A)\n",
        "   - Implemented evaluation job (Step B)\n",
        "   - Implemented promotion job (Step C)\n",
        "   - Ran full automated pipeline\n",
        "\n",
        "8. âœ… Offline Mode\n",
        "   - Created offline run\n",
        "   - Learned how to sync runs\n",
        "\n",
        "\n",
        "9. âœ… Post-Training Analysis (Public API)\n",
        "   - Queried runs programmatically\n",
        "   - Compared model architectures\n",
        "   - Updated run summaries\n",
        "\n",
        "Domain-specific considerations for marine biology:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "- Underwater images often have color shifts (blue/green)\n",
        "- Consider underwater-specific augmentations\n",
        "- Class imbalance is common (rare species)\n",
        "- Image quality varies with depth and conditions\n",
        "\n",
        "Next steps to explore:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "- Try different model architectures (ResNet, ViT)\n",
        "- Add underwater-specific augmentations\n",
        "- Implement W&B Sweeps for hyperparameter optimization\n",
        "- Set up GitHub Actions with the CI/CD scripts\n",
        "- Explore W&B Reports for sharing results\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bxYIvNNbGdc"
      },
      "outputs": [],
      "source": [
        "# Cleanup temporary files\n",
        "import shutil\n",
        "\n",
        "cleanup_files = [\n",
        "    \"artifacts\",\n",
        "    \"ci_model_placeholder.json\"\n",
        "]\n",
        "\n",
        "for f in cleanup_files:\n",
        "    if os.path.isdir(f):\n",
        "        shutil.rmtree(f)\n",
        "        print(f\"Removed directory: {f}\")\n",
        "    elif os.path.isfile(f):\n",
        "        os.remove(f)\n",
        "        print(f\"Removed file: {f}\")\n",
        "\n",
        "for f in os.listdir(\".\"):\n",
        "    if f.startswith(\"best_model_\") and f.endswith(\".pth\"):\n",
        "        os.remove(f)\n",
        "        print(f\"Removed: {f}\")\n",
        "\n",
        "print(\"\\nâœ… Cleanup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "post-training-analysis"
      },
      "source": [
        "---\n",
        "\n",
        "## 9. Post-Training Analysis with Public API\n",
        "\n",
        "After multiple workshop participants have trained their models, use the W&B Public API to:\n",
        "- Fetch run data programmatically\n",
        "- Compare runs across the project\n",
        "- Update run summaries after the fact\n",
        "\n",
        "**Note:** This section works best after several participants have logged runs!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api-fetch-runs"
      },
      "outputs": [],
      "source": [
        "# Fetch all workshop training runs using the Public API\n",
        "api = wandb.Api()\n",
        "\n",
        "# Query runs with our workshop tag\n",
        "runs = api.runs(\n",
        "    path=f\"{WANDB_ENTITY}/{WANDB_PROJECT}\",\n",
        "    filters={\"tags\": \"workshop-uk-2026\"},  # Filter by our workshop tag!\n",
        "    order=\"-created_at\"  # Most recent first\n",
        ")\n",
        "\n",
        "print(f\"Found {len(runs)} workshop runs:\\n\")\n",
        "for run in runs[:10]:  # Show first 10\n",
        "    val_acc = run.summary.get('best_val_accuracy', 'N/A')\n",
        "    model = run.config.get('model_name', 'unknown')\n",
        "    print(f\"  {run.name}: {model} | Val Acc: {val_acc}\")\n",
        "    print(f\"    Tags: {run.tags}\")\n",
        "    print(f\"    URL: {run.url}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api-compare-runs"
      },
      "outputs": [],
      "source": [
        "# Compare model architectures: resnet50 vs efficientnet_b0\n",
        "import pandas as pd\n",
        "\n",
        "# Collect data from all workshop runs\n",
        "run_data = []\n",
        "for run in runs:\n",
        "    if run.state == \"finished\":  # Only completed runs\n",
        "        run_data.append({\n",
        "            \"name\": run.name,\n",
        "            \"model\": run.config.get(\"model_name\", \"unknown\"),\n",
        "            \"learning_rate\": run.config.get(\"learning_rate\", None),\n",
        "            \"batch_size\": run.config.get(\"batch_size\", None),\n",
        "            \"epochs\": run.config.get(\"epochs\", None),\n",
        "            \"best_val_acc\": run.summary.get(\"best_val_accuracy\", None),\n",
        "            \"test_acc\": run.summary.get(\"test_accuracy\", None),\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(run_data)\n",
        "print(\"Workshop Runs Comparison:\\n\")\n",
        "display(df)\n",
        "\n",
        "# Group by model architecture\n",
        "if len(df) > 0 and 'model' in df.columns:\n",
        "    print(\"\\nAverage Performance by Model:\")\n",
        "    print(df.groupby('model')['best_val_acc'].agg(['mean', 'std', 'count']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api-update-summary"
      },
      "outputs": [],
      "source": [
        "# Update a run's summary after the fact\n",
        "# This is useful for adding computed metrics or notes\n",
        "\n",
        "# Get a specific run by ID (replace with your run ID)\n",
        "# You can find run IDs in the W&B URL: wandb.ai/entity/project/runs/RUN_ID\n",
        "\n",
        "# Example: Add a custom tag to the best performing run\n",
        "if len(df) > 0:\n",
        "    # Find the best run\n",
        "    best_idx = df['best_val_acc'].idxmax()\n",
        "    best_run_name = df.loc[best_idx, 'name']\n",
        "    \n",
        "    # Fetch that run\n",
        "    for run in runs:\n",
        "        if run.name == best_run_name:\n",
        "            print(f\"Best run: {run.name}\")\n",
        "            print(f\"  Current summary: {dict(run.summary)}\")\n",
        "            \n",
        "            # Add a custom metric to the summary\n",
        "            run.summary['workshop_rank'] = 1\n",
        "            run.summary['notes'] = 'Best performing model in workshop'\n",
        "            run.summary.update()  # Save changes\n",
        "            \n",
        "            print(f\"\\n  Updated summary!\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api-history-analysis"
      },
      "outputs": [],
      "source": [
        "# Access detailed training history (all logged steps)\n",
        "# Useful for custom visualizations or analysis\n",
        "\n",
        "if len(runs) > 0:\n",
        "    # Get the most recent run's history\n",
        "    recent_run = runs[0]\n",
        "    history = recent_run.history()\n",
        "    \n",
        "    print(f\"Run: {recent_run.name}\")\n",
        "    print(f\"History shape: {history.shape}\")\n",
        "    print(f\"\\nAvailable metrics: {list(history.columns)[:10]}...\")\n",
        "    \n",
        "    # Show epoch-level metrics\n",
        "    epoch_cols = [c for c in history.columns if 'epoch' in c.lower() or 'train/' in c or 'val/' in c]\n",
        "    if epoch_cols:\n",
        "        print(f\"\\nEpoch-level data:\")\n",
        "        display(history[epoch_cols].dropna().head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlYwfRupbGdd"
      },
      "source": [
        "---\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [W&B Documentation](https://docs.wandb.ai/)\n",
        "- [W&B Artifacts Guide](https://docs.wandb.ai/guides/artifacts)\n",
        "- [W&B Model Registry](https://docs.wandb.ai/guides/model-registry)\n",
        "- [W&B API Reference](https://docs.wandb.ai/ref/python)\n",
        "- [HuggingFace Datasets](https://huggingface.co/datasets)\n",
        "- [timm Model Zoo](https://github.com/huggingface/pytorch-image-models)\n",
        "\n",
        "---\n",
        "\n",
        "**Thank you for completing this workshop!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sony_workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
