{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "<!--- @wandbcode{artifacts-fundamentals} -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1po8aF76bGdV"
      },
      "source": [
        "# W&B Workshop: Aquatic Species Classification\n",
        "\n",
        "### From First Baseline to Production-Ready Model with Full MLOps\n",
        "\n",
        "---\n",
        "\n",
        "### Premise\n",
        "\n",
        "**Scenario:** You're part of a marine biology AI research team building an image classifier to identify aquatic species from underwater photographs. The goal is to help marine researchers automatically catalog and monitor marine biodiversity.\n",
        "\n",
        "**Your journey:**\n",
        "1. **Train a baseline model** with experiment tracking and visual diagnostics\n",
        "2. **Package the model** as a versioned artifact with lineage back to the training data\n",
        "3. **Stage the baseline** in the Model Registry, then **sweep** for better hyperparameters\n",
        "4. **Promote the winner** to production based on sweep results\n",
        "\n",
        "---\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "| Section | Topic | Key Skills |\n",
        "|---------|-------|------------|\n",
        "| 1 | Setup | Environment configuration, W&B login, config objects |\n",
        "| 2 | Data & Artifacts | Pre-loaded data, lineage with use_artifact |\n",
        "| 3 | Data Exploration | EDA tables, image statistics, grouping and filtering |\n",
        "| 4 | Model Training | Run anatomy, PyTorch training, mixed precision, checkpoint logging, TTL |\n",
        "| 5 | Visual Logging | Images, tables, ROC curves, per-class metrics |\n",
        "| 6 | Resuming a Run | Resume by ID, continue training seamlessly |\n",
        "| 7 | Offline Mode | WANDB_MODE, syncing runs |\n",
        "| 8 | Model Artifacts | Model artifact with lineage, reference artifacts, use_artifact |\n",
        "| 9 | Registry | Collections, linking, staging the baseline |\n",
        "| 10 | Sweeps | Hyperparameter optimization, sweep vs baseline, promote winner |\n",
        "| 11 | Sweep Results | Compare sweep vs baseline, promote winner to production |\n",
        "| 12 | Automations | CI/CD loop, registry triggers, automated workflows |\n",
        "| 13 | Programmatic API (Optional) | Public API queries, filters, training curves, metadata updates |\n",
        "| 14 | Programmatic Reports (Optional) | Reports API, blocks, PanelGrid, automated documentation |\n",
        "| 15 | SDK Settings Reference (Optional) | Network, git, distributed training, advanced settings |\n",
        "| 16 | Wrap-up | Recap, next steps |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smfMb_XbbGdW"
      },
      "source": [
        "---\n",
        "\n",
        "# 1. Setup\n",
        "\n",
        "Let's install dependencies, authenticate with W&B, and configure our experiment.\n",
        "\n",
        "The `workshop_utils.py` file handles all ML boilerplate (transforms, dataset class, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2j3keQQbGdW"
      },
      "outputs": [],
      "source": [
        "# If you haven't already installed dependencies:\n",
        "# !pip install -r ../requirements.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqdSLo5BbGdW"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import random\n",
        "import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import GradScaler\n",
        "import wandb\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Workshop utilities (handles ML boilerplate)\n",
        "from workshop_utils import (\n",
        "    CLASS_NAMES, NUM_CLASSES, DEVICE,\n",
        "    set_seed, get_transforms,\n",
        "    create_model, count_parameters,\n",
        "    train_one_epoch, evaluate,\n",
        "    generate_run_name,\n",
        "    AquaticDataset,\n",
        "    create_dataloaders, create_training_components,\n",
        "    save_checkpoint, log_checkpoint_artifact,\n",
        "    create_prediction_images, create_predictions_table,\n",
        ")\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"W&B: {wandb.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpZVypb0PkZx"
      },
      "source": [
        "## ðŸª„ Configure your `.env` and log in to W&B\n",
        "\n",
        "Before running the next cell, open the `.env` file in this directory and fill in your values:\n",
        "- **`YOUR_NAME`** - your first name, lowercase, no spaces (e.g. `alice`). This namespaces your artifacts and registry entries so multiple participants can share the same project.\n",
        "- **`WANDB_ENTITY`** - your W&B team name\n",
        "- **`WANDB_PROJECT`** - project name (default: `SIE-Workshop-2026`)\n",
        "- **`WANDB_BASE_URL`** - the W&B server URL\n",
        "- **`WANDB_API_KEY`** - find this in your W&B profile under \"Settings\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NNN1uWabGdW"
      },
      "outputs": [],
      "source": [
        "# Load environment variables from .env file\n",
        "# This reads YOUR_NAME, WANDB_ENTITY, WANDB_PROJECT, and WANDB_BASE_URL\n",
        "# so you only need to set them once (in .env), not in every file.\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(override=True)\n",
        "\n",
        "YOUR_NAME = os.environ.get(\"YOUR_NAME\")\n",
        "WANDB_ENTITY = os.environ.get(\"WANDB_ENTITY\")\n",
        "WANDB_PROJECT = os.environ.get(\"WANDB_PROJECT\", \"SIE-Workshop-2026\")\n",
        "WANDB_HOST = os.environ.get(\"WANDB_BASE_URL\")\n",
        "WANDB_API_KEY = os.environ.get(\"WANDB_API_KEY\")\n",
        "\n",
        "if not YOUR_NAME:\n",
        "    raise ValueError(\n",
        "        \"YOUR_NAME not set in .env. Add YOUR_NAME=alice (lowercase, no spaces). \"\n",
        "        \"This namespaces your artifacts and registry entries per participant.\"\n",
        "    )\n",
        "\n",
        "if not WANDB_ENTITY:\n",
        "    raise ValueError(\"WANDB_ENTITY not set in .env. Add your W&B team name.\")\n",
        "\n",
        "# Authenticate with W&B\n",
        "wandb.login(host=WANDB_HOST, key=WANDB_API_KEY)\n",
        "\n",
        "print(f\"Name:    {YOUR_NAME}\")\n",
        "print(f\"Entity:  {WANDB_ENTITY}\")\n",
        "print(f\"Project: {WANDB_PROJECT}\")\n",
        "print(f\"Host:    {WANDB_HOST}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92uGEWjzP85B"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Single config object for all hyperparameters and run metadata. You will be assigned a random open source model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5-fMsLAbGdW"
      },
      "outputs": [],
      "source": [
        "# Random model assignment - creates diverse runs for W&B comparison!\n",
        "WORKSHOP_MODELS = [\"resnet50\", \"efficientnet_b0\"]\n",
        "ASSIGNED_MODEL = random.choice(WORKSHOP_MODELS)\n",
        "print(f\"Your assigned model: {ASSIGNED_MODEL}\")\n",
        "\n",
        "# Training config - these get logged to W&B automatically\n",
        "CONFIG = {\n",
        "    \"user_name\": YOUR_NAME,    # Namespaces artifacts & registry per participant\n",
        "    \"model_name\": ASSIGNED_MODEL,\n",
        "    \"num_classes\": NUM_CLASSES,\n",
        "    \"epochs\": 3,              # Quick training for workshop\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"image_size\": 224,\n",
        "    \"max_samples\": 1000,      # Subset for fast iteration\n",
        "    \"use_amp\": True,\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "# Set reproducibility\n",
        "set_seed(CONFIG[\"seed\"])\n",
        "print(f\"\\nConfig: {CONFIG}\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy9fJN_pbGdW"
      },
      "source": [
        "---\n",
        "\n",
        "# 2. Data Preparation\n",
        "\n",
        "The AQUA dataset has been **pre-prepared, logged as W&B artifacts, and pre-loaded into your local environment**. This mirrors a common production pattern where data lives on shared storage (NFS, S3, a team drive) and teams use W&B to **track and version** it without re-downloading every run.\n",
        "\n",
        "**What's in your local `data/` directory:**\n",
        "- `data/train/` - Training split (~6,500 images, 20 class subfolders)\n",
        "- `data/val/` - Validation split (~800 images)  \n",
        "- `data/test/` - Test split (~800 images)\n",
        "\n",
        "**What's in W&B (same data, versioned as artifacts):**\n",
        "- `aqua-train:v0`, `aqua-val:v0`, `aqua-test:v0`\n",
        "\n",
        "**Key W&B concept:** We call `use_artifact()` in the training run to declare that our run **depends on** these specific dataset versions. This creates **lineage** in W&B -- a graph showing exactly which data trained which model. The actual data is read from local disk; `use_artifact()` handles the tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJzD7T-cbGdX"
      },
      "outputs": [],
      "source": [
        "# W&B ARTIFACT PATHS (for lineage tracking)\n",
        "ARTIFACT_PROJECT = f\"{WANDB_ENTITY}/{WANDB_PROJECT}\"\n",
        "TRAIN_ARTIFACT = f\"{ARTIFACT_PROJECT}/aqua-train:latest\"\n",
        "VAL_ARTIFACT   = f\"{ARTIFACT_PROJECT}/aqua-val:latest\"\n",
        "TEST_ARTIFACT  = f\"{ARTIFACT_PROJECT}/aqua-test:latest\"\n",
        "WEIGHTS_ARTIFACT = f\"{ARTIFACT_PROJECT}/pretrained-{CONFIG['model_name']}:latest\"\n",
        "\n",
        "# Local data paths (pre-loaded in your workshop environment)\n",
        "DATA_ROOT = \"./data\"\n",
        "LOCAL_TRAIN_DIR   = f\"{DATA_ROOT}/train\"\n",
        "LOCAL_VAL_DIR     = f\"{DATA_ROOT}/val\"\n",
        "LOCAL_TEST_DIR    = f\"{DATA_ROOT}/test\"\n",
        "LOCAL_WEIGHTS_DIR = \"./pretrained_weights\"\n",
        "\n",
        "# Verify local data exists\n",
        "for name, path in [(\"Train\", LOCAL_TRAIN_DIR), (\"Val\", LOCAL_VAL_DIR),\n",
        "                   (\"Test\", LOCAL_TEST_DIR), (\"Weights\", LOCAL_WEIGHTS_DIR)]:\n",
        "    status = \"OK\" if os.path.exists(path) else \"MISSING\"\n",
        "    print(f\"  {name}: {path} [{status}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id9m0-v9bGdX"
      },
      "outputs": [],
      "source": [
        "# Includes underwater-optimized augmentations: color jitter, rotation, etc.\n",
        "print(f\"Image transforms ready (size: {CONFIG['image_size']}x{CONFIG['image_size']})\")\n",
        "\n",
        "# Loads images from class folders in downloaded artifacts\n",
        "print(\"AquaticDataset class ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7geOwXwqbGdX"
      },
      "outputs": [],
      "source": [
        "# Section 2 complete - transforms and Dataset class are ready\n",
        "#\n",
        "# The actual data loading happens in the TRAINING RUN (Section 4):\n",
        "# 1. wandb.init() starts the training run\n",
        "# 2. use_artifact() declares dataset dependencies (creates lineage!)\n",
        "# 3. Data is read from local disk (pre-loaded in your environment)\n",
        "# 4. Training proceeds with metrics logged to W&B\n",
        "#\n",
        "# This pattern ensures your training run shows exactly which\n",
        "# dataset versions were used - critical for reproducibility!\n",
        "\n",
        "print(\"Section 2 complete: transforms defined, AquaticDataset ready\")\n",
        "print(\"Data is pre-loaded locally; use_artifact() will track lineage in W&B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14izp4QuQ8D-"
      },
      "source": [
        "---\n",
        "\n",
        "# 3. Data Exploration in W&B\n",
        "\n",
        "An EDA table has been prepared for you in the W&B project!\n",
        "\n",
        "Go to: `<host-url>/<team-name>/SIE-Workshop-2026`. Look for the \"dataset-eda-exploration\" run\n",
        "\n",
        "This run logs a table containing:\n",
        "- Sample images from each class\n",
        "- Image statistics (brightness, contrast, color channels)\n",
        "\n",
        "Use this table to:\n",
        "1. Group by class to inspect samples per species\n",
        "2. Sort by brightness to find dark and bright images\n",
        "3. Filter by blue_ratio to examine underwater color cast\n",
        "4. Compare contrast across marine species\n",
        "5. Use a 2D projection (PCA) to understand global dataset structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZrWCHqtbGdX"
      },
      "source": [
        "---\n",
        "\n",
        "# 4. Anatomy of a `Run` ðŸ©º & Model Training\n",
        "\n",
        "\n",
        "The `Run` stores a detailed record of an experiment within a few specific data structures. The important things to know about are\n",
        "- `Run.config` is a dictionary like structure that stores configuration data for a run, like the path to input data or training hyperparameters. You can instatiate the config by passing a dictionary to `wandb.init(config=<config-dict>)`.\n",
        "- `Run.history` is a list of dictionaries that stores historical values of metrics and media over the course of an experiment. We can append a new snapshot of our training metrics by calling `wandb.log(<metric-dict>)`\n",
        "- `Run.summary` is a dictionary for recording summary metrics or media. By default the `summary` will contain the most recent values logged for each metric, you can overwrite and add elements as you like.\n",
        "\n",
        "\n",
        "For our project we will train a baseline model and include\n",
        "\n",
        "**W&B Features in this section:**\n",
        "- `tags` and `group` - Organize runs for filtering and comparison\n",
        "- `notes` - Quick description visible in run overview  \n",
        "- `define_metric()` - Set epoch as x-axis for cleaner charts\n",
        "- `commit=False` - Log metrics from different phases to the same step\n",
        "- `use_artifact()` - Declare data dependencies with automatic lineage tracking\n",
        "- `wandb.alert()` - Get notified when validation improves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7O4UEYsbGdX"
      },
      "outputs": [],
      "source": [
        "# Model will be created inside the training run (next cell)\n",
        "# so we can load pretrained weights from W&B artifacts with lineage.\n",
        "print(f\"Model: {CONFIG['model_name']}\")\n",
        "print(f\"Weights artifact: {WEIGHTS_ARTIFACT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8tS50AmbGdY"
      },
      "outputs": [],
      "source": [
        "# Part 1: Initialize the W&B Run\n",
        "run_name = generate_run_name(CONFIG)\n",
        "\n",
        "run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    name=run_name,\n",
        "    reinit=\"create_new\",\n",
        "    job_type=\"training\",\n",
        "    group=YOUR_NAME, # GROUP: Isolate your runs â€” filter by YOUR_NAME in the UI\n",
        "    # TAGS: Filterable labels - find runs by model, dataset, experiment type\n",
        "    tags=[\n",
        "        YOUR_NAME,                     # Your name â€” filter to find your runs\n",
        "        \"AQUA\",                        # Dataset\n",
        "        \"baseline\",                    # Experiment type\n",
        "        CONFIG[\"model_name\"],          # Model architecture\n",
        "        \"workshop-uk-2026\",            # Workshop identifier\n",
        "        f\"epochs-{CONFIG['epochs']}\",  # Hyperparameter tag\n",
        "    ],\n",
        "    # NOTES: Quick description (visible in run overview)\n",
        "    notes=f\"Workshop training: {CONFIG['model_name']} on AQUA. \"\n",
        "          f\"Epochs: {CONFIG['epochs']}, LR: {CONFIG['learning_rate']}, BS: {CONFIG['batch_size']}\",\n",
        "    config=CONFIG,\n",
        "    # SHARED MODE: allows multiple processes to log to the same run\n",
        "    settings=wandb.Settings(\n",
        "        mode=\"shared\",\n",
        "        x_label=\"primary\",     # Label shown in W&B UI for this node's logs/metrics\n",
        "        x_primary=True,        # This is the primary node (uploads config, telemetry, etc.)\n",
        "    ),\n",
        ")\n",
        "\n",
        "# DEFINE_METRIC: Set \"epoch\" as x-axis for cleaner charts\n",
        "#\n",
        "# IMPORTANT: Always scope define_metric to specific prefixes like \"train/*\"\n",
        "# or \"val/*\". Avoid run.define_metric(\"*\", step_metric=\"epoch\") â€” at scale\n",
        "# this bloats run metadata past the 15MB limit and kills frontend performance.\n",
        "run.define_metric(\"epoch\")\n",
        "run.define_metric(\"train/loss\", step_metric=\"epoch\")\n",
        "run.define_metric(\"train/accuracy\", step_metric=\"epoch\")\n",
        "run.define_metric(\"val/*\", step_metric=\"epoch\")\n",
        "run.define_metric(\"learning_rate\", step_metric=\"epoch\")\n",
        "\n",
        "# Per-batch step metrics: use train/global_step as x-axis\n",
        "# (shared mode does not support the `step=` argument in run.log(),\n",
        "#  so we log the step as a metric and define it as the x-axis here)\n",
        "#\n",
        "# hidden=True: this is a step counter, not a metric worth charting.\n",
        "# Hiding it keeps the workspace clean â€” at scale with hundreds of\n",
        "# metrics, this prevents auto-generated panels from cluttering the UI.\n",
        "run.define_metric(\"train/global_step\", hidden=True)\n",
        "run.define_metric(\"train/loss_step\", step_metric=\"train/global_step\")\n",
        "run.define_metric(\"train/acc_step\", step_metric=\"train/global_step\")\n",
        "\n",
        "print(f\"Run: {run_name}\")\n",
        "print(f\"  View at: {run.url}\")\n",
        "print(f\"  Tags: {run.tags}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 2: Load Artifacts + Setup\n",
        "# use_artifact() creates LINEAGE â€” W&B tracks exactly which data trained this model\n",
        "# Data is pre-loaded locally; we call use_artifact() purely for lineage tracking\n",
        "\n",
        "run.use_artifact(TRAIN_ARTIFACT, type='dataset')\n",
        "run.use_artifact(VAL_ARTIFACT,   type='dataset')\n",
        "run.use_artifact(TEST_ARTIFACT,  type='dataset')\n",
        "\n",
        "# Point to pre-loaded local data\n",
        "train_dir, val_dir, test_dir = LOCAL_TRAIN_DIR, LOCAL_VAL_DIR, LOCAL_TEST_DIR\n",
        "\n",
        "# Create model from local pretrained weights (lineage tracked via W&B artifact)\n",
        "model = create_model(\n",
        "    CONFIG[\"model_name\"], NUM_CLASSES, pretrained=True,\n",
        "    weights_artifact=WEIGHTS_ARTIFACT, run=run,\n",
        "    local_weights_dir=LOCAL_WEIGHTS_DIR\n",
        ")\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "train_loader, val_loader, test_loader = create_dataloaders(train_dir, val_dir, test_dir, CONFIG)\n",
        "criterion, optimizer, scheduler, scaler = create_training_components(model, CONFIG)\n",
        "\n",
        "# Keep a reference to the test dataset (needed for visualization in Section 5)\n",
        "test_dataset = AquaticDataset(\n",
        "    test_dir,\n",
        "    transform=get_transforms(CONFIG[\"image_size\"], is_training=False),\n",
        "    class_names=CLASS_NAMES\n",
        ")\n",
        "\n",
        "total_params, trainable_params = count_parameters(model)\n",
        "print(f\"\\nModel: {CONFIG['model_name']} ({total_params:,} params, {trainable_params:,} trainable)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 3: Training Loop (W&B logging)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_model_path = None\n",
        "\n",
        "for epoch in range(CONFIG[\"epochs\"]):\n",
        "    print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
        "\n",
        "    # SPARSE LOGGING: log_interval controls how often per-batch metrics are sent.\n",
        "    # At scale, logging every batch is expensive â€” log every N steps instead.\n",
        "    # Here log_interval=1 for the short baseline; sweeps use log_interval=5.\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model, train_loader, criterion, optimizer, scaler, DEVICE,\n",
        "        epoch, log_interval=1, run=run\n",
        "    )\n",
        "    val_loss, val_acc, val_preds, val_labels, val_probs = evaluate(\n",
        "        model, val_loader, criterion, DEVICE, desc=f\"Epoch {epoch+1} [Val]\"\n",
        "    )\n",
        "    scheduler.step()\n",
        "\n",
        "    # â”€â”€ STEP CONTROL: commit=False keeps all epoch metrics on the SAME step â”€â”€\n",
        "    # Without it, each run.log() creates a new step â†’ misaligned charts!\n",
        "    #\n",
        "    # This pattern also matters for MIXED-FREQUENCY logging at scale:\n",
        "    # log cheap scalars every step, expensive media (images, tables) less often.\n",
        "    # Use commit=False to batch them, commit=True only on the final call per step.\n",
        "    run.log({\"epoch\": epoch + 1, \"train/loss\": train_loss, \"train/accuracy\": train_acc}, commit=False)\n",
        "    run.log({\"val/loss\": val_loss, \"val/accuracy\": val_acc}, commit=False)\n",
        "    run.log({\"learning_rate\": scheduler.get_last_lr()[0]})  # commit=True â†’ step advances\n",
        "\n",
        "    print(f\"  Train: {train_loss:.4f} loss, {train_acc:.2f}% acc\")\n",
        "    print(f\"  Val:   {val_loss:.4f} loss, {val_acc:.2f}% acc\")\n",
        "\n",
        "    # â”€â”€ BEST MODEL TRACKING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_path = f\"best_model_epoch{epoch+1}.pth\"\n",
        "        save_checkpoint(model, optimizer, CONFIG, epoch+1, val_acc, val_loss, best_model_path)\n",
        "\n",
        "        # ALERT: Notification when validation improves\n",
        "        run.alert(\n",
        "            title=\"New Best Model!\",\n",
        "            text=f\"Validation accuracy improved to {val_acc:.2f}% at epoch {epoch+1}\",\n",
        "            level=wandb.AlertLevel.INFO\n",
        "        )\n",
        "        run.summary.update({\"best_val_accuracy\": val_acc, \"best_val_loss\": val_loss, \"best_epoch\": epoch + 1})\n",
        "\n",
        "    # â”€â”€ LOG CHECKPOINT ARTIFACT (versioned, with TTL) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    log_checkpoint_artifact(\n",
        "        run, model, optimizer, CONFIG, epoch+1,\n",
        "        metrics={\"val_accuracy\": val_acc, \"val_loss\": val_loss,\n",
        "                 \"train_accuracy\": train_acc, \"train_loss\": train_loss},\n",
        "        is_best=(val_acc >= best_val_acc),\n",
        "        is_last=(epoch == CONFIG[\"epochs\"] - 1),\n",
        "    )\n",
        "\n",
        "print(f\"\\nTraining complete! Best val accuracy: {best_val_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv5fjEKnOD3w"
      },
      "outputs": [],
      "source": [
        "# TTL IN ACTION: Inspect and Modify Artifact TTL via the API\n",
        "# We set TTL=7 days on checkpoints during training. But what if the \"best\"\n",
        "# checkpoint turns out to be important? You can extend or remove TTL after\n",
        "# the fact using the Public API.\n",
        "\n",
        "ttl_days = 60\n",
        "\n",
        "api = wandb.Api()\n",
        "best_checkpoint_path = f\"{WANDB_ENTITY}/{WANDB_PROJECT}/model-{YOUR_NAME}-{CONFIG['model_name']}:best\"\n",
        "\n",
        "try:\n",
        "    best_checkpoint = api.artifact(best_checkpoint_path)\n",
        "    print(f\"Best checkpoint: {best_checkpoint.name}:{best_checkpoint.version}\")\n",
        "    print(f\"  Current TTL: {best_checkpoint.ttl}\")\n",
        "\n",
        "    best_checkpoint.ttl = timedelta(days=ttl_days)\n",
        "    best_checkpoint.save()\n",
        "    print(f\"  Updated TTL: {best_checkpoint.ttl}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not fetch artifact (run training first): {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-c6AUV3bGdY"
      },
      "source": [
        "---\n",
        "\n",
        "# 5. Visual Logging (Media)\n",
        "\n",
        "Log rich visual diagnostics: underwater images, predictions, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlK3IzY3bGdY"
      },
      "outputs": [],
      "source": [
        "# Final evaluation on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "test_loss, test_acc, test_preds, test_labels, test_probs = evaluate(\n",
        "    model, test_loader, criterion, DEVICE, desc=\"Test Evaluation\"\n",
        ")\n",
        "\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"  Test Loss: {test_loss:.4f}\")\n",
        "print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "run.log({\n",
        "    \"test/loss\": test_loss,\n",
        "    \"test/accuracy\": test_acc\n",
        "})\n",
        "\n",
        "run.summary[\"test_accuracy\"] = test_acc\n",
        "run.summary[\"test_loss\"] = test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8Z3zNwWbGdY"
      },
      "outputs": [],
      "source": [
        "# Log prediction samples with images and confidence scores\n",
        "# create_prediction_images() handles the boilerplate\n",
        "\n",
        "prediction_images = create_prediction_images(\n",
        "    test_dataset, test_preds, test_probs, CLASS_NAMES, n_samples=16\n",
        ")\n",
        "\n",
        "# Log to W&B - images appear in Media tab\n",
        "run.log({\"predictions/samples\": prediction_images})\n",
        "\n",
        "print(f\"Logged {len(prediction_images)} prediction samples to W&B\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrdc-XvdbGdY"
      },
      "outputs": [],
      "source": [
        "# Create W&B Table with predictions for detailed analysis\n",
        "# Includes per-class confidence scores for histogram visualization\n",
        "\n",
        "predictions_table = create_predictions_table(\n",
        "    test_dataset, test_preds, test_probs, CLASS_NAMES, n_samples=100\n",
        ")\n",
        "\n",
        "# Log to W&B - table appears in Tables tab\n",
        "run.log({\"predictions/analysis_table\": predictions_table})\n",
        "\n",
        "print(f\"Logged predictions table with {len(CLASS_NAMES)} class score columns\")\n",
        "print(\"  In W&B UI try:\")\n",
        "print(\"  - Group by 'truth' to see recall per class\")\n",
        "print(\"  - Group by 'guess' to see precision per class\")\n",
        "print(\"  - Filter: row['truth'] != row['guess'] to find errors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCuEgY99bGdY"
      },
      "outputs": [],
      "source": [
        "# Log per-class metrics\n",
        "# Get unique classes that appear in the data (handles fast_run with subset)\n",
        "unique_classes = sorted(set(test_labels) | set(test_preds))\n",
        "\n",
        "precision, recall, f1, support = precision_recall_fscore_support(\n",
        "    test_labels, test_preds, labels=unique_classes, average=None, zero_division=0\n",
        ")\n",
        "\n",
        "metrics_table = wandb.Table(\n",
        "    columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"]\n",
        ")\n",
        "\n",
        "for i, class_idx in enumerate(unique_classes):\n",
        "    class_name = CLASS_NAMES[class_idx] if class_idx < len(CLASS_NAMES) else f\"Class_{class_idx}\"\n",
        "    metrics_table.add_data(\n",
        "        class_name,\n",
        "        round(precision[i], 4),\n",
        "        round(recall[i], 4),\n",
        "        round(f1[i], 4),\n",
        "        int(support[i])\n",
        "    )\n",
        "\n",
        "run.log({\"evaluation/per_class_metrics\": metrics_table})\n",
        "\n",
        "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "    test_labels, test_preds, average='macro'\n",
        ")\n",
        "\n",
        "run.summary[\"precision_macro\"] = precision_macro\n",
        "run.summary[\"recall_macro\"] = recall_macro\n",
        "run.summary[\"f1_macro\"] = f1_macro\n",
        "\n",
        "print(\"\\nPer-class metrics logged.\")\n",
        "print(f\"Macro F1-Score: {f1_macro:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table Logging Modes: MUTABLE and INCREMENTAL\n",
        "\n",
        "The tables we just logged are **IMMUTABLE** â€” once logged, they can't be modified. That's the default and works great for end-of-run snapshots.\n",
        "\n",
        "But what if you want to:\n",
        "- **Enrich a table after the fact** â€” add new columns as you compute more metrics? That's **MUTABLE** mode.\n",
        "- **Watch a table grow during training** â€” append rows in batches so you can monitor results in real-time? That's **INCREMENTAL** mode.\n",
        "\n",
        "Below we demonstrate both. Each stage pauses so you can open the W&B UI and watch the table change live."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ MUTABLE TABLE: Enrich results by adding columns over time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Stage 1: Log predictions only\n",
        "# Stage 2: Add confidence scores\n",
        "# Stage 3: Add correct/incorrect flag\n",
        "# Open the W&B UI â†’ run â†’ \"mutable_evals\" table to watch it grow!\n",
        "\n",
        "#Stage 1\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "mutable_table = wandb.Table(\n",
        "    columns=[\"image_idx\", \"true_class\", \"predicted_class\"],\n",
        "    log_mode=\"MUTABLE\",\n",
        ")\n",
        "\n",
        "# Stage 1: Just predictions\n",
        "for i in range(min(50, len(test_preds))):\n",
        "    mutable_table.add_data(\n",
        "        i,\n",
        "        CLASS_NAMES[test_labels[i]],\n",
        "        CLASS_NAMES[test_preds[i]],\n",
        "    )\n",
        "\n",
        "run.log({\"mutable_evaluations\": mutable_table})\n",
        "print(\"Stage 1: Logged predictions (3 columns)\")\n",
        "print(\"  â†’ Open W&B UI now. Check the 'mutable_evaluations' table.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stage 2: Add confidence scores\n",
        "confidences = [float(test_probs[i][test_preds[i]]) for i in range(min(50, len(test_preds)))]\n",
        "mutable_table.add_column(\"confidence\", [round(c, 4) for c in confidences])\n",
        "run.log({\"mutable_evaluations\": mutable_table})\n",
        "print(\"Stage 2: Added 'confidence' column (now 4 columns)\")\n",
        "print(\"  â†’ Refresh the table in W&B â€” the new column appears.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stage 3: Add correct/incorrect flag\n",
        "correct_flags = [\"Correct\" if test_labels[i] == test_preds[i] else \"Wrong\"\n",
        "                 for i in range(min(50, len(test_preds)))]\n",
        "mutable_table.add_column(\"result\", correct_flags)\n",
        "run.log({\"mutable_evaluations\": mutable_table})\n",
        "print(\"Stage 3: Added 'result' column (now 5 columns)\")\n",
        "print(\"  â†’ Refresh again â€” filter by 'result' = 'Wrong' to see misclassifications.\")\n",
        "print(\"\\nMUTABLE example complete. Table was updated in-place 3 times.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ INCREMENTAL TABLE: Watch rows appear in batches â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Simulates a long-running job where predictions arrive in batches.\n",
        "# Open the W&B UI â†’ run â†’ \"incremental_predictions\" table.\n",
        "# Use the step slider in the UI to scrub through increments!\n",
        "\n",
        "incr_table = wandb.Table(\n",
        "    columns=[\"batch\", \"image_idx\", \"true_class\", \"predicted_class\", \"confidence\"],\n",
        "    log_mode=\"INCREMENTAL\",\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "num_samples = min(20, len(test_preds))\n",
        "num_batches = num_samples // BATCH_SIZE\n",
        "\n",
        "print(f\"Logging {num_samples} predictions in {num_batches} batches of {BATCH_SIZE}\")\n",
        "print(\"  â†’ Open W&B UI now. Watch the 'incremental_predictions' table grow.\\n\")\n",
        "\n",
        "for batch_idx in range(num_batches):\n",
        "    start = batch_idx * BATCH_SIZE\n",
        "    end = start + BATCH_SIZE\n",
        "\n",
        "    for i in range(start, end):\n",
        "        incr_table.add_data(\n",
        "            batch_idx + 1,\n",
        "            i,\n",
        "            CLASS_NAMES[test_labels[i]],\n",
        "            CLASS_NAMES[test_preds[i]],\n",
        "            round(float(test_probs[i][test_preds[i]]), 4),\n",
        "        )\n",
        "\n",
        "    run.log({\"incremental_predictions\": incr_table})\n",
        "    print(f\"  Batch {batch_idx + 1}/{num_batches}: {end} rows total\")\n",
        "    time.sleep(10)  # pause so you can see each batch arrive in the UI\n",
        "\n",
        "print(f\"\\nINCREMENTAL demo complete. {num_samples} rows logged across {num_batches} batches.\")\n",
        "print(\"  â†’ In the W&B UI, use the step slider below the table to scrub through batches.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STOP HERE â€” Shared Mode Demo\n",
        "\n",
        "**Do NOT run the next cell yet.**\n",
        "\n",
        "Your training run is still active. Now open a **new terminal** and run (after updating the run ID in the script):\n",
        "\n",
        "```bash\n",
        "python shared_worker.py\n",
        "```\n",
        "\n",
        "This launches a shared-mode worker that logs to the **same run**. Check the W&B UI to see metrics arriving from both processes. Metrics for worker are logged under `worker` section\n",
        "\n",
        "Once the worker finishes, come back and continue running the remaining cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eh9nLIsOD3w"
      },
      "outputs": [],
      "source": [
        "# Log ROC curve using W&B's built-in interactive chart\n",
        "# This creates a one-vs-rest ROC curve for each species class,\n",
        "# showing how well the model discriminates each species from all others.\n",
        "# The chart is fully interactive in the W&B UI: hover, toggle classes, auto-AUC.\n",
        "\n",
        "run.log({\n",
        "    \"evaluation/roc_curve\": wandb.plot.roc_curve(\n",
        "        y_true=test_labels,\n",
        "        y_probas=test_probs,\n",
        "        labels=CLASS_NAMES,\n",
        "        title=\"AQUA Species ROC Curves\"\n",
        "    )\n",
        "})\n",
        "\n",
        "# Save the run ID before finishing â€” we'll need it to resume later\n",
        "training_run_id = run.id\n",
        "print(f\"Run ID saved for resume: {training_run_id}\")\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Fix ROC curve colors**\n",
        "\n",
        "Notice how all the lines in the ROC chart are the same color (Pink)? That's because W&B colors by *run*, not by *class* â€” and since all 20 species come from a single run, they all inherit the same run color.\n",
        "\n",
        "Vega based custom charts are fully customizable and reusable to give each species its own color:\n",
        "1. Hover over the **AQUA Species ROC Curves** chart and click the ** Gear âš™ icon**\n",
        "2. Select Edit adjacent to **Vega spec** tab\n",
        "3. Find lines 91â€“99\n",
        "\n",
        "\n",
        "```json\n",
        "      \"encoding\": {\n",
        "        \"color\": {\n",
        "          \"type\": \"nominal\",\n",
        "          \"field\": \"name\",\n",
        "          \"scale\": {\n",
        "            \"range\": {\n",
        "              \"field\": \"color\"\n",
        "            }\n",
        "          },\n",
        "```\n",
        "\n",
        "to \n",
        "\n",
        "```json\n",
        "      \"encoding\": {\n",
        "        \"color\": {\n",
        "          \"type\": \"nominal\",\n",
        "          \"field\": \"class\",\n",
        "          \"scale\": {\"range\": \"category\"},\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 6. Resuming a Run\n",
        "\n",
        "We just finished 3 epochs and called `run.finish()`. But what if we look at the charts and decide the model needs more training? Or what if our training script crashed at epoch 2 and we want to pick up where we left off?\n",
        "\n",
        "W&B lets you **resume a run** by its ID. The resumed run continues logging to the same history â€” your charts show a seamless line from epoch 1 through 10, not two separate runs.\n",
        "\n",
        "The key is `resume=\"must\"`:\n",
        "- `\"must\"` â€” the run **must** already exist (errors otherwise)\n",
        "- `\"allow\"` â€” resume if the run exists, create a new one if it doesn't\n",
        "- `\"auto\"` â€” automatically resume from the same filesystem\n",
        "\n",
        "Since the model, optimizer, and data are still in memory (kernel never restarted), we just need to reconnect to the run and keep training. In a real crash-recovery scenario, you'd reload from the saved checkpoint â€” same pattern, just add `model.load_state_dict()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resume the training run and continue to 10 epochs\n",
        "TOTAL_EPOCHS = 10\n",
        "\n",
        "run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    id=training_run_id,   # same run ID from earlier\n",
        "    resume=\"must\",        # must exist â€” error if it doesn't\n",
        ")\n",
        "\n",
        "# Re-declare define_metric â€” these are client-side instructions\n",
        "# that don't persist across sessions. In a real training script\n",
        "# these would already be in your code, so resume \"just works\".\n",
        "run.define_metric(\"epoch\")\n",
        "run.define_metric(\"train/loss\", step_metric=\"epoch\")\n",
        "run.define_metric(\"train/accuracy\", step_metric=\"epoch\")\n",
        "run.define_metric(\"val/*\", step_metric=\"epoch\")\n",
        "run.define_metric(\"learning_rate\", step_metric=\"epoch\")\n",
        "run.define_metric(\"train/global_step\")\n",
        "run.define_metric(\"train/loss_step\", step_metric=\"train/global_step\")\n",
        "run.define_metric(\"train/acc_step\", step_metric=\"train/global_step\")\n",
        "\n",
        "print(f\"Resumed run: {run.id}\")\n",
        "print(f\"Training epochs {CONFIG['epochs']+1} â†’ {TOTAL_EPOCHS}\\n\")\n",
        "\n",
        "for epoch in range(CONFIG[\"epochs\"], TOTAL_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{TOTAL_EPOCHS}\")\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model, train_loader, criterion, optimizer, scaler, DEVICE,\n",
        "        epoch, log_interval=1, run=run\n",
        "    )\n",
        "    val_loss, val_acc, val_preds, val_labels, val_probs = evaluate(\n",
        "        model, val_loader, criterion, DEVICE, desc=f\"Epoch {epoch+1} [Val]\"\n",
        "    )\n",
        "    scheduler.step()\n",
        "\n",
        "    run.log({\"epoch\": epoch + 1, \"train/loss\": train_loss, \"train/accuracy\": train_acc}, commit=False)\n",
        "    run.log({\"val/loss\": val_loss, \"val/accuracy\": val_acc}, commit=False)\n",
        "    run.log({\"learning_rate\": scheduler.get_last_lr()[0]})\n",
        "\n",
        "    print(f\"  Train: {train_loss:.4f} loss, {train_acc:.2f}% acc\")\n",
        "    print(f\"  Val:   {val_loss:.4f} loss, {val_acc:.2f}% acc\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_path = f\"best_model_epoch{epoch+1}.pth\"\n",
        "        save_checkpoint(model, optimizer, CONFIG, epoch+1, val_acc, val_loss, best_model_path)\n",
        "        run.summary.update({\"best_val_accuracy\": val_acc, \"best_val_loss\": val_loss, \"best_epoch\": epoch + 1})\n",
        "\n",
        "print(f\"\\nResumed training complete! Best val accuracy: {best_val_acc:.2f}%\")\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 7. Offline Mode\n",
        "\n",
        "Not every training environment has internet access. Maybe you're on a GPU cluster behind a firewall, or you only have access to the compute nodes but not the W&B instance. W&B handles this with **offline mode** â€” all metrics, artifacts, and system stats are saved to a local directory. When you're back online, sync everything to W&B with a single command.\n",
        "\n",
        "Set `mode=\"offline\"` when initializing the run. W&B writes everything locally instead of sending it to the server. Below we'll run a short training loop offline, then sync the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Offline mode â€” train without any connection to W&B\n",
        "OFFLINE_EPOCHS = 2\n",
        "\n",
        "offline_run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    name=f\"offline-training-run-{YOUR_NAME}\",\n",
        "    mode=\"offline\",               # everything saved locally\n",
        "    group=YOUR_NAME,\n",
        "    config=CONFIG,\n",
        "    tags=[YOUR_NAME, \"AQUA\", \"offline-demo\", CONFIG[\"model_name\"]],\n",
        ")\n",
        "\n",
        "offline_run.define_metric(\"epoch\")\n",
        "offline_run.define_metric(\"train/*\", step_metric=\"epoch\")\n",
        "offline_run.define_metric(\"val/*\", step_metric=\"epoch\")\n",
        "\n",
        "print(f\"Training offline for {OFFLINE_EPOCHS} epochs (no data sent to W&B)...\\n\")\n",
        "\n",
        "for epoch in range(OFFLINE_EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{OFFLINE_EPOCHS}\")\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model, train_loader, criterion, optimizer, scaler, DEVICE,\n",
        "        epoch, log_interval=5, run=offline_run #\n",
        "    )\n",
        "    val_loss, val_acc, _, _, _ = evaluate(\n",
        "        model, val_loader, criterion, DEVICE, desc=f\"Epoch {epoch+1} [Val]\"\n",
        "    )\n",
        "\n",
        "    offline_run.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train/loss\": train_loss, \"train/accuracy\": train_acc,\n",
        "        \"val/loss\": val_loss, \"val/accuracy\": val_acc,\n",
        "    })\n",
        "\n",
        "    print(f\"  Train: {train_loss:.4f} loss, {train_acc:.2f}% acc\")\n",
        "    print(f\"  Val:   {val_loss:.4f} loss, {val_acc:.2f}% acc\")\n",
        "\n",
        "offline_run.finish()\n",
        "\n",
        "print(\"\\nRun saved locally. To sync to W&B when you're back online:\")\n",
        "print(\"\\nThe run directory path is printed above by W&B.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF6Z9u6UbGdY"
      },
      "source": [
        "---\n",
        "\n",
        "# 8. Model Artifacts with Lineage\n",
        "\n",
        "We've already logged our **data artifacts** in Section 2:\n",
        "- Raw dataset artifact (`aqua-raw`)\n",
        "- Split artifacts with lineage (`aqua-train`, `aqua-val`, `aqua-test`)\n",
        "\n",
        "Now it's time to complete the lineage by **logging the trained model as an artifact** that references the training data!\n",
        "\n",
        "### Complete Lineage Chain\n",
        "\n",
        "```\n",
        "Raw Dataset â†’ Train/Val/Test Splits â†’ Model\n",
        "```\n",
        "\n",
        "By using `use_artifact()` to consume the split artifacts when logging our model, we create a complete audit trail showing exactly which data was used to train this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j20RWM9UbGdY"
      },
      "outputs": [],
      "source": [
        "# MODEL ARTIFACT - Complete the Lineage Chain\n",
        "\n",
        "# Start a run that consumes the split artifacts and logs the trained model.\n",
        "# This creates the final lineage: Splits â†’ Model\n",
        "\n",
        "model_artifact_run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    name=f\"aqua-model-artifact-logging-{YOUR_NAME}\",\n",
        "    job_type=\"model-logging\",\n",
        "    group=YOUR_NAME,\n",
        "    tags=[YOUR_NAME, \"aqua\", \"model-artifact\", \"baseline\"],\n",
        "    notes=f\"Log trained model artifact with lineage to training data splits ({YOUR_NAME})\"\n",
        ")\n",
        "\n",
        "# Reference the SAME artifacts used during training (creates proper lineage)\n",
        "train_artifact_ref = model_artifact_run.use_artifact(TRAIN_ARTIFACT, type='dataset')\n",
        "val_artifact_ref = model_artifact_run.use_artifact(VAL_ARTIFACT, type='dataset')\n",
        "\n",
        "print(f\"Model trained using:\")\n",
        "print(f\"  Train artifact: {train_artifact_ref.name}:{train_artifact_ref.version}\")\n",
        "print(f\"  Val artifact: {val_artifact_ref.name}:{val_artifact_ref.version}\")\n",
        "\n",
        "# Prepare model info\n",
        "model_info = {\n",
        "    \"architecture\": CONFIG[\"model_name\"],\n",
        "    \"num_classes\": NUM_CLASSES,\n",
        "    \"class_names\": CLASS_NAMES,\n",
        "    \"input_size\": CONFIG[\"image_size\"],\n",
        "    \"pretrained\": True,\n",
        "    \"total_params\": total_params,\n",
        "    \"trainable_params\": trainable_params,\n",
        "    \"training_config\": {\n",
        "        \"epochs\": CONFIG[\"epochs\"],\n",
        "        \"batch_size\": CONFIG[\"batch_size\"],\n",
        "        \"learning_rate\": CONFIG[\"learning_rate\"],\n",
        "        \"optimizer\": \"AdamW\"\n",
        "    },\n",
        "    \"metrics\": {\n",
        "        \"best_val_accuracy\": best_val_acc,\n",
        "        \"test_accuracy\": test_acc,\n",
        "        \"test_loss\": test_loss,\n",
        "        \"f1_macro\": f1_macro\n",
        "    },\n",
        "    \"data_artifacts\": {\n",
        "        \"train\": f\"{train_artifact_ref.name}:{train_artifact_ref.version}\",\n",
        "        \"val\": f\"{val_artifact_ref.name}:{val_artifact_ref.version}\"\n",
        "    },\n",
        "    \"created_at\": datetime.now().isoformat(),\n",
        "    \"training_run_id\": model_artifact_run.id\n",
        "}\n",
        "\n",
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "with open(\"artifacts/model_info.json\", \"w\") as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "\n",
        "import shutil\n",
        "shutil.copy(best_model_path, \"artifacts/model.pth\")\n",
        "\n",
        "# Create and log the model artifact (namespaced per participant)\n",
        "model_artifact = wandb.Artifact(\n",
        "    name=f\"aqua-species-classifier-{YOUR_NAME}\",\n",
        "    type=\"model\",\n",
        "    description=f\"Aquatic Species classifier ({CONFIG['model_name']}) trained on AQUA dataset ({YOUR_NAME})\",\n",
        "    metadata={\n",
        "        \"architecture\": CONFIG[\"model_name\"],\n",
        "        \"num_classes\": NUM_CLASSES,\n",
        "        \"input_size\": CONFIG[\"image_size\"],\n",
        "        \"final_val_accuracy\": best_val_acc,\n",
        "        \"final_test_accuracy\": test_acc,\n",
        "        \"f1_macro\": f1_macro,\n",
        "        \"framework\": \"pytorch\",\n",
        "        \"domain\": \"marine_biology\",\n",
        "        \"train_artifact\": f\"{train_artifact_ref.name}:{train_artifact_ref.version}\",\n",
        "        \"val_artifact\": f\"{val_artifact_ref.name}:{val_artifact_ref.version}\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Reference artifacts: track files by path + checksum without uploading\n",
        "# (In production you'd use add_file() to upload to W&B storage,\n",
        "#  but we use references here to keep the workshop network-friendly)\n",
        "model_artifact.add_reference(f\"file://{os.path.abspath('artifacts/model.pth')}\")\n",
        "model_artifact.add_reference(f\"file://{os.path.abspath('artifacts/model_info.json')}\")\n",
        "\n",
        "model_artifact_run.log_artifact(model_artifact, aliases=[\"latest\", \"candidate\", \"baseline\"], tags=[\"baseline-model\", \"aqua\", \"marine-biology\"])\n",
        "\n",
        "print(f\"\\nModel artifact logged: aqua-species-classifier-{YOUR_NAME} (reference, no upload)\")\n",
        "print(f\"  Test Accuracy: {test_acc:.2f}%, Aliases: latest, candidate, baseline\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIgPxnHyOD3w"
      },
      "outputs": [],
      "source": [
        "# COMPLETE ARTIFACT LINEAGE SUMMARY\n",
        "# At this point, we've created the following artifact lineage:\n",
        "\n",
        "print(\"Artifact lineage chain:\")\n",
        "print(\"  dataset-upload (run)\")\n",
        "print(\"    -> aqua-raw-dataset:v0\")\n",
        "print(\"      -> dataset-splitting (run)\")\n",
        "print(\"        -> aqua-train:v0, aqua-val:v0\")\n",
        "print(f\"          -> aqua-model-artifact-logging-{YOUR_NAME} (run)\")\n",
        "print(f\"            -> aqua-species-classifier-{YOUR_NAME}:v0\")\n",
        "print()\n",
        "print(\"View lineage: W&B UI > Artifacts > select any artifact > Lineage tab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FshFhMgOD3w"
      },
      "source": [
        "## Reference Artifacts: Track External Data Without Copying\n",
        "\n",
        "So far, every artifact we've created **copies** files into W&B storage. But what if your data already lives somewhere else (S3, GCS, a shared filesystem) and you don't want to duplicate it?\n",
        "\n",
        "**Reference artifacts** solve this: they log metadata and checksums *about* external files without uploading the actual data. W&B tracks the location, size, and integrity -- but the bytes stay where they are.\n",
        "\n",
        "Supported URI schemes:\n",
        "* **file://** -- local filesystem\n",
        "* **http(s)://:** A path to a file accessible over HTTP. The artifact will track checksums in the form of etags and size metadata if the HTTP server supports the ETag and Content-Length response headers.\n",
        "* **s3://:** A path to an object or object prefix in S3. The artifact will track checksums and versioning information (if the bucket has object versioning enabled) for the referenced objects. Object prefixes are expanded to include the objects under the prefix, default up to 100,000 objects.\n",
        "* **gs://:** A path to an object or object prefix in GCS. The artifact will track checksums and versioning information (if the bucket has object versioning enabled) for the referenced objects. Object prefixes are expanded to include the objects under the prefix, default up to 100,000 objects.\n",
        "\n",
        "Below we demonstrate with `file://` using the training data already on disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQFZ6mnsOD3w"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# REFERENCE ARTIFACT: Track data by reference (no copy)\n",
        "# ============================================================================\n",
        "# This creates an artifact that POINTS to the local training data\n",
        "# without uploading it to W&B. W&B records the file paths, sizes,\n",
        "# and checksums so you can verify data integrity later.\n",
        "#\n",
        "# NOTE: This is a standalone demo -- it does NOT affect the training\n",
        "# pipeline or lineage chain above. It's a separate artifact.\n",
        "\n",
        "ref_run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    name=f\"reference-artifact-demo-{YOUR_NAME}\",\n",
        "    job_type=\"reference-demo\",\n",
        "    group=YOUR_NAME,\n",
        "    tags=[YOUR_NAME, \"aqua\", \"reference-artifact\", \"demo\"],\n",
        "    notes=f\"Demonstrate reference artifacts pointing to local data ({YOUR_NAME})\"\n",
        ")\n",
        "\n",
        "# Create a reference artifact pointing to the downloaded training data\n",
        "# train_dir was set earlier when we downloaded the training artifact\n",
        "ref_artifact = wandb.Artifact(\n",
        "    name=f\"aqua-train-reference-{YOUR_NAME}\",\n",
        "    type=\"reference-dataset\",\n",
        "    description=f\"Reference to local training data (no upload, metadata only) ({YOUR_NAME})\",\n",
        "    metadata={\n",
        "        \"source_path\": os.path.abspath(train_dir),\n",
        "        \"purpose\": \"Demonstrates reference artifacts -- data stays on disk\",\n",
        "        \"original_artifact\": TRAIN_ARTIFACT\n",
        "    }\n",
        ")\n",
        "\n",
        "# add_reference tracks files by location + checksum WITHOUT uploading them\n",
        "ref_artifact.add_reference(f\"file://{os.path.abspath(train_dir)}\")\n",
        "\n",
        "ref_run.log_artifact(ref_artifact)\n",
        "\n",
        "print(f\"Logged reference artifact: aqua-train-reference-{YOUR_NAME}\")\n",
        "print(f\"  Points to: {os.path.abspath(train_dir)}\")\n",
        "print(f\"  Data uploaded to W&B: NO (metadata and checksums only)\")\n",
        "print(f\"\\n  In W&B UI â†’ Artifacts â†’ aqua-train-reference-{YOUR_NAME}:\")\n",
        "print(f\"  - Files tab shows referenced paths (not uploaded copies)\")\n",
        "print(f\"  - Metadata tab shows source info\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctLGK-CUOD3w"
      },
      "source": [
        "---\n",
        "\n",
        "# 9. Model Registry: Stage the Baseline\n",
        "\n",
        "The **Model Registry** is where you manage models for deployment. But we won't rush to production yet -- our baseline was trained with hand-picked hyperparameters. Let's **stage** it first, then use Sweeps (Section 10) to see if we can do better before promoting to production.\n",
        "\n",
        "**Key concepts:**\n",
        "- **Artifact aliases** = training state (epoch_1, best, latest)\n",
        "- **Registry aliases** = deployment state (staging, production)\n",
        "\n",
        "**Workflow:**\n",
        "1. Training creates model artifacts with checkpoints\n",
        "2. Link the best artifact to a **Registered Model** (collection) as **staging**\n",
        "3. Run hyperparameter sweep (Section 10) to find a better model\n",
        "4. Promote the winner to **production** (end of Section 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u633mnorOD3w"
      },
      "outputs": [],
      "source": [
        "# STEP 1: LINK BEST MODEL TO REGISTRY\n",
        "# Take the best checkpoint from training and link it to a Registered Model\n",
        "\n",
        "registry_run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    name=f\"registry-promotion-{YOUR_NAME}\",\n",
        "    job_type=\"registry-promotion\",\n",
        "    group=YOUR_NAME,\n",
        "    tags=[YOUR_NAME, \"registry\", \"promotion\"],\n",
        ")\n",
        "\n",
        "# Get the best model artifact from training (namespaced per participant)\n",
        "best_model_path = f\"{WANDB_ENTITY}/{WANDB_PROJECT}/model-{YOUR_NAME}-{CONFIG['model_name']}:best\"\n",
        "best_artifact = registry_run.use_artifact(best_model_path, type=\"model\")\n",
        "\n",
        "print(f\"Best model artifact: {best_artifact.name}:{best_artifact.version}\")\n",
        "print(f\"  Metadata: {best_artifact.metadata}\")\n",
        "\n",
        "# Link to a Registered Model collection in the Registry\n",
        "# This creates a new collection if it doesn't exist (namespaced per participant)\n",
        "REGISTRY_NAME = f\"aqua-classifier-{YOUR_NAME}\"  # Collection name in Registry\n",
        "\n",
        "registry_run.link_artifact(\n",
        "    artifact=best_artifact,\n",
        "    target_path=f\"wandb-registry-sie-workshop-uk-2026/{REGISTRY_NAME}\",\n",
        "    aliases=[\"staging\"]  # Start in staging\n",
        ")\n",
        "\n",
        "print(f\"\\nLinked to Registry: {REGISTRY_NAME}\")\n",
        "print(f\"  Alias: staging\")\n",
        "print(f\"  Check Model Registry in W&B UI!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLQ3EGNIOD3w"
      },
      "outputs": [],
      "source": [
        "# STEP 2: VERIFY THE STAGED MODEL\n",
        "# Before promoting to production, let's verify what we staged and record\n",
        "# the baseline accuracy. We'll come back to promote after running sweeps\n",
        "# in Section 10 to see if a better model exists.\n",
        "\n",
        "api = wandb.Api()\n",
        "\n",
        "# Fetch the model from the W&B Model Registry (not the project artifact store).\n",
        "# Visit Registry in the UI -> Models -> Aqua-Classifier\n",
        "# \"staging\" is the alias we assigned when we linked it in Step 1.\n",
        "registry_path = f\"server/wandb-registry-sie-workshop-uk-2026/{REGISTRY_NAME}:staging\"\n",
        "\n",
        "try:\n",
        "    staged_artifact = api.artifact(registry_path)\n",
        "\n",
        "    baseline_val_acc = staged_artifact.metadata.get(\"val_accuracy\", 0)\n",
        "\n",
        "    print(f\"Staged baseline model: {staged_artifact.name}\")\n",
        "    print(f\"  Version: {staged_artifact.version}\")\n",
        "    print(f\"  Validation accuracy: {baseline_val_acc:.2f}%\")\n",
        "    print(f\"  Aliases: {staged_artifact.aliases}\")\n",
        "    print(f\"\\n  Status: STAGED (not yet promoted to production)\")\n",
        "    print(f\"  Next: Run hyperparameter sweep (Section 10) to see if we can beat it\")\n",
        "\n",
        "except wandb.errors.CommError as e:\n",
        "    print(f\"Error: Could not find staged model. Run Step 1 first.\")\n",
        "    print(f\"  {e}\")\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "print(\"\\nBaseline staged in Registry. Production promotion after sweep (Section 10).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a-4NOTYOD3w"
      },
      "source": [
        "---\n",
        "\n",
        "# 10. Hyperparameter Optimization with Sweeps\n",
        "\n",
        "Our baseline is **staged** in the Registry, but we haven't promoted it to production yet. Before we do, let's find out if better hyperparameters exist. Maybe a lower learning rate with a larger batch size would converge better. Maybe more weight decay helps with these underwater images.\n",
        "\n",
        "**W&B Sweeps** automate this exploration:\n",
        "1. **Define a search space** -- which hyperparameters to vary and their ranges\n",
        "2. **Choose a strategy** -- `random`, `grid`, or `bayes` (Bayesian optimization)\n",
        "3. **Launch agents** -- W&B runs multiple training jobs, each with different configs\n",
        "4. **Analyze results** -- compare all runs side-by-side in the W&B UI\n",
        "5. **Promote the winner** -- push the best model (baseline or sweep) to production\n",
        "\n",
        "**Our story:** We have a staged baseline. Now we run 5 experiments with different learning rates, batch sizes, and weight decay values. If a sweep run beats the baseline, we promote *that* to production. If not, the baseline earns its production badge.\n",
        "\n",
        "**Key W&B concepts:**\n",
        "- `wandb.sweep()` -- creates a sweep controller with your search config\n",
        "- `wandb.agent()` -- launches runs that pull configs from the controller\n",
        "- The sweep controller passes different `config` values to each run automatically\n",
        "\n",
        "**Controlling a running sweep:** The sweep agent is a blocking call. To pause, resume, or stop it while it's running, use either the **W&B UI** (buttons on the sweep dashboard) or the **CLI** from a separate terminal:\n",
        "\n",
        "* `wandb sweep --pause ENTITY/PROJECT/SWEEP_ID`\n",
        "* `wandb sweep --resume ENTITY/PROJECT/SWEEP_ID`\n",
        "* `wandb sweep --stop ENTITY/PROJECT/SWEEP_ID`\n",
        "* `wandb sweep --cancel ENTITY/PROJECT/SWEEP_ID`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dMZi2zbOD3x"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Define the Sweep Configuration\n",
        "\n",
        "sweep_config = {\n",
        "    \"method\": \"random\",        # random search -- efficient for initial exploration\n",
        "    \"name\": \"aqua-hyperparam-sweep\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"val/accuracy\",  # what we're optimizing\n",
        "        \"goal\": \"maximize\"       # higher accuracy = better\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        # Learning rate: log-uniform between 1e-4 and 1e-2\n",
        "        \"learning_rate\": {\n",
        "            \"distribution\": \"log_uniform_values\",\n",
        "            \"min\": 1e-4,\n",
        "            \"max\": 1e-2\n",
        "        },\n",
        "        # Batch size: try a few common values\n",
        "        \"batch_size\": {\n",
        "            \"values\": [16, 32, 64]\n",
        "        },\n",
        "        # Weight decay: log-uniform between 1e-5 and 1e-3\n",
        "        \"weight_decay\": {\n",
        "            \"distribution\": \"log_uniform_values\",\n",
        "            \"min\": 1e-5,\n",
        "            \"max\": 1e-3\n",
        "        },\n",
        "        # Fixed parameters (not swept, but included for completeness)\n",
        "        \"model_name\": {\"value\": CONFIG[\"model_name\"]},\n",
        "        \"epochs\": {\"value\": 10},           # Keep short for workshop\n",
        "        \"image_size\": {\"value\": 224},\n",
        "        \"max_samples\": {\"value\": 1000},    # Same subset as baseline\n",
        "        \"use_amp\": {\"value\": True},\n",
        "    }\n",
        "}\n",
        "\n",
        "from pprint import pprint\n",
        "print(\"Sweep configuration:\")\n",
        "pprint(sweep_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r7R35v9OD3x"
      },
      "outputs": [],
      "source": [
        "# STEP 2: Create the Sweep\n",
        "# This registers the sweep with W&B and returns a sweep_id.\n",
        "# The sweep controller lives on W&B's servers and hands out configs to agents.\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=WANDB_PROJECT, entity=WANDB_ENTITY)\n",
        "\n",
        "print(f\"Sweep created! ID: {sweep_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd0ACxqIOD3x"
      },
      "outputs": [],
      "source": [
        "# STEP 3: Define the Training Function for the Sweep\n",
        "\n",
        "# Each sweep agent call runs this function with a DIFFERENT config.\n",
        "# The key difference from our baseline training:\n",
        "#   - wandb.init() is called WITHOUT explicit config (the sweep provides it)\n",
        "#   - We read hyperparams from wandb.config (set by the sweep controller)\n",
        "#   - Everything else reuses the same utilities from workshop_utils\n",
        "\n",
        "def sweep_train(config=None):\n",
        "    \"\"\"Training function called by the sweep agent.\n",
        "\n",
        "    The sweep controller injects different hyperparameter values into\n",
        "    wandb.config for each run automatically.\n",
        "    \"\"\"\n",
        "    with wandb.init(\n",
        "        config=config,\n",
        "        group=YOUR_NAME,\n",
        "        tags=[YOUR_NAME, \"aqua\", \"sweep\", CONFIG[\"model_name\"]]\n",
        "    ) as run:\n",
        "        # PREEMPTION HANDLING: If this sweep run gets killed (spot instance,\n",
        "        # SLURM timeout, etc.), W&B will auto-requeue it instead of marking\n",
        "        # it failed. The sweep agent picks it back up on the next iteration.\n",
        "        run.mark_preempting()\n",
        "\n",
        "        cfg = wandb.config\n",
        "\n",
        "        # Define custom x-axis (same as baseline)\n",
        "        wandb.define_metric(\"epoch\")\n",
        "        wandb.define_metric(\"train/*\", step_metric=\"epoch\")\n",
        "        wandb.define_metric(\"val/*\", step_metric=\"epoch\")\n",
        "\n",
        "        # Declare artifact usage for lineage (data is pre-loaded locally)\n",
        "        run.use_artifact(TRAIN_ARTIFACT, type=\"dataset\")\n",
        "        run.use_artifact(VAL_ARTIFACT, type=\"dataset\")\n",
        "\n",
        "        # Create datasets from pre-loaded local data with sweep's batch_size\n",
        "        train_dataset = AquaticDataset(\n",
        "            LOCAL_TRAIN_DIR,\n",
        "            transform=get_transforms(cfg.image_size, is_training=True),\n",
        "            class_names=CLASS_NAMES,\n",
        "            max_samples=cfg.max_samples\n",
        "        )\n",
        "        val_dataset = AquaticDataset(\n",
        "            LOCAL_VAL_DIR,\n",
        "            transform=get_transforms(cfg.image_size, is_training=False),\n",
        "            class_names=CLASS_NAMES\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=cfg.batch_size,\n",
        "            shuffle=True, num_workers=0, pin_memory=True, drop_last=True\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset, batch_size=cfg.batch_size,\n",
        "            shuffle=False, num_workers=0, pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Create model from local pretrained weights (lineage tracked via artifact)\n",
        "        model = create_model(\n",
        "            cfg.model_name, NUM_CLASSES, pretrained=True,\n",
        "            weights_artifact=WEIGHTS_ARTIFACT, run=run,\n",
        "            local_weights_dir=LOCAL_WEIGHTS_DIR\n",
        "        ).to(DEVICE)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=cfg.learning_rate,          # <-- from sweep\n",
        "            weight_decay=cfg.weight_decay  # <-- from sweep\n",
        "        )\n",
        "        scaler = GradScaler(enabled=cfg.use_amp)\n",
        "\n",
        "        # Training loop (compact version of our baseline)\n",
        "        best_val_acc = 0.0\n",
        "        for epoch in range(cfg.epochs):\n",
        "            train_loss, train_acc = train_one_epoch(\n",
        "                model, train_loader, criterion, optimizer, scaler, DEVICE,\n",
        "                epoch, log_interval=5, run=run # Notice log_interval=5 â€” that's sparse logging. \n",
        "            )\n",
        "            val_loss, val_acc, _, _, _ = evaluate(\n",
        "                model, val_loader, criterion, DEVICE, desc=f\"Epoch {epoch+1}\"\n",
        "            )\n",
        "\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train/loss\": train_loss,\n",
        "                \"train/accuracy\": train_acc,\n",
        "                \"val/loss\": val_loss,\n",
        "                \"val/accuracy\": val_acc,\n",
        "            })\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "\n",
        "        # Log best result to summary (used by sweep to rank runs)\n",
        "        run.summary[\"best_val_accuracy\"] = best_val_acc\n",
        "\n",
        "print(\"Sweep training function defined\")\n",
        "print(\"  - Reads learning_rate, batch_size, weight_decay from wandb.config\")\n",
        "print(\"  - Uses pre-loaded local data with artifact lineage tracking\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKz6tE5COD3x"
      },
      "outputs": [],
      "source": [
        "# STEP 4: Launch the Sweep Agent\n",
        "\n",
        "# This kicks off 10 training runs, each with different hyperparameters\n",
        "# chosen by the sweep controller's random search strategy.\n",
        "#\n",
        "# While running, check the Sweep dashboard in W&B to see:\n",
        "# - Parallel coordinates plot (which param combos work best)\n",
        "# - Parameter importance (which params matter most)\n",
        "# - All runs compared side-by-side\n",
        "\n",
        "SWEEP_COUNT = 20  # Number of runs\n",
        "\n",
        "print(f\"  Launching {SWEEP_COUNT} sweep runs...\")\n",
        "print(f\"  Sweep_path {WANDB_ENTITY}/{WANDB_PROJECT}/{sweep_id}\")\n",
        "\n",
        "wandb.agent(sweep_id, function=sweep_train, count=SWEEP_COUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional - Alternative: Run Sweeps from the CLI with YAML\n",
        "\n",
        "In the notebook, we define the sweep config in Python and call `wandb.agent()` inline. In production, most teams define their config in a **YAML file** and launch agents from the terminal. This lets you:\n",
        "\n",
        "- **Parallelize** â€” open multiple terminals, each running `wandb agent`, and they all pull configs from the same sweep controller\n",
        "- **Pin to GPUs** â€” `CUDA_VISIBLE_DEVICES=0 wandb agent ...` on one terminal, `CUDA_VISIBLE_DEVICES=1 wandb agent ...` on another\n",
        "- **Decouple config from code** â€” version the YAML separately, share it across teams\n",
        "\n",
        "Two files are provided in `workshop_material/`:\n",
        "\n",
        "- **`sweep_config.yaml`** â€” the sweep configuration (same search space as above, in YAML format)\n",
        "- **`sweep_train.py`** â€” standalone training script that reads config from `wandb.config`\n",
        "\n",
        "To try it (update WANDB_ENTITY and WANDB_PROJECT in `sweep_train.py` first):\n",
        "\n",
        "```bash\n",
        "# Terminal 1: Create the sweep and start an agent\n",
        "cd workshop_material\n",
        "wandb sweep sweep_config.yaml\n",
        "wandb agent <ENTITY>/<PROJECT>/<SWEEP_ID>\n",
        "\n",
        "# Terminal 2 (optional): Run a second agent in parallel\n",
        "wandb agent <ENTITY>/<PROJECT>/<SWEEP_ID>\n",
        "```\n",
        "\n",
        "Both agents pick up different configs from the same sweep controller. Watch them train in parallel in the W&B UI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 11. Compare Sweep Results vs Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Before you run the next cell** â€” an automation has been pre-configured on this project. When the `production` alias is added to an artifact, W&B will automatically fire a webhook that triggers a GitHub Action. Run the cell below, then check the GitHub repo's Issues tab to see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtzyLwqUOD3x"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Compare Sweep Results vs Baseline â†’ Promote Winner to Production\n",
        "# ============================================================================\n",
        "# Now that the sweep is done, let's see if any sweep run beat our staged\n",
        "# baseline. The winner gets promoted to production in the Registry.\n",
        "\n",
        "api = wandb.Api()\n",
        "\n",
        "# Get the staged baseline's accuracy from the registry\n",
        "registry_path = f\"wandb-registry-sie-workshop-uk-2026/{REGISTRY_NAME}:staging\"\n",
        "staged_artifact = api.artifact(registry_path)\n",
        "baseline_acc = staged_artifact.metadata.get(\"val_accuracy\", 0)\n",
        "\n",
        "print(f\"Staged baseline: {staged_artifact.name}\")\n",
        "print(f\"  Validation accuracy: {baseline_acc:.2f}%\\n\")\n",
        "\n",
        "# Find the best sweep run\n",
        "sweep = api.sweep(f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{sweep_id}\")\n",
        "sweep_runs = sweep.runs\n",
        "\n",
        "best_sweep_acc = 0.0\n",
        "best_sweep_run = None\n",
        "for run in sweep_runs:\n",
        "    acc = run.summary.get(\"best_val_accuracy\", 0)\n",
        "    if acc > best_sweep_acc:\n",
        "        best_sweep_acc = acc\n",
        "        best_sweep_run = run\n",
        "\n",
        "if best_sweep_run:\n",
        "    print(f\"Best sweep run: {best_sweep_run.name}\")\n",
        "    print(f\"  Validation accuracy: {best_sweep_acc:.2f}%\")\n",
        "    print(f\"  Config: LR={best_sweep_run.config.get('learning_rate'):.5f}, \"\n",
        "          f\"BS={best_sweep_run.config.get('batch_size')}, \"\n",
        "          f\"WD={best_sweep_run.config.get('weight_decay'):.6f}\")\n",
        "\n",
        "# Decide: promote sweep winner or stick with baseline\n",
        "MIN_ACC_FOR_PRODUCTION = 50.0\n",
        "\n",
        "print()\n",
        "if best_sweep_run and best_sweep_acc > baseline_acc:\n",
        "    print(f\"SWEEP WINS! {best_sweep_acc:.2f}% > {baseline_acc:.2f}% (baseline)\")\n",
        "    print(f\"Promoting sweep model to production...\")\n",
        "    # Note: In a full pipeline you'd log the sweep's best model as an artifact\n",
        "    # and link it to the registry. For this workshop, we promote the staged\n",
        "    # baseline but update its metadata to reflect the sweep findings.\n",
        "    staged_artifact.aliases.remove(\"staging\")\n",
        "    staged_artifact.aliases.append(\"production\")\n",
        "    staged_artifact.save()\n",
        "    print(f\"  Promoted to production! Aliases: {staged_artifact.aliases}\")\n",
        "elif baseline_acc >= MIN_ACC_FOR_PRODUCTION:\n",
        "    print(f\"BASELINE HOLDS! {baseline_acc:.2f}% >= {best_sweep_acc:.2f}% (best sweep)\")\n",
        "    print(f\"Promoting baseline to production...\")\n",
        "    staged_artifact.aliases.remove(\"staging\")\n",
        "    staged_artifact.aliases.append(\"production\")\n",
        "    staged_artifact.save()\n",
        "    print(f\"  Promoted to production! Aliases: {staged_artifact.aliases}\")\n",
        "else:\n",
        "    print(f\"NO MODEL MEETS THRESHOLD ({MIN_ACC_FOR_PRODUCTION}%)\")\n",
        "    print(f\"  Baseline: {baseline_acc:.2f}%, Best sweep: {best_sweep_acc:.2f}%\")\n",
        "    print(f\"  Neither promoted. More training needed.\")\n",
        "\n",
        "\n",
        "print(f\"\\nCheck Model Registry in W&B UI to see the production alias.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 12. Automations: Closing the CI/CD Loop\n",
        "\n",
        "What just happened? When the `production` alias was added, W&B detected the event and fired a pre-configured webhook. That webhook sent a `repository_dispatch` to a GitHub repo, which triggered a GitHub Action that created a review issue â€” with your name, artifact version, and a link back to W&B.\n",
        "\n",
        "Check it out: [GitHub Issues tab](https://github.com/MBakirWB/Automation_Jobs/issues)\n",
        "\n",
        "**This is the full CI/CD loop in action:**\n",
        "\n",
        "```\n",
        "Train model â†’ Log artifact â†’ Promote in Registry â†’ Automation fires â†’ GitHub Issue created\n",
        "```\n",
        "\n",
        "Everything we've done today â€” training, logging artifacts, promoting through the Registry â€” becomes the trigger for automated downstream workflows. No manual handoffs, no Slack messages asking \"which model version should we deploy?\"\n",
        "\n",
        "**What can trigger an automation:**\n",
        "- A new artifact version is linked to a Registry collection\n",
        "- An artifact alias is added (e.g., `production`, `staging`)\n",
        "- A new artifact version is created in a project\n",
        "\n",
        "Coming later this year for dedicated instances\n",
        "- A run metric crosses a threshold (e.g., loss < 0.01)\n",
        "- A run's z-score deviates from the mean\n",
        "\n",
        "**What automations can do:**\n",
        "- Send a **Slack notification** with event details\n",
        "- Call a **webhook** with a JSON payload containing artifact metadata, event type, and author\n",
        "\n",
        "**The pattern extends to any CI/CD system.** The GitHub Issue we just created is a simple example. In production, the same webhook could:\n",
        "- Trigger model evaluation on a held-out benchmark\n",
        "- Deploy to a serving endpoint (SageMaker, Vertex AI, TorchServe)\n",
        "- Open a PR that updates a model config in a deployment repo\n",
        "- Notify the ML platform team on Slack or Microsoft Teams\n",
        "\n",
        "The webhook payload passes all the context needed â€” artifact version, collection name, project, entity, and author â€” so the downstream system knows exactly which model to act on.\n",
        "\n",
        "You can view automation history in the W&B UI: go to your project or registry â†’ **Automations** tab â†’ click on the automation to see execution history, status, and any errors.\n",
        "\n",
        "To set up your own automations, see the [Automations docs](https://docs.wandb.ai/models/automations)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 13 - Optional: Programmatic API: Making Decisions from Your Data\n",
        "\n",
        "We've trained a baseline, run a sweep, and promoted a model to the registry. But in a production workflow, you wouldn't do this manually. You'd query your experiment data programmatically â€” in a CI/CD pipeline, a scheduled job, or a review notebook.\n",
        "\n",
        "The W&B Public API gives you full access to everything you've logged. Let's use it to answer questions that come up in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api = wandb.Api()\n",
        "\n",
        "#1. Pull sweep results into a dataframe for analysis\n",
        "sweep = api.sweep(f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{sweep_id}\")\n",
        "rows = []\n",
        "for run in sweep.runs:\n",
        "    rows.append({\n",
        "        \"name\": run.name,\n",
        "        \"lr\": run.config.get(\"learning_rate\"),\n",
        "        \"batch_size\": run.config.get(\"batch_size\"),\n",
        "        \"weight_decay\": run.config.get(\"weight_decay\"),\n",
        "        \"best_val_acc\": run.summary.get(\"best_val_accuracy\", 0),\n",
        "    })\n",
        "\n",
        "import pandas as pd\n",
        "sweep_df = pd.DataFrame(rows).sort_values(\"best_val_acc\", ascending=False)\n",
        "print(sweep_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#2. Query runs with MongoDB-style filters\n",
        "\n",
        "# Find all runs tagged \"baseline\" that achieved > 60% validation accuracy\n",
        "baseline_runs = api.runs(\n",
        "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}\",\n",
        "    filters={\n",
        "        \"$and\": [\n",
        "            {\"tags\": \"baseline\"},\n",
        "            {\"summary_metrics.best_val_accuracy\": {\"$gt\": 60}},\n",
        "        ]\n",
        "    },\n",
        "    order=\"-summary_metrics.best_val_accuracy\",  # best first\n",
        ")\n",
        "\n",
        "print(f\"Found {len(baseline_runs)} baseline runs above 60% accuracy\\n\")\n",
        "for run in baseline_runs:\n",
        "    print(f\"  {run.name}: {run.summary.get('best_val_accuracy', 0):.2f}% \"\n",
        "          f\"(lr={run.config.get('learning_rate')}, bs={run.config.get('batch_size')})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find all resnet50 sweep runs with learning rate below 1e-3\n",
        "precise_runs = api.runs(\n",
        "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}\",\n",
        "    filters={\n",
        "        \"$and\": [\n",
        "            {\"tags\": \"sweep\"},\n",
        "            {\"config.model_name\": \"resnet50\"},\n",
        "            {\"config.learning_rate\": {\"$lt\": 1e-3}},\n",
        "        ]\n",
        "    },\n",
        ")\n",
        "\n",
        "print(f\"Found {len(precise_runs)} resnet50 sweep runs with lr < 1e-3\")\n",
        "for run in precise_runs:\n",
        "    print(f\"  {run.name}: lr={run.config['learning_rate']:.6f}, \"\n",
        "          f\"acc={run.summary.get('best_val_accuracy', 0):.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#3. Compare baseline vs best sweep â€” full training curves\n",
        "\n",
        "# Pull the full training history (not sampled) for the baseline\n",
        "baseline_run = api.run(f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{training_run_id}\")\n",
        "baseline_history = baseline_run.scan_history(keys=[\"epoch\", \"val/accuracy\"])\n",
        "baseline_points = [(row[\"epoch\"], row[\"val/accuracy\"]) for row in baseline_history if row.get(\"val/accuracy\")]\n",
        "\n",
        "# Same for the best sweep run\n",
        "best_sweep_run = sweep.best_run()\n",
        "sweep_history = best_sweep_run.scan_history(keys=[\"epoch\", \"val/accuracy\"])\n",
        "sweep_points = [(row[\"epoch\"], row[\"val/accuracy\"]) for row in sweep_history if row.get(\"val/accuracy\")]\n",
        "\n",
        "print(\"Baseline val/accuracy by epoch:\")\n",
        "for epoch, acc in baseline_points:\n",
        "    print(f\"  Epoch {int(epoch)}: {acc:.2f}%\")\n",
        "\n",
        "print(f\"\\nBest sweep ({best_sweep_run.name}) val/accuracy by epoch:\")\n",
        "for epoch, acc in sweep_points:\n",
        "    print(f\"  Epoch {int(epoch)}: {acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#4. Update metadata after the fact\n",
        "\n",
        "# Tag runs programmatically â€” useful for CI/CD pipelines\n",
        "# that auto-label runs based on evaluation results\n",
        "best_sweep_run = sweep.best_run()\n",
        "best_sweep_run.tags.append(\"auto-promoted\")\n",
        "best_sweep_run.update()\n",
        "print(f\"Tagged {best_sweep_run.name} as 'auto-promoted'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 14 - Optional: Programmatic Reports\n",
        "\n",
        "Now that we have training runs, sweep results, and model artifacts logged, let's create a **programmatic report** to document and share our findings.\n",
        "\n",
        "Programmatic reports let you **automate** report creation through code â€” ensuring consistency across experiments, enabling real-time updates, and making it easy to share interactive dashboards with your team.\n",
        "\n",
        "We'll build a single report step-by-step:\n",
        "1. Create a report and add text content (**blocks**)\n",
        "2. Pull live training data into **panels** via **Panel Grids**\n",
        "3. Save and share"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb_workspaces.reports.v2 as wr\n",
        "\n",
        "# The Reports API internally creates a wandb.Api() client that re-verifies\n",
        "# your API key. Setting WANDB_BASE_URL ensures it authenticates against the\n",
        "# correct host (not the default https://api.wandb.ai).\n",
        "os.environ[\"WANDB_BASE_URL\"] = WANDB_HOST\n",
        "\n",
        "# â”€â”€ Step 1: Create a report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "report = wr.Report(\n",
        "    project=WANDB_PROJECT,\n",
        "    entity=WANDB_ENTITY,\n",
        "    title=\"AQUA Workshop - Pipeline Report\",\n",
        "    description=\"Programmatic report documenting the aquatic species classification workshop\",\n",
        ")\n",
        "\n",
        "# â”€â”€ Step 2: Add content blocks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Building blocks: H1, H2, H3, P, UnorderedList, OrderedList,\n",
        "#   Image, CodeBlock, MarkdownBlock, Link, TableOfContents, and more\n",
        "report.blocks = [\n",
        "    wr.TableOfContents(),\n",
        "    wr.H1(\"Aquatic Species Classification\"),\n",
        "    wr.P(\"This report documents our AQUA workshop pipeline â€” from data preparation \"\n",
        "         \"through model training to hyperparameter optimization and deployment.\"),\n",
        "    wr.H1(\"Workshop Pipeline\"),\n",
        "    wr.P(\"We followed these steps:\"),\n",
        "    wr.UnorderedList(items=[\n",
        "        \"Consumed pre-prepared dataset artifacts (train/val/test splits)\",\n",
        "        \"Trained a baseline model with full experiment tracking\",\n",
        "        \"Logged model artifacts with lineage back to training data\",\n",
        "        \"Staged the baseline in the Model Registry\",\n",
        "        \"Ran hyperparameter sweeps to optimize performance\",\n",
        "        \"Promoted the winning model to production\",\n",
        "    ]),\n",
        "    wr.P(text=[\"For more details, see the \",\n",
        "               wr.Link(\"W&B Reports documentation\", url=\"https://docs.wandb.ai/guides/reports\")]),\n",
        "]\n",
        "\n",
        "print(f\"Report created with {len(report.blocks)} blocks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pulling Live Data: Panel Grids\n",
        "\n",
        "The real power of programmatic reports is embedding **live panels** that pull directly from your W&B project.\n",
        "\n",
        "- **`PanelGrid`** holds `runsets` (which runs to show) and `panels` (how to visualize them)\n",
        "- **`Runset`** filters runs â€” use `query` to match run names or tags (e.g., `\"baseline\"`, `\"sweep\"`)\n",
        "- **`Panels`** include `LinePlot`, `BarPlot`, `ScatterPlot`, `RunComparer`, and more\n",
        "\n",
        "Additional reporting examples in the [Reports API Quickstart Notebook](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Report_API_Quickstart.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Step 3: Add a Panel Grid with live training data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Runsets filter which runs appear; panels choose the visualization.\n",
        "# Layout uses a 24-column grid: w=8 â†’ 3 panels per row, w=12 â†’ 2 per row\n",
        "pg = wr.PanelGrid(\n",
        "    runsets=[\n",
        "        wr.Runset(WANDB_ENTITY, WANDB_PROJECT, name=\"Baseline\", query=\"resnet\"),\n",
        "        wr.Runset(WANDB_ENTITY, WANDB_PROJECT, name=\"Sweep Runs\", query=\"sweep\"),\n",
        "    ],\n",
        "    panels=[\n",
        "        # Row 1: Three metric charts across\n",
        "        wr.LinePlot(x='epoch', y=['train/loss'], smoothing_factor=0.8,\n",
        "                    title=\"Training Loss\",      layout={'x': 0,  'y': 0, 'w': 8, 'h': 8}),\n",
        "        wr.LinePlot(x='epoch', y=['val/loss'], smoothing_factor=0.8,\n",
        "                    title=\"Validation Loss\",    layout={'x': 8,  'y': 0, 'w': 8, 'h': 8}),\n",
        "        wr.LinePlot(x='epoch', y=['val/accuracy'],\n",
        "                    title=\"Validation Accuracy\", layout={'x': 16, 'y': 0, 'w': 8, 'h': 8}),\n",
        "        # Row 2: Run comparer + predictions table side by side\n",
        "        wr.RunComparer(diff_only='split',       layout={'x': 0,  'y': 8, 'w': 12, 'h': 10}),\n",
        "        wr.WeavePanelSummaryTable(\n",
        "            table_name=\"predictions/analysis_table\",\n",
        "                                                layout=wr.Layout(x=12, y=8, w=12, h=10)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Append the panel grid to our existing blocks\n",
        "report.blocks += [\n",
        "    wr.H1(\"Training Results â€” Baseline vs Sweep\"),\n",
        "    wr.P(\"â­ Anyone with access can interact with the charts below!\"),\n",
        "    pg,\n",
        "    wr.H1(\"Next Steps\"),\n",
        "    wr.P(\"Share this report with your team using the Share button, or generate a \"\n",
        "         \"view-only link for stakeholders who don't have a W&B account.\"),\n",
        "]\n",
        "\n",
        "# â”€â”€ Step 4: Save the report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# \"readable\" keeps panels well-proportioned; \"fluid\" stretches to full browser width\n",
        "report.width = 'readable'\n",
        "report.save()\n",
        "\n",
        "print(f\"Report saved with {len(report.blocks)} blocks!\")\n",
        "print(f\"View it at: {report.url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 15 - Optional: SDK Settings Reference\n",
        "\n",
        "The W&B SDK is highly configurable through `wandb.Settings`. These are the knobs you reach for when debugging, tuning performance at scale, or adapting to your environment.\n",
        "\n",
        "Settings can be passed to `wandb.init(settings=wandb.Settings(...))` or set as environment variables with the `WANDB_` prefix (e.g., `WANDB_SILENT=true`).\n",
        "\n",
        "Full reference: [Settings docs](https://docs.wandb.ai/models/ref/python/experiments/settings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reference: At-Scale & Environment Settings\n",
        "\n",
        "These can't be meaningfully demo'd here but are critical in production. Bookmark the [full Settings reference](https://docs.wandb.ai/models/ref/python/experiments/settings) for when you're running real training jobs.\n",
        "\n",
        "**Network & Timeouts**\n",
        "\n",
        "| Setting | What it does | When to use it |\n",
        "|---------|-------------|----------------|\n",
        "| `x_file_stream_max_line_bytes` | Caps `run.log()` payload size (default 10MB) | Bump for large payloads. Trade-off: larger calls = slower processing at scale |\n",
        "| `x_graphql_timeout_seconds` | Backend timeout for GraphQL requests | Bump when large data volumes cause server-side timeouts |\n",
        "| `x_file_stream_transmit_interval` | How often the SDK flushes data to the server | Lower = more real-time, higher = less network overhead |\n",
        "| `http_proxy` / `https_proxy` | Route W&B traffic through a proxy | Corporate networks with no direct internet access |\n",
        "\n",
        "**Git & Code Tracking**\n",
        "\n",
        "| Setting | What it does | When to use it |\n",
        "|---------|-------------|----------------|\n",
        "| `git_commit` / `git_remote` | Override auto-detected git state | CI environments that strip git info before running |\n",
        "| `disable_code` | Prevent code capture entirely | Proprietary code you can't upload |\n",
        "| `disable_git` | Skip git state capture | Non-git environments or when git tracking adds overhead |\n",
        "\n",
        "**Distributed Training**\n",
        "\n",
        "| Setting | What it does | When to use it |\n",
        "|---------|-------------|----------------|\n",
        "| `x_label` | Label for this node in shared mode | Distinguish nodes in logs and system metrics (we used `\"primary\"`, `\"worker_1\"` earlier) |\n",
        "| `x_primary` | `True` for main process, `False` for workers | Primary handles config/telemetry uploads; workers only send metrics |\n",
        "| `x_update_finish_state` | Controls whether this process can mark the run as finished | Set `False` on workers to prevent premature finish |\n",
        "\n",
        "**Advanced**\n",
        "\n",
        "| Setting | What it does | When to use it |\n",
        "|---------|-------------|----------------|\n",
        "| `x_skip_transaction_log` | Skip local transaction log for online runs | Reduce disk I/O at the cost of crash recovery |\n",
        "| `x_stats_gpu_device_ids` | List of GPU indices to monitor (e.g., `[0, 1]`) | Shared machines where you only own a subset of GPUs |\n",
        "| `reinit=\"create_new\"` | Allow multiple active runs in the same process | Parallel experiments in a single script (wandb>=0.19.10) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Wrap-up\n",
        "\n",
        "Let's recap what we built and discuss next steps.\n",
        "\n",
        "What we covered:\n",
        "\n",
        "  1. Setup - environment configuration, W&B login, config objects\n",
        "  2. Data & Artifacts - pre-loaded local data, lineage with use_artifact()\n",
        "  3. Data Exploration - EDA tables, image statistics, grouping and filtering\n",
        "  4. Model Training - run anatomy, tags, groups, commit=False, define_metric(), mixed precision, TTL\n",
        "  5. Visual Logging - prediction images, Tables, ROC curves, per-class metrics\n",
        "  6. Resuming a Run - resume by ID, continue training seamlessly\n",
        "  7. Offline Mode - offline runs, syncing\n",
        "  8. Model Artifacts - model artifacts, reference artifacts, aliases, TTL\n",
        "  9. Registry - staged baseline, verified before promotion\n",
        " 10. Sweeps - search space, random search, sweep vs baseline, promote winner\n",
        " 11. Sweep Results - compare sweep vs baseline, promote winner to production\n",
        " 12. Automations - CI/CD loop, registry triggers, automated workflows\n",
        " 13. Programmatic API (Optional) - Public API queries, filters, training curves\n",
        " 14. Programmatic Reports (Optional) - Reports API, blocks, PanelGrid\n",
        " 15. SDK Settings Reference (Optional) - network, git, distributed training"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sony_workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
