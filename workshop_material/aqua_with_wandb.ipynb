{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "<!--- @wandbcode{artifacts-fundamentals} -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1po8aF76bGdV"
      },
      "source": [
        "# W&B Workshop: Aquatic Species Classification\n",
        "\n",
        "### From First Baseline to Production-Ready Model with Full MLOps\n",
        "\n",
        "---\n",
        "\n",
        "### Premise\n",
        "\n",
        "**Scenario:** You're part of a marine biology AI research team building an image classifier to identify aquatic species from underwater photographs. The goal is to help marine researchers automatically catalog and monitor marine biodiversity.\n",
        "\n",
        "**Your journey:**\n",
        "1. **Train a baseline model** with experiment tracking and visual diagnostics\n",
        "2. **Package the model** as a versioned artifact with lineage back to the training data\n",
        "3. **Stage the baseline** in the Model Registry, then **sweep** for better hyperparameters\n",
        "4. **Promote the winner** to production based on sweep results\n",
        "\n",
        "---\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "| Section | Topic | Key Skills |\n",
        "|---------|-------|------------|\n",
        "| 1 | Setup | Environment configuration, W&B login, config objects |\n",
        "| 2 | Data & Artifacts | Consume pre-prepared dataset artifacts, lineage with use_artifact |\n",
        "| 3 | Experiment Tracking | Run initialization, logging profiles, step alignment, commit=False |\n",
        "| 4 | Model Training | PyTorch training, mixed precision, checkpoint logging, TTL |\n",
        "| 5 | Visual Logging | Images, tables, ROC curves, per-class metrics |\n",
        "| 6 | Model Artifacts | Model artifact with lineage, reference artifacts, use_artifact |\n",
        "| 7 | Registry | Collections, linking, staging the baseline |\n",
        "| 8 | Sweeps | Hyperparameter optimization, sweep vs baseline, promote winner |\n",
        "| 9 | Offline Mode | WANDB_MODE, syncing runs |\n",
        "| 10 | Programmatic Reports | Reports API, blocks, PanelGrid, automated documentation |\n",
        "| 11 | Wrap-up | Recap, next steps |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smfMb_XbbGdW"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "Let's install dependencies, authenticate with W&B, and configure our experiment.\n",
        "\n",
        "The `workshop_utils.py` file handles all ML boilerplate (transforms, dataset class, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2j3keQQbGdW"
      },
      "outputs": [],
      "source": [
        "# If you haven't already installed dependencies:\n",
        "# !pip install -r ../requirements.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqdSLo5BbGdW"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import random\n",
        "import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import GradScaler\n",
        "import wandb\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Workshop utilities (handles ML boilerplate)\n",
        "from workshop_utils import (\n",
        "    CLASS_NAMES, NUM_CLASSES, DEVICE,\n",
        "    set_seed, get_transforms,\n",
        "    create_model, count_parameters,\n",
        "    train_one_epoch, evaluate,\n",
        "    generate_run_name,\n",
        "    AquaticDataset,\n",
        "    create_dataloaders, create_training_components,\n",
        "    save_checkpoint, log_checkpoint_artifact,\n",
        "    create_prediction_images, create_predictions_table,\n",
        ")\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"W&B: {wandb.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpZVypb0PkZx"
      },
      "source": [
        "## ðŸª„ Log in to W&B\n",
        "- You can explicitly login using `wandb login` or `wandb.login()` (See below)\n",
        "- Alternatively you can set [optional environment variables](https://docs.wandb.ai/guides/track/environment-variables/#optional-environment-variables). There are several env variables which you can set to change the behavior of W&B logging. The most important are:\n",
        "    - `WANDB_API_KEY` - find this in your \"Settings\" section under your profile\n",
        "    - `WANDB_BASE_URL` - this is the url of the W&B server\n",
        "- Find your API Token in \"Prof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NNN1uWabGdW"
      },
      "outputs": [],
      "source": [
        "WANDB_ENTITY = \"m-bakir\"     # W&B team name\n",
        "WANDB_PROJECT = \"SIE-Workshop-2026\"    # Project name\n",
        "\n",
        "# Authenticate with W&B\n",
        "WANDB_HOST = \"https://mbakir.wandb.io/\" \n",
        "wandb.login(host= WANDB_HOST)\n",
        "\n",
        "# app. This has been changed to your private instance\n",
        "# WANDB_HOST = \"<https://<user-defined-full-url>\" \n",
        "\n",
        "# Alternative you can configure this with environment variables:\n",
        "# export WANDB_API_KEY=\"<your-api-key>\"\n",
        "# export WANDB_BASE_URL=\"<your-wandb-endpoint>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92uGEWjzP85B"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Single config object for all hyperparameters and run metadata. You will be assigned a random open source model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5-fMsLAbGdW"
      },
      "outputs": [],
      "source": [
        "# Random model assignment - creates diverse runs for W&B comparison!\n",
        "WORKSHOP_MODELS = [\"resnet50\", \"efficientnet_b0\"]\n",
        "ASSIGNED_MODEL = random.choice(WORKSHOP_MODELS)\n",
        "print(f\"Your assigned model: {ASSIGNED_MODEL}\")\n",
        "\n",
        "# Training config - these get logged to W&B automatically\n",
        "CONFIG = {\n",
        "    \"model_name\": ASSIGNED_MODEL,\n",
        "    \"num_classes\": NUM_CLASSES,\n",
        "    \"epochs\": 3,              # Quick training for workshop\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"image_size\": 224,\n",
        "    \"max_samples\": 1000,      # Subset for fast iteration\n",
        "    \"use_amp\": True,\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "# Set reproducibility\n",
        "set_seed(CONFIG[\"seed\"])\n",
        "print(f\"\\nConfig: {CONFIG}\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy9fJN_pbGdW"
      },
      "source": [
        "## 2. Data Preparation\n",
        "\n",
        "The AQUA dataset has been **pre-prepared, logged as W&B artifacts, and pre-loaded into your local environment**. This mirrors a common production pattern where data lives on shared storage (NFS, S3, a team drive) and teams use W&B to **track and version** it without re-downloading every run.\n",
        "\n",
        "**What's in your local `data/` directory:**\n",
        "- `data/train/` - Training split (~6,500 images, 20 class subfolders)\n",
        "- `data/val/` - Validation split (~800 images)  \n",
        "- `data/test/` - Test split (~800 images)\n",
        "\n",
        "**What's in W&B (same data, versioned as artifacts):**\n",
        "- `aqua-train:v0`, `aqua-val:v0`, `aqua-test:v0`\n",
        "\n",
        "**Key W&B concept:** We call `use_artifact()` in the training run to declare that our run **depends on** these specific dataset versions. This creates **lineage** in W&B -- a graph showing exactly which data trained which model. The actual data is read from local disk; `use_artifact()` handles the tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJzD7T-cbGdX"
      },
      "outputs": [],
      "source": [
        "# W&B ARTIFACT PATHS (for lineage tracking)\n",
        "ARTIFACT_PROJECT = f\"{WANDB_ENTITY}/{WANDB_PROJECT}\"\n",
        "TRAIN_ARTIFACT = f\"{ARTIFACT_PROJECT}/aqua-train:v0\"\n",
        "VAL_ARTIFACT   = f\"{ARTIFACT_PROJECT}/aqua-val:v0\"\n",
        "TEST_ARTIFACT  = f\"{ARTIFACT_PROJECT}/aqua-test:v0\"\n",
        "WEIGHTS_ARTIFACT = f\"{ARTIFACT_PROJECT}/pretrained-{CONFIG['model_name']}:latest\"\n",
        "\n",
        "# Local data paths (pre-loaded in your workshop environment)\n",
        "DATA_ROOT = \"./data\"\n",
        "LOCAL_TRAIN_DIR   = f\"{DATA_ROOT}/train\"\n",
        "LOCAL_VAL_DIR     = f\"{DATA_ROOT}/val\"\n",
        "LOCAL_TEST_DIR    = f\"{DATA_ROOT}/test\"\n",
        "LOCAL_WEIGHTS_DIR = \"./pretrained_weights\"\n",
        "\n",
        "# Verify local data exists\n",
        "for name, path in [(\"Train\", LOCAL_TRAIN_DIR), (\"Val\", LOCAL_VAL_DIR),\n",
        "                   (\"Test\", LOCAL_TEST_DIR), (\"Weights\", LOCAL_WEIGHTS_DIR)]:\n",
        "    status = \"OK\" if os.path.exists(path) else \"MISSING\"\n",
        "    print(f\"  {name}: {path} [{status}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id9m0-v9bGdX"
      },
      "outputs": [],
      "source": [
        "# Includes underwater-optimized augmentations: color jitter, rotation, etc.\n",
        "print(f\"Image transforms ready (size: {CONFIG['image_size']}x{CONFIG['image_size']})\")\n",
        "\n",
        "# Loads images from class folders in downloaded artifacts\n",
        "print(\"AquaticDataset class ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7geOwXwqbGdX"
      },
      "outputs": [],
      "source": [
        "# Section 2 complete - transforms and Dataset class are ready\n",
        "#\n",
        "# The actual data loading happens in the TRAINING RUN (Section 4):\n",
        "# 1. wandb.init() starts the training run\n",
        "# 2. use_artifact() declares dataset dependencies (creates lineage!)\n",
        "# 3. Data is read from local disk (pre-loaded in your environment)\n",
        "# 4. Training proceeds with metrics logged to W&B\n",
        "#\n",
        "# This pattern ensures your training run shows exactly which\n",
        "# dataset versions were used - critical for reproducibility!\n",
        "\n",
        "print(\"Section 2 complete: transforms defined, AquaticDataset ready\")\n",
        "print(\"Data is pre-loaded locally; use_artifact() will track lineage in W&B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14izp4QuQ8D-"
      },
      "source": [
        "## 3. Data Exploration in W&B\n",
        "\n",
        "An EDA table has been prepared for you in the W&B project!\n",
        "\n",
        "Go to: `https://sie.wandb.io/team/SIE-Workshop-2026`. Look for the \"dataset-eda-exploration\" run\n",
        "\n",
        "This run logs a table containing:\n",
        "- Sample images from each class\n",
        "- Image statistics (brightness, contrast, color channels)\n",
        "\n",
        "Use this table to:\n",
        "1. Group by class to inspect samples per species\n",
        "2. Sort by brightness to find dark and bright images\n",
        "3. Filter by blue_ratio to examine underwater color cast\n",
        "4. Compare contrast across marine species\n",
        "5. Use a 2D projection (PCA) to understand global dataset structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZrWCHqtbGdX"
      },
      "source": [
        "## 4. Anatomy of a `Run` ðŸ©º & Model Training\n",
        "\n",
        "\n",
        "The `Run` stores a detailed record of an experiment within a few specific data structures. The important things to know about are\n",
        "- `Run.config` is a dictionary like structure that stores configuration data for a run, like the path to input data or training hyperparameters. You can instatiate the config by passing a dictionary to `wandb.init(config=<config-dict>)`.\n",
        "- `Run.history` is a list of dictionaries that stores historical values of metrics and media over the course of an experiment. We can append a new snapshot of our training metrics by calling `wandb.log(<metric-dict>)`\n",
        "- `Run.summary` is a dictionary for recording summary metrics or media. By default the `summary` will contain the most recent values logged for each metric, you can overwrite and add elements as you like.\n",
        "\n",
        "\n",
        "For our project we will train a baseline model and include\n",
        "\n",
        "**W&B Features in this section:**\n",
        "- `tags` and `group` - Organize runs for filtering and comparison\n",
        "- `notes` - Quick description visible in run overview  \n",
        "- `define_metric()` - Set epoch as x-axis for cleaner charts\n",
        "- `commit=False` - Log metrics from different phases to the same step\n",
        "- `use_artifact()` - Declare data dependencies with automatic lineage tracking\n",
        "- `wandb.alert()` - Get notified when validation improves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7O4UEYsbGdX"
      },
      "outputs": [],
      "source": [
        "# Model will be created inside the training run (next cell)\n",
        "# so we can load pretrained weights from W&B artifacts with lineage.\n",
        "print(f\"Model: {CONFIG['model_name']}\")\n",
        "print(f\"Weights artifact: {WEIGHTS_ARTIFACT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8tS50AmbGdY"
      },
      "outputs": [],
      "source": [
        "# Part 1: Initialize the W&B Run\n",
        "run_name = generate_run_name(CONFIG)\n",
        "\n",
        "run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    name=run_name,\n",
        "    reinit=\"create_new\",\n",
        "    job_type=\"training\",\n",
        "    group=CONFIG[\"model_name\"], # GROUP: Links related runs together (all resnet50 runs grouped)\n",
        "    # TAGS: Filterable labels - find runs by model, dataset, experiment type\n",
        "    tags=[\n",
        "        \"AQUA\",                      # Dataset\n",
        "        \"baseline\",                    # Experiment type\n",
        "        CONFIG[\"model_name\"],          # Model architecture\n",
        "        \"workshop-uk-2026\",            # Workshop identifier\n",
        "        f\"epochs-{CONFIG['epochs']}\",  # Hyperparameter tag\n",
        "    ],\n",
        "    # NOTES: Quick description (visible in run overview)\n",
        "    notes=f\"Workshop training: {CONFIG['model_name']} on AQUA. \"\n",
        "          f\"Epochs: {CONFIG['epochs']}, LR: {CONFIG['learning_rate']}, BS: {CONFIG['batch_size']}\",\n",
        "    config=CONFIG,\n",
        "    # SHARED MODE: allows multiple processes to log to the same run\n",
        "    settings=wandb.Settings(\n",
        "        mode=\"shared\",\n",
        "        x_label=\"primary\",     # Label shown in W&B UI for this node's logs/metrics\n",
        "        x_primary=True,        # This is the primary node (uploads config, telemetry, etc.)\n",
        "    ),\n",
        ")\n",
        "\n",
        "# DEFINE_METRIC: Set \"epoch\" as x-axis for cleaner charts\n",
        "run.define_metric(\"epoch\")\n",
        "run.define_metric(\"train/loss\", step_metric=\"epoch\")\n",
        "run.define_metric(\"train/accuracy\", step_metric=\"epoch\")\n",
        "run.define_metric(\"val/*\", step_metric=\"epoch\")\n",
        "run.define_metric(\"learning_rate\", step_metric=\"epoch\")\n",
        "\n",
        "# Per-batch step metrics: use train/global_step as x-axis\n",
        "# (shared mode does not support the `step=` argument in run.log(),\n",
        "#  so we log the step as a metric and define it as the x-axis here)\n",
        "run.define_metric(\"train/global_step\")\n",
        "run.define_metric(\"train/loss_step\", step_metric=\"train/global_step\")\n",
        "run.define_metric(\"train/acc_step\", step_metric=\"train/global_step\")\n",
        "\n",
        "print(f\"Run: {run_name}\")\n",
        "print(f\"  View at: {run.url}\")\n",
        "print(f\"  Tags: {run.tags}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 2: Load Artifacts + Setup\n",
        "# use_artifact() creates LINEAGE â€” W&B tracks exactly which data trained this model\n",
        "# Data is pre-loaded locally; we call use_artifact() purely for lineage tracking\n",
        "\n",
        "run.use_artifact(TRAIN_ARTIFACT, type='dataset')\n",
        "run.use_artifact(VAL_ARTIFACT,   type='dataset')\n",
        "run.use_artifact(TEST_ARTIFACT,  type='dataset')\n",
        "\n",
        "# Point to pre-loaded local data\n",
        "train_dir, val_dir, test_dir = LOCAL_TRAIN_DIR, LOCAL_VAL_DIR, LOCAL_TEST_DIR\n",
        "\n",
        "# Create model from local pretrained weights (lineage tracked via W&B artifact)\n",
        "model = create_model(\n",
        "    CONFIG[\"model_name\"], NUM_CLASSES, pretrained=True,\n",
        "    weights_artifact=WEIGHTS_ARTIFACT, run=run,\n",
        "    local_weights_dir=LOCAL_WEIGHTS_DIR\n",
        ")\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "train_loader, val_loader, test_loader = create_dataloaders(train_dir, val_dir, test_dir, CONFIG)\n",
        "criterion, optimizer, scheduler, scaler = create_training_components(model, CONFIG)\n",
        "\n",
        "# Keep a reference to the test dataset (needed for visualization in Section 5)\n",
        "test_dataset = AquaticDataset(\n",
        "    test_dir,\n",
        "    transform=get_transforms(CONFIG[\"image_size\"], is_training=False),\n",
        "    class_names=CLASS_NAMES\n",
        ")\n",
        "\n",
        "total_params, trainable_params = count_parameters(model)\n",
        "print(f\"\\nModel: {CONFIG['model_name']} ({total_params:,} params, {trainable_params:,} trainable)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 3: Training Loop (W&B logging)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_model_path = None\n",
        "\n",
        "for epoch in range(CONFIG[\"epochs\"]):\n",
        "    print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model, train_loader, criterion, optimizer, scaler, DEVICE,\n",
        "        epoch, log_interval=1, run=run\n",
        "    )\n",
        "    val_loss, val_acc, val_preds, val_labels, val_probs = evaluate(\n",
        "        model, val_loader, criterion, DEVICE, desc=f\"Epoch {epoch+1} [Val]\"\n",
        "    )\n",
        "    scheduler.step()\n",
        "\n",
        "    # â”€â”€ STEP CONTROL: commit=False keeps all epoch metrics on the SAME step â”€â”€\n",
        "    # Without it, each wandb.log() creates a new step â†’ misaligned charts!\n",
        "    run.log({\"epoch\": epoch + 1, \"train/loss\": train_loss, \"train/accuracy\": train_acc}, commit=False)\n",
        "    run.log({\"val/loss\": val_loss, \"val/accuracy\": val_acc}, commit=False)\n",
        "    run.log({\"learning_rate\": scheduler.get_last_lr()[0]})  # commit=True â†’ step advances\n",
        "\n",
        "    print(f\"  Train: {train_loss:.4f} loss, {train_acc:.2f}% acc\")\n",
        "    print(f\"  Val:   {val_loss:.4f} loss, {val_acc:.2f}% acc\")\n",
        "\n",
        "    # â”€â”€ BEST MODEL TRACKING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_path = f\"best_model_epoch{epoch+1}.pth\"\n",
        "        save_checkpoint(model, optimizer, CONFIG, epoch+1, val_acc, val_loss, best_model_path)\n",
        "\n",
        "        # ALERT: Notification when validation improves\n",
        "        run.alert(\n",
        "            title=\"New Best Model!\",\n",
        "            text=f\"Validation accuracy improved to {val_acc:.2f}% at epoch {epoch+1}\",\n",
        "            level=wandb.AlertLevel.INFO\n",
        "        )\n",
        "        run.summary.update({\"best_val_accuracy\": val_acc, \"best_val_loss\": val_loss, \"best_epoch\": epoch + 1})\n",
        "\n",
        "    # â”€â”€ LOG CHECKPOINT ARTIFACT (versioned, with TTL) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    log_checkpoint_artifact(\n",
        "        run, model, optimizer, CONFIG, epoch+1,\n",
        "        metrics={\"val_accuracy\": val_acc, \"val_loss\": val_loss,\n",
        "                 \"train_accuracy\": train_acc, \"train_loss\": train_loss},\n",
        "        is_best=(val_acc >= best_val_acc),\n",
        "        is_last=(epoch == CONFIG[\"epochs\"] - 1),\n",
        "    )\n",
        "\n",
        "print(f\"\\nTraining complete! Best val accuracy: {best_val_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv5fjEKnOD3w"
      },
      "outputs": [],
      "source": [
        "# TTL IN ACTION: Inspect and Modify Artifact TTL via the API\n",
        "# We set TTL=7 days on checkpoints during training. But what if the \"best\"\n",
        "# checkpoint turns out to be important? You can extend or remove TTL after\n",
        "# the fact using the Public API.\n",
        "\n",
        "ttl_days = 60\n",
        "\n",
        "api = wandb.Api()\n",
        "best_checkpoint_path = f\"{WANDB_ENTITY}/{WANDB_PROJECT}/model-{CONFIG['model_name']}:best\"\n",
        "\n",
        "try:\n",
        "    best_checkpoint = api.artifact(best_checkpoint_path)\n",
        "    print(f\"Best checkpoint: {best_checkpoint.name}:{best_checkpoint.version}\")\n",
        "    print(f\"  Current TTL: {best_checkpoint.ttl}\")\n",
        "\n",
        "    best_checkpoint.ttl = timedelta(days=ttl_days)\n",
        "    best_checkpoint.save()\n",
        "    print(f\"  Updated TTL: {best_checkpoint.ttl}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not fetch artifact (run training first): {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-c6AUV3bGdY"
      },
      "source": [
        "## 5. Visual Logging (Media)\n",
        "\n",
        "Log rich visual diagnostics: underwater images, predictions, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlK3IzY3bGdY"
      },
      "outputs": [],
      "source": [
        "# Final evaluation on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "test_loss, test_acc, test_preds, test_labels, test_probs = evaluate(\n",
        "    model, test_loader, criterion, DEVICE, desc=\"Test Evaluation\"\n",
        ")\n",
        "\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"  Test Loss: {test_loss:.4f}\")\n",
        "print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "run.log({\n",
        "    \"test/loss\": test_loss,\n",
        "    \"test/accuracy\": test_acc\n",
        "})\n",
        "\n",
        "run.summary[\"test_accuracy\"] = test_acc\n",
        "run.summary[\"test_loss\"] = test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8Z3zNwWbGdY"
      },
      "outputs": [],
      "source": [
        "# Log prediction samples with images and confidence scores\n",
        "# create_prediction_images() handles the boilerplate\n",
        "\n",
        "prediction_images = create_prediction_images(\n",
        "    test_dataset, test_preds, test_probs, CLASS_NAMES, n_samples=16\n",
        ")\n",
        "\n",
        "# Log to W&B - images appear in Media tab\n",
        "run.log({\"predictions/samples\": prediction_images})\n",
        "\n",
        "print(f\"Logged {len(prediction_images)} prediction samples to W&B\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrdc-XvdbGdY"
      },
      "outputs": [],
      "source": [
        "# Create W&B Table with predictions for detailed analysis\n",
        "# Includes per-class confidence scores for histogram visualization\n",
        "\n",
        "predictions_table = create_predictions_table(\n",
        "    test_dataset, test_preds, test_probs, CLASS_NAMES, n_samples=100\n",
        ")\n",
        "\n",
        "# Log to W&B - table appears in Tables tab\n",
        "run.log({\"predictions/analysis_table\": predictions_table})\n",
        "\n",
        "print(f\"Logged predictions table with {len(CLASS_NAMES)} class score columns\")\n",
        "print(\"  In W&B UI try:\")\n",
        "print(\"  - Group by 'truth' to see recall per class\")\n",
        "print(\"  - Group by 'guess' to see precision per class\")\n",
        "print(\"  - Filter: row['truth'] != row['guess'] to find errors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCuEgY99bGdY"
      },
      "outputs": [],
      "source": [
        "# Log per-class metrics\n",
        "# Get unique classes that appear in the data (handles fast_run with subset)\n",
        "unique_classes = sorted(set(test_labels) | set(test_preds))\n",
        "\n",
        "precision, recall, f1, support = precision_recall_fscore_support(\n",
        "    test_labels, test_preds, labels=unique_classes, average=None, zero_division=0\n",
        ")\n",
        "\n",
        "metrics_table = wandb.Table(\n",
        "    columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"]\n",
        ")\n",
        "\n",
        "for i, class_idx in enumerate(unique_classes):\n",
        "    class_name = CLASS_NAMES[class_idx] if class_idx < len(CLASS_NAMES) else f\"Class_{class_idx}\"\n",
        "    metrics_table.add_data(\n",
        "        class_name,\n",
        "        round(precision[i], 4),\n",
        "        round(recall[i], 4),\n",
        "        round(f1[i], 4),\n",
        "        int(support[i])\n",
        "    )\n",
        "\n",
        "run.log({\"evaluation/per_class_metrics\": metrics_table})\n",
        "\n",
        "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "    test_labels, test_preds, average='macro'\n",
        ")\n",
        "\n",
        "run.summary[\"precision_macro\"] = precision_macro\n",
        "run.summary[\"recall_macro\"] = recall_macro\n",
        "run.summary[\"f1_macro\"] = f1_macro\n",
        "\n",
        "print(\"\\nPer-class metrics logged.\")\n",
        "print(f\"Macro F1-Score: {f1_macro:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### STOP HERE â€” Shared Mode Demo\n",
        "\n",
        "**Do NOT run the next cell yet.**\n",
        "\n",
        "Your training run is still active. Now open a **new terminal** and run (after updating the run ID in the script):\n",
        "\n",
        "```bash\n",
        "python shared_worker.py\n",
        "```\n",
        "\n",
        "This launches a shared-mode worker that logs to the **same run**. Check the W&B UI to see metrics arriving from both processes.\n",
        "\n",
        "Once the worker finishes, come back and continue running the remaining cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eh9nLIsOD3w"
      },
      "outputs": [],
      "source": [
        "# Log ROC curve using W&B's built-in interactive chart\n",
        "# This creates a one-vs-rest ROC curve for each species class,\n",
        "# showing how well the model discriminates each species from all others.\n",
        "# The chart is fully interactive in the W&B UI: hover, toggle classes, auto-AUC.\n",
        "\n",
        "run.log({\n",
        "    \"evaluation/roc_curve\": wandb.plot.roc_curve(\n",
        "        y_true=test_labels,\n",
        "        y_probas=test_probs,\n",
        "        labels=CLASS_NAMES,\n",
        "        title=\"AQUA Species ROC Curves\"\n",
        "    )\n",
        "})\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Fix ROC curve colors**\n",
        "\n",
        "Notice how all the lines in the ROC chart are the same color (Pink)? That's because W&B colors by *run*, not by *class* â€” and since all 20 species come from a single run, they all inherit the same run color.\n",
        "\n",
        "Vega based custom charts are fully customizable and reusable to give each species its own color:\n",
        "1. Hover over the **AQUA Species ROC Curves** chart and click the ** Gear âš™ icon**\n",
        "2. Select the **Vega spec** tab\n",
        "3. Find lines 91â€“99\n",
        "\n",
        "\n",
        "```json\n",
        "      \"encoding\": {\n",
        "        \"color\": {\n",
        "          \"type\": \"nominal\",\n",
        "          \"field\": \"name\",\n",
        "          \"scale\": {\n",
        "            \"range\": {\n",
        "              \"field\": \"color\"\n",
        "            }\n",
        "          },\n",
        "```\n",
        "\n",
        "to \n",
        "\n",
        "```json\n",
        "      \"encoding\": {\n",
        "        \"color\": {\n",
        "          \"type\": \"nominal\",\n",
        "          \"field\": \"class\",\n",
        "          \"scale\": {\"range\": \"category\"},\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF6Z9u6UbGdY"
      },
      "source": [
        "## 6. Model Artifacts with Lineage\n",
        "\n",
        "We've already logged our **data artifacts** in Section 2:\n",
        "- Raw dataset artifact (`aqua-raw`)\n",
        "- Split artifacts with lineage (`aqua-train`, `aqua-val`, `aqua-test`)\n",
        "\n",
        "Now it's time to complete the lineage by **logging the trained model as an artifact** that references the training data!\n",
        "\n",
        "### Complete Lineage Chain\n",
        "\n",
        "```\n",
        "Raw Dataset â†’ Train/Val/Test Splits â†’ Model\n",
        "```\n",
        "\n",
        "By using `use_artifact()` to consume the split artifacts when logging our model, we create a complete audit trail showing exactly which data was used to train this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j20RWM9UbGdY"
      },
      "outputs": [],
      "source": [
        "# MODEL ARTIFACT - Complete the Lineage Chain\n",
        "\n",
        "# Start a run that consumes the split artifacts and logs the trained model.\n",
        "# This creates the final lineage: Splits â†’ Model\n",
        "\n",
        "model_artifact_run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    name=\"aqua-model-artifact-logging\",\n",
        "    job_type=\"model-logging\",\n",
        "    tags=[\"aqua\", \"model-artifact\", \"baseline\"],\n",
        "    notes=\"Log trained model artifact with lineage to training data splits\"\n",
        ")\n",
        "\n",
        "# Reference the SAME artifacts used during training (creates proper lineage)\n",
        "train_artifact_ref = model_artifact_run.use_artifact(TRAIN_ARTIFACT, type='dataset')\n",
        "val_artifact_ref = model_artifact_run.use_artifact(VAL_ARTIFACT, type='dataset')\n",
        "\n",
        "print(f\"Model trained using:\")\n",
        "print(f\"  Train artifact: {train_artifact_ref.name}:{train_artifact_ref.version}\")\n",
        "print(f\"  Val artifact: {val_artifact_ref.name}:{val_artifact_ref.version}\")\n",
        "\n",
        "# Prepare model info\n",
        "model_info = {\n",
        "    \"architecture\": CONFIG[\"model_name\"],\n",
        "    \"num_classes\": NUM_CLASSES,\n",
        "    \"class_names\": CLASS_NAMES,\n",
        "    \"input_size\": CONFIG[\"image_size\"],\n",
        "    \"pretrained\": True,\n",
        "    \"total_params\": total_params,\n",
        "    \"trainable_params\": trainable_params,\n",
        "    \"training_config\": {\n",
        "        \"epochs\": CONFIG[\"epochs\"],\n",
        "        \"batch_size\": CONFIG[\"batch_size\"],\n",
        "        \"learning_rate\": CONFIG[\"learning_rate\"],\n",
        "        \"optimizer\": \"AdamW\"\n",
        "    },\n",
        "    \"metrics\": {\n",
        "        \"best_val_accuracy\": best_val_acc,\n",
        "        \"test_accuracy\": test_acc,\n",
        "        \"test_loss\": test_loss,\n",
        "        \"f1_macro\": f1_macro\n",
        "    },\n",
        "    \"data_artifacts\": {\n",
        "        \"train\": f\"{train_artifact_ref.name}:{train_artifact_ref.version}\",\n",
        "        \"val\": f\"{val_artifact_ref.name}:{val_artifact_ref.version}\"\n",
        "    },\n",
        "    \"created_at\": datetime.now().isoformat(),\n",
        "    \"training_run_id\": model_artifact_run.id\n",
        "}\n",
        "\n",
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "with open(\"artifacts/model_info.json\", \"w\") as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "\n",
        "import shutil\n",
        "shutil.copy(best_model_path, \"artifacts/model.pth\")\n",
        "\n",
        "# Create and log the model artifact\n",
        "model_artifact = wandb.Artifact(\n",
        "    name=\"aqua-species-classifier\",\n",
        "    type=\"model\",\n",
        "    description=f\"Aquatic Species classifier ({CONFIG['model_name']}) trained on AQUA dataset\",\n",
        "    metadata={\n",
        "        \"architecture\": CONFIG[\"model_name\"],\n",
        "        \"num_classes\": NUM_CLASSES,\n",
        "        \"input_size\": CONFIG[\"image_size\"],\n",
        "        \"final_val_accuracy\": best_val_acc,\n",
        "        \"final_test_accuracy\": test_acc,\n",
        "        \"f1_macro\": f1_macro,\n",
        "        \"framework\": \"pytorch\",\n",
        "        \"domain\": \"marine_biology\",\n",
        "        \"train_artifact\": f\"{train_artifact_ref.name}:{train_artifact_ref.version}\",\n",
        "        \"val_artifact\": f\"{val_artifact_ref.name}:{val_artifact_ref.version}\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Reference artifacts: track files by path + checksum without uploading\n",
        "# (In production you'd use add_file() to upload to W&B storage,\n",
        "#  but we use references here to keep the workshop network-friendly)\n",
        "model_artifact.add_reference(f\"file://{os.path.abspath('artifacts/model.pth')}\")\n",
        "model_artifact.add_reference(f\"file://{os.path.abspath('artifacts/model_info.json')}\")\n",
        "\n",
        "model_artifact_run.log_artifact(model_artifact, aliases=[\"latest\", \"candidate\", \"baseline\"], tags=[\"baseline-model\", \"aqua\", \"marine-biology\"])\n",
        "\n",
        "print(f\"\\nModel artifact logged: aqua-species-classifier (reference, no upload)\")\n",
        "print(f\"  Test Accuracy: {test_acc:.2f}%, Aliases: latest, candidate, baseline\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIgPxnHyOD3w"
      },
      "outputs": [],
      "source": [
        "# COMPLETE ARTIFACT LINEAGE SUMMARY\n",
        "# At this point, we've created the following artifact lineage:\n",
        "\n",
        "print(\"Artifact lineage chain:\")\n",
        "print(\"  aqua-raw (source-of-truth)\")\n",
        "print(\"    -> aqua-train, aqua-val, aqua-test (splits)\")\n",
        "print(\"      -> aqua-species-classifier (model)\")\n",
        "print()\n",
        "print(\"View lineage: W&B UI > Artifacts > select any artifact > Lineage tab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FshFhMgOD3w"
      },
      "source": [
        "### Reference Artifacts: Track External Data Without Copying\n",
        "\n",
        "So far, every artifact we've created **copies** files into W&B storage. But what if your data already lives somewhere else (S3, GCS, a shared filesystem) and you don't want to duplicate it?\n",
        "\n",
        "**Reference artifacts** solve this: they log metadata and checksums *about* external files without uploading the actual data. W&B tracks the location, size, and integrity -- but the bytes stay where they are.\n",
        "\n",
        "Supported URI schemes:\n",
        "* **file://** -- local filesystem\n",
        "* **http(s)://:** A path to a file accessible over HTTP. The artifact will track checksums in the form of etags and size metadata if the HTTP server supports the ETag and Content-Length response headers.\n",
        "* **s3://:** A path to an object or object prefix in S3. The artifact will track checksums and versioning information (if the bucket has object versioning enabled) for the referenced objects. Object prefixes are expanded to include the objects under the prefix, default up to 100,000 objects.\n",
        "* **gs://:** A path to an object or object prefix in GCS. The artifact will track checksums and versioning information (if the bucket has object versioning enabled) for the referenced objects. Object prefixes are expanded to include the objects under the prefix, default up to 100,000 objects.\n",
        "\n",
        "Below we demonstrate with `file://` using the training data already on disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQFZ6mnsOD3w"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# REFERENCE ARTIFACT: Track data by reference (no copy)\n",
        "# ============================================================================\n",
        "# This creates an artifact that POINTS to the local training data\n",
        "# without uploading it to W&B. W&B records the file paths, sizes,\n",
        "# and checksums so you can verify data integrity later.\n",
        "#\n",
        "# NOTE: This is a standalone demo -- it does NOT affect the training\n",
        "# pipeline or lineage chain above. It's a separate artifact.\n",
        "\n",
        "ref_run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    name=\"reference-artifact-demo\",\n",
        "    job_type=\"reference-demo\",\n",
        "    tags=[\"aqua\", \"reference-artifact\", \"demo\"],\n",
        "    notes=\"Demonstrate reference artifacts pointing to local data\"\n",
        ")\n",
        "\n",
        "# Create a reference artifact pointing to the downloaded training data\n",
        "# train_dir was set earlier when we downloaded the training artifact\n",
        "ref_artifact = wandb.Artifact(\n",
        "    name=\"aqua-train-reference\",\n",
        "    type=\"reference-dataset\",\n",
        "    description=\"Reference to local training data (no upload, metadata only)\",\n",
        "    metadata={\n",
        "        \"source_path\": os.path.abspath(train_dir),\n",
        "        \"purpose\": \"Demonstrates reference artifacts -- data stays on disk\",\n",
        "        \"original_artifact\": TRAIN_ARTIFACT\n",
        "    }\n",
        ")\n",
        "\n",
        "# add_reference tracks files by location + checksum WITHOUT uploading them\n",
        "ref_artifact.add_reference(f\"file://{os.path.abspath(train_dir)}\")\n",
        "\n",
        "ref_run.log_artifact(ref_artifact)\n",
        "\n",
        "print(f\"Logged reference artifact: aqua-train-reference\")\n",
        "print(f\"  Points to: {os.path.abspath(train_dir)}\")\n",
        "print(f\"  Data uploaded to W&B: NO (metadata and checksums only)\")\n",
        "print(f\"\\n  In W&B UI â†’ Artifacts â†’ aqua-train-reference:\")\n",
        "print(f\"  - Files tab shows referenced paths (not uploaded copies)\")\n",
        "print(f\"  - Metadata tab shows source info\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctLGK-CUOD3w"
      },
      "source": [
        "## 7. Model Registry: Stage the Baseline\n",
        "\n",
        "The **Model Registry** is where you manage models for deployment. But we won't rush to production yet -- our baseline was trained with hand-picked hyperparameters. Let's **stage** it first, then use Sweeps (Section 8) to see if we can do better before promoting to production.\n",
        "\n",
        "**Key concepts:**\n",
        "- **Artifact aliases** = training state (epoch_1, best, latest)\n",
        "- **Registry aliases** = deployment state (staging, production)\n",
        "\n",
        "**Workflow:**\n",
        "1. Training creates model artifacts with checkpoints\n",
        "2. Link the best artifact to a **Registered Model** (collection) as **staging**\n",
        "3. Run hyperparameter sweep (Section 8) to find a better model\n",
        "4. Promote the winner to **production** (end of Section 8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u633mnorOD3w"
      },
      "outputs": [],
      "source": [
        "# STEP 1: LINK BEST MODEL TO REGISTRY\n",
        "# Take the best checkpoint from training and link it to a Registered Model\n",
        "\n",
        "registry_run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    name=\"registry-promotion\",\n",
        "    job_type=\"registry-promotion\",\n",
        "    tags=[\"registry\", \"promotion\"],\n",
        ")\n",
        "\n",
        "# Get the best model artifact from training\n",
        "best_model_path = f\"{WANDB_ENTITY}/{WANDB_PROJECT}/model-{CONFIG['model_name']}:best\"\n",
        "best_artifact = registry_run.use_artifact(best_model_path, type=\"model\")\n",
        "\n",
        "print(f\"Best model artifact: {best_artifact.name}:{best_artifact.version}\")\n",
        "print(f\"  Metadata: {best_artifact.metadata}\")\n",
        "\n",
        "# Link to a Registered Model collection in the Registry\n",
        "# This creates a new collection if it doesn't exist\n",
        "REGISTRY_NAME = \"aqua-classifier\"  # Collection name in Registry\n",
        "\n",
        "registry_run.link_artifact(\n",
        "    artifact=best_artifact,\n",
        "    target_path=f\"wandb-registry-model/{REGISTRY_NAME}\",\n",
        "    aliases=[\"staging\"]  # Start in staging\n",
        ")\n",
        "\n",
        "print(f\"\\nLinked to Registry: {REGISTRY_NAME}\")\n",
        "print(f\"  Alias: staging\")\n",
        "print(f\"  Check Model Registry in W&B UI!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLQ3EGNIOD3w"
      },
      "outputs": [],
      "source": [
        "# STEP 2: VERIFY THE STAGED MODEL\n",
        "# Before promoting to production, let's verify what we staged and record\n",
        "# the baseline accuracy. We'll come back to promote after running sweeps\n",
        "# in Section 8 to see if a better model exists.\n",
        "\n",
        "api = wandb.Api()\n",
        "\n",
        "# Get the staged model from registry\n",
        "registry_path = f\"{WANDB_ENTITY}/wandb-registry-model/{REGISTRY_NAME}:staging\"\n",
        "\n",
        "try:\n",
        "    staged_artifact = api.artifact(registry_path)\n",
        "\n",
        "    baseline_val_acc = staged_artifact.metadata.get(\"val_accuracy\", 0)\n",
        "\n",
        "    print(f\"Staged baseline model: {staged_artifact.name}\")\n",
        "    print(f\"  Version: {staged_artifact.version}\")\n",
        "    print(f\"  Validation accuracy: {baseline_val_acc:.2f}%\")\n",
        "    print(f\"  Aliases: {staged_artifact.aliases}\")\n",
        "    print(f\"\\n  Status: STAGED (not yet promoted to production)\")\n",
        "    print(f\"  Next: Run hyperparameter sweep (Section 8) to see if we can beat it\")\n",
        "\n",
        "except wandb.errors.CommError as e:\n",
        "    print(f\"Error: Could not find staged model. Run Step 1 first.\")\n",
        "    print(f\"  {e}\")\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "print(\"\\nBaseline staged in Registry. Production promotion after sweep (Section 8).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a-4NOTYOD3w"
      },
      "source": [
        "---\n",
        "\n",
        "## 8. Hyperparameter Optimization with Sweeps\n",
        "\n",
        "Our baseline is **staged** in the Registry, but we haven't promoted it to production yet. Before we do, let's find out if better hyperparameters exist. Maybe a lower learning rate with a larger batch size would converge better. Maybe more weight decay helps with these underwater images.\n",
        "\n",
        "**W&B Sweeps** automate this exploration:\n",
        "1. **Define a search space** -- which hyperparameters to vary and their ranges\n",
        "2. **Choose a strategy** -- `random`, `grid`, or `bayes` (Bayesian optimization)\n",
        "3. **Launch agents** -- W&B runs multiple training jobs, each with different configs\n",
        "4. **Analyze results** -- compare all runs side-by-side in the W&B UI\n",
        "5. **Promote the winner** -- push the best model (baseline or sweep) to production\n",
        "\n",
        "**Our story:** We have a staged baseline. Now we run 5 experiments with different learning rates, batch sizes, and weight decay values. If a sweep run beats the baseline, we promote *that* to production. If not, the baseline earns its production badge.\n",
        "\n",
        "**Key W&B concepts:**\n",
        "- `wandb.sweep()` -- creates a sweep controller with your search config\n",
        "- `wandb.agent()` -- launches runs that pull configs from the controller\n",
        "- The sweep controller passes different `config` values to each run automatically\n",
        "\n",
        "**Controlling a running sweep:** The sweep agent is a blocking call. To pause, resume, or stop it while it's running, use either the **W&B UI** (buttons on the sweep dashboard) or the **CLI** from a separate terminal:\n",
        "\n",
        "* `wandb sweep --pause ENTITY/PROJECT/SWEEP_ID`\n",
        "* `wandb sweep --resume ENTITY/PROJECT/SWEEP_ID`\n",
        "* `wandb sweep --stop ENTITY/PROJECT/SWEEP_ID`\n",
        "* `wandb sweep --cancel ENTITY/PROJECT/SWEEP_ID`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dMZi2zbOD3x"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Define the Sweep Configuration\n",
        "\n",
        "sweep_config = {\n",
        "    \"method\": \"random\",        # random search -- efficient for initial exploration\n",
        "    \"name\": \"aqua-hyperparam-sweep\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"val/accuracy\",  # what we're optimizing\n",
        "        \"goal\": \"maximize\"       # higher accuracy = better\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        # Learning rate: log-uniform between 1e-4 and 1e-2\n",
        "        \"learning_rate\": {\n",
        "            \"distribution\": \"log_uniform_values\",\n",
        "            \"min\": 1e-4,\n",
        "            \"max\": 1e-2\n",
        "        },\n",
        "        # Batch size: try a few common values\n",
        "        \"batch_size\": {\n",
        "            \"values\": [16, 32, 64]\n",
        "        },\n",
        "        # Weight decay: log-uniform between 1e-5 and 1e-3\n",
        "        \"weight_decay\": {\n",
        "            \"distribution\": \"log_uniform_values\",\n",
        "            \"min\": 1e-5,\n",
        "            \"max\": 1e-3\n",
        "        },\n",
        "        # Fixed parameters (not swept, but included for completeness)\n",
        "        \"model_name\": {\"value\": CONFIG[\"model_name\"]},\n",
        "        \"epochs\": {\"value\": 2},           # Keep short for workshop\n",
        "        \"image_size\": {\"value\": 224},\n",
        "        \"max_samples\": {\"value\": 1000},    # Same subset as baseline\n",
        "        \"use_amp\": {\"value\": True},\n",
        "    }\n",
        "}\n",
        "\n",
        "from pprint import pprint\n",
        "print(\"Sweep configuration:\")\n",
        "pprint(sweep_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r7R35v9OD3x"
      },
      "outputs": [],
      "source": [
        "# STEP 2: Create the Sweep\n",
        "# This registers the sweep with W&B and returns a sweep_id.\n",
        "# The sweep controller lives on W&B's servers and hands out configs to agents.\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=WANDB_PROJECT, entity=WANDB_ENTITY)\n",
        "\n",
        "print(f\"Sweep created! ID: {sweep_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd0ACxqIOD3x"
      },
      "outputs": [],
      "source": [
        "# STEP 3: Define the Training Function for the Sweep\n",
        "\n",
        "# Each sweep agent call runs this function with a DIFFERENT config.\n",
        "# The key difference from our baseline training:\n",
        "#   - wandb.init() is called WITHOUT explicit config (the sweep provides it)\n",
        "#   - We read hyperparams from wandb.config (set by the sweep controller)\n",
        "#   - Everything else reuses the same utilities from workshop_utils\n",
        "\n",
        "def sweep_train(config=None):\n",
        "    \"\"\"Training function called by the sweep agent.\n",
        "\n",
        "    The sweep controller injects different hyperparameter values into\n",
        "    wandb.config for each run automatically.\n",
        "    \"\"\"\n",
        "    with wandb.init(\n",
        "        config=config,\n",
        "        group=CONFIG[\"model_name\"],\n",
        "        tags=[\"aqua\", \"sweep\", CONFIG[\"model_name\"]]\n",
        "    ) as run:\n",
        "        cfg = wandb.config\n",
        "\n",
        "        # Define custom x-axis (same as baseline)\n",
        "        wandb.define_metric(\"epoch\")\n",
        "        wandb.define_metric(\"train/*\", step_metric=\"epoch\")\n",
        "        wandb.define_metric(\"val/*\", step_metric=\"epoch\")\n",
        "\n",
        "        # Declare artifact usage for lineage (data is pre-loaded locally)\n",
        "        run.use_artifact(TRAIN_ARTIFACT, type=\"dataset\")\n",
        "        run.use_artifact(VAL_ARTIFACT, type=\"dataset\")\n",
        "\n",
        "        # Create datasets from pre-loaded local data with sweep's batch_size\n",
        "        train_dataset = AquaticDataset(\n",
        "            LOCAL_TRAIN_DIR,\n",
        "            transform=get_transforms(cfg.image_size, is_training=True),\n",
        "            class_names=CLASS_NAMES,\n",
        "            max_samples=cfg.max_samples\n",
        "        )\n",
        "        val_dataset = AquaticDataset(\n",
        "            LOCAL_VAL_DIR,\n",
        "            transform=get_transforms(cfg.image_size, is_training=False),\n",
        "            class_names=CLASS_NAMES\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=cfg.batch_size,\n",
        "            shuffle=True, num_workers=0, pin_memory=True, drop_last=True\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset, batch_size=cfg.batch_size,\n",
        "            shuffle=False, num_workers=0, pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Create model from local pretrained weights (lineage tracked via artifact)\n",
        "        model = create_model(\n",
        "            cfg.model_name, NUM_CLASSES, pretrained=True,\n",
        "            weights_artifact=WEIGHTS_ARTIFACT, run=run,\n",
        "            local_weights_dir=LOCAL_WEIGHTS_DIR\n",
        "        ).to(DEVICE)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=cfg.learning_rate,          # <-- from sweep\n",
        "            weight_decay=cfg.weight_decay  # <-- from sweep\n",
        "        )\n",
        "        scaler = GradScaler(enabled=cfg.use_amp)\n",
        "\n",
        "        # Training loop (compact version of our baseline)\n",
        "        best_val_acc = 0.0\n",
        "        for epoch in range(cfg.epochs):\n",
        "            train_loss, train_acc = train_one_epoch(\n",
        "                model, train_loader, criterion, optimizer, scaler, DEVICE,\n",
        "                epoch, log_interval=5, run=run\n",
        "            )\n",
        "            val_loss, val_acc, _, _, _ = evaluate(\n",
        "                model, val_loader, criterion, DEVICE, desc=f\"Epoch {epoch+1}\"\n",
        "            )\n",
        "\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train/loss\": train_loss,\n",
        "                \"train/accuracy\": train_acc,\n",
        "                \"val/loss\": val_loss,\n",
        "                \"val/accuracy\": val_acc,\n",
        "            })\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "\n",
        "        # Log best result to summary (used by sweep to rank runs)\n",
        "        run.summary[\"best_val_accuracy\"] = best_val_acc\n",
        "\n",
        "print(\"Sweep training function defined\")\n",
        "print(\"  - Reads learning_rate, batch_size, weight_decay from wandb.config\")\n",
        "print(\"  - Uses pre-loaded local data with artifact lineage tracking\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKz6tE5COD3x"
      },
      "outputs": [],
      "source": [
        "# STEP 4: Launch the Sweep Agent\n",
        "\n",
        "# This kicks off 10 training runs, each with different hyperparameters\n",
        "# chosen by the sweep controller's random search strategy.\n",
        "#\n",
        "# While running, check the Sweep dashboard in W&B to see:\n",
        "# - Parallel coordinates plot (which param combos work best)\n",
        "# - Parameter importance (which params matter most)\n",
        "# - All runs compared side-by-side\n",
        "\n",
        "SWEEP_COUNT = 10  # Number of runs\n",
        "\n",
        "print(f\"  Launching {SWEEP_COUNT} sweep runs...\")\n",
        "print(f\"  Sweep_path {WANDB_ENTITY}/{WANDB_PROJECT}/{sweep_id}\")\n",
        "\n",
        "wandb.agent(sweep_id, function=sweep_train, count=SWEEP_COUNT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtzyLwqUOD3x"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 5: Compare Sweep Results vs Baseline â†’ Promote Winner to Production\n",
        "# ============================================================================\n",
        "# Now that the sweep is done, let's see if any sweep run beat our staged\n",
        "# baseline. The winner gets promoted to production in the Registry.\n",
        "\n",
        "api = wandb.Api()\n",
        "\n",
        "# Get the staged baseline's accuracy from the registry\n",
        "registry_path = f\"wandb-registry-model/{REGISTRY_NAME}:staging\"\n",
        "staged_artifact = api.artifact(registry_path)\n",
        "baseline_acc = staged_artifact.metadata.get(\"val_accuracy\", 0)\n",
        "\n",
        "print(f\"Staged baseline: {staged_artifact.name}\")\n",
        "print(f\"  Validation accuracy: {baseline_acc:.2f}%\\n\")\n",
        "\n",
        "# Find the best sweep run\n",
        "sweep = api.sweep(f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{sweep_id}\")\n",
        "sweep_runs = sweep.runs\n",
        "\n",
        "best_sweep_acc = 0.0\n",
        "best_sweep_run = None\n",
        "for run in sweep_runs:\n",
        "    acc = run.summary.get(\"best_val_accuracy\", 0)\n",
        "    if acc > best_sweep_acc:\n",
        "        best_sweep_acc = acc\n",
        "        best_sweep_run = run\n",
        "\n",
        "if best_sweep_run:\n",
        "    print(f\"Best sweep run: {best_sweep_run.name}\")\n",
        "    print(f\"  Validation accuracy: {best_sweep_acc:.2f}%\")\n",
        "    print(f\"  Config: LR={best_sweep_run.config.get('learning_rate'):.5f}, \"\n",
        "          f\"BS={best_sweep_run.config.get('batch_size')}, \"\n",
        "          f\"WD={best_sweep_run.config.get('weight_decay'):.6f}\")\n",
        "\n",
        "# Decide: promote sweep winner or stick with baseline\n",
        "MIN_ACC_FOR_PRODUCTION = 50.0\n",
        "\n",
        "print()\n",
        "if best_sweep_run and best_sweep_acc > baseline_acc:\n",
        "    print(f\"SWEEP WINS! {best_sweep_acc:.2f}% > {baseline_acc:.2f}% (baseline)\")\n",
        "    print(f\"Promoting sweep model to production...\")\n",
        "    # Note: In a full pipeline you'd log the sweep's best model as an artifact\n",
        "    # and link it to the registry. For this workshop, we promote the staged\n",
        "    # baseline but update its metadata to reflect the sweep findings.\n",
        "    staged_artifact.aliases.append(\"production\")\n",
        "    staged_artifact.save()\n",
        "    print(f\"  Promoted to production! Aliases: {staged_artifact.aliases}\")\n",
        "elif baseline_acc >= MIN_ACC_FOR_PRODUCTION:\n",
        "    print(f\"BASELINE HOLDS! {baseline_acc:.2f}% >= {best_sweep_acc:.2f}% (best sweep)\")\n",
        "    print(f\"Promoting baseline to production...\")\n",
        "    staged_artifact.aliases.remove(\"staging\")\n",
        "    staged_artifact.aliases.append(\"production\")\n",
        "    staged_artifact.save()\n",
        "    print(f\"  Promoted to production! Aliases: {staged_artifact.aliases}\")\n",
        "else:\n",
        "    print(f\"NO MODEL MEETS THRESHOLD ({MIN_ACC_FOR_PRODUCTION}%)\")\n",
        "    print(f\"  Baseline: {baseline_acc:.2f}%, Best sweep: {best_sweep_acc:.2f}%\")\n",
        "    print(f\"  Neither promoted. More training needed.\")\n",
        "\n",
        "print(f\"\\nCheck Model Registry in W&B UI to see the production alias.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QErW_40GbGdc"
      },
      "source": [
        "## 10. Offline Runs ðŸ“¶\n",
        "\n",
        "If you're training on an offline machine and want to upload your results to our servers afterwards, we have a feature for you!\n",
        "\n",
        "1. When executing a wandb run, set `mode=\"offline\"` to save the metrics locally, no internet required.\n",
        "\n",
        "2. When you're ready, run wandb init in your directory to set the project name.\n",
        "Run wandb sync `YOUR_RUN_DIRECTORY` to push the metrics to your instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq8K2vfdbGdc"
      },
      "outputs": [],
      "source": [
        "offline_run = wandb.init(\n",
        "    entity = WANDB_ENTITY,\n",
        "    project = WANDB_PROJECT,\n",
        "    name=  f\"exp_run_offline_mode\",\n",
        "    mode = \"offline\"\n",
        ")\n",
        "\n",
        "for i in range(10):\n",
        "  wandb.log({\"offline-metric\": i})\n",
        "\n",
        "offline_run.finish()\n",
        "\n",
        "# After the run, sync it with:\n",
        "# !wandb sync run_file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 11. Programmatic Reports\n",
        "\n",
        "Now that we have training runs, sweep results, and model artifacts logged, let's create a **programmatic report** to document and share our findings.\n",
        "\n",
        "Programmatic reports let you **automate** report creation through code â€” ensuring consistency across experiments, enabling real-time updates, and making it easy to share interactive dashboards with your team.\n",
        "\n",
        "We'll build a single report step-by-step:\n",
        "1. Create a report and add text content (**blocks**)\n",
        "2. Pull live training data into **panels** via **Panel Grids**\n",
        "3. Save and share"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb_workspaces.reports.v2 as wr\n",
        "\n",
        "# The Reports API internally creates a wandb.Api() client that re-verifies\n",
        "# your API key. Setting WANDB_BASE_URL ensures it authenticates against the\n",
        "# correct host (not the default https://api.wandb.ai).\n",
        "os.environ[\"WANDB_BASE_URL\"] = WANDB_HOST\n",
        "\n",
        "# â”€â”€ Step 1: Create a report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "report = wr.Report(\n",
        "    project=WANDB_PROJECT,\n",
        "    entity=WANDB_ENTITY,\n",
        "    title=\"AQUA Workshop - Pipeline Report\",\n",
        "    description=\"Programmatic report documenting the aquatic species classification workshop\",\n",
        ")\n",
        "\n",
        "# â”€â”€ Step 2: Add content blocks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Building blocks: H1, H2, H3, P, UnorderedList, OrderedList,\n",
        "#   Image, CodeBlock, MarkdownBlock, Link, TableOfContents, and more\n",
        "report.blocks = [\n",
        "    wr.TableOfContents(),\n",
        "    wr.H1(\"Aquatic Species Classification\"),\n",
        "    wr.P(\"This report documents our AQUA workshop pipeline â€” from data preparation \"\n",
        "         \"through model training to hyperparameter optimization and deployment.\"),\n",
        "    wr.H1(\"Workshop Pipeline\"),\n",
        "    wr.P(\"We followed these steps:\"),\n",
        "    wr.UnorderedList(items=[\n",
        "        \"Consumed pre-prepared dataset artifacts (train/val/test splits)\",\n",
        "        \"Trained a baseline model with full experiment tracking\",\n",
        "        \"Logged model artifacts with lineage back to training data\",\n",
        "        \"Staged the baseline in the Model Registry\",\n",
        "        \"Ran hyperparameter sweeps to optimize performance\",\n",
        "        \"Promoted the winning model to production\",\n",
        "    ]),\n",
        "    wr.P(text=[\"For more details, see the \",\n",
        "               wr.Link(\"W&B Reports documentation\", url=\"https://docs.wandb.ai/guides/reports\")]),\n",
        "]\n",
        "\n",
        "print(f\"Report created with {len(report.blocks)} blocks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pulling Live Data: Panel Grids\n",
        "\n",
        "The real power of programmatic reports is embedding **live panels** that pull directly from your W&B project.\n",
        "\n",
        "- **`PanelGrid`** holds `runsets` (which runs to show) and `panels` (how to visualize them)\n",
        "- **`Runset`** filters runs â€” use `query` to match run names or tags (e.g., `\"baseline\"`, `\"sweep\"`)\n",
        "- **`Panels`** include `LinePlot`, `BarPlot`, `ScatterPlot`, `RunComparer`, and more\n",
        "\n",
        "Additional reporting examples in the [Reports API Quickstart Notebook](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Report_API_Quickstart.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Step 3: Add a Panel Grid with live training data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Runsets filter which runs appear; panels choose the visualization.\n",
        "# Layout uses a 24-column grid: w=8 â†’ 3 panels per row, w=12 â†’ 2 per row\n",
        "pg = wr.PanelGrid(\n",
        "    runsets=[\n",
        "        wr.Runset(WANDB_ENTITY, WANDB_PROJECT, name=\"Baseline\", query=\"resnet\"),\n",
        "        wr.Runset(WANDB_ENTITY, WANDB_PROJECT, name=\"Sweep Runs\", query=\"sweep\"),\n",
        "    ],\n",
        "    panels=[\n",
        "        # Row 1: Three metric charts across\n",
        "        wr.LinePlot(x='epoch', y=['train/loss'], smoothing_factor=0.8,\n",
        "                    title=\"Training Loss\",      layout={'x': 0,  'y': 0, 'w': 8, 'h': 8}),\n",
        "        wr.LinePlot(x='epoch', y=['val/loss'], smoothing_factor=0.8,\n",
        "                    title=\"Validation Loss\",    layout={'x': 8,  'y': 0, 'w': 8, 'h': 8}),\n",
        "        wr.LinePlot(x='epoch', y=['val/accuracy'],\n",
        "                    title=\"Validation Accuracy\", layout={'x': 16, 'y': 0, 'w': 8, 'h': 8}),\n",
        "        # Row 2: Run comparer + predictions table side by side\n",
        "        wr.RunComparer(diff_only='split',       layout={'x': 0,  'y': 8, 'w': 12, 'h': 10}),\n",
        "        wr.WeavePanelSummaryTable(\n",
        "            table_name=\"predictions/analysis_table\",\n",
        "                                                layout=wr.Layout(x=12, y=8, w=12, h=10)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Append the panel grid to our existing blocks\n",
        "report.blocks += [\n",
        "    wr.H1(\"Training Results â€” Baseline vs Sweep\"),\n",
        "    wr.P(\"â­ Anyone with access can interact with the charts below!\"),\n",
        "    pg,\n",
        "    wr.H1(\"Next Steps\"),\n",
        "    wr.P(\"Share this report with your team using the Share button, or generate a \"\n",
        "         \"view-only link for stakeholders who don't have a W&B account.\"),\n",
        "]\n",
        "\n",
        "# â”€â”€ Step 4: Save the report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# \"readable\" keeps panels well-proportioned; \"fluid\" stretches to full browser width\n",
        "report.width = 'readable'\n",
        "report.save()\n",
        "\n",
        "print(f\"Report saved with {len(report.blocks)} blocks!\")\n",
        "print(f\"View it at: {report.url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Wrap-up\n",
        "\n",
        "Let's recap what we built and discuss next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\"\"AQUA WORKSHOP COMPLETE\n",
        "\n",
        "What we covered:\n",
        "\n",
        " 1. Data Pipeline - artifact consumption, lineage with use_artifact()\n",
        " 2. Experiment Tracking - naming, tags, groups, commit=False, define_metric()\n",
        " 3. Model Training - mixed precision, checkpoint artifacts with TTL, alerts\n",
        " 4. Visual Logging - prediction images, Tables, ROC curves, per-class metrics\n",
        " 5. Artifacts & Lineage - model artifacts, reference artifacts, aliases, TTL\n",
        " 6. Registry - staged baseline, verified before promotion\n",
        " 7. Sweeps - search space, random search, sweep vs baseline, promote winner\n",
        " 8. Offline Mode - offline runs, syncing\n",
        " 9. Programmatic Reports - Reports API, blocks, PanelGrid, automated documentation\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sony_workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
