{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he8InAMLqYjr"
      },
      "source": [
        "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "<!--- @wandbcode{artifacts-fundamentals} -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTTuUs_AqYjr"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgKADHIRwWA-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnJOcJHu7_c9"
      },
      "source": [
        "## ðŸª„ Install `wandb` library and login\n",
        "\n",
        "\n",
        "Start by installing the library and logging in to your free account.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVpSalRK7_c-"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E83xgBm8yjn"
      },
      "source": [
        "## Log in to W&B\n",
        "- You can explicitly login using `wandb login` or `wandb.login()` (See below)\n",
        "- Alternatively you can set environment variables. There are several env variables which you can set to change the behavior of W&B logging. The most important are:\n",
        "    - `WANDB_API_KEY` - find this in your \"Settings\" section under your profile\n",
        "    - `WANDB_BASE_URL` - this is the url of the W&B server\n",
        "- Find your API Token in \"Profile\" -> \"Setttings\" in the W&B App\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuOsR1MZ82tr"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "WANDB_ENTITY = None #@param #Entity can be your personal entity or point to a team you are a part of!\n",
        "WANDB_PROJECT = \"wandb-artifacts-registry\" #@param\n",
        "WANDB_HOST= None #@param\n",
        "YOUR_NAME = None #We will use this for our filtering and grouping to make it easy for your to identify your runs in the project\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moQHLuZ-7_c-"
      },
      "outputs": [],
      "source": [
        "\n",
        "#wandb.login(host=WANDB_HOST) #or wandb.login(key=<key>, host=<host>)\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4rjPwCKRgDT"
      },
      "source": [
        "#Artifacts\n",
        "\n",
        "Use W&B Artifacts to track and version data as the inputs and outputs of your W&B Runs. In addition to logging hyperparameters, metadata, and metrics to a run, you can use an artifact to log the dataset used to train the model as input and the resulting model checkpoints as outputs.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1i8L9OxTwtIKCA8beTN8u_jVQzIi1ga4y\" alt=\"Artifacts Simple\" width=\"700\" height=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbpEARWxqYjs"
      },
      "source": [
        "## Create a Dataset\n",
        "Let's create some datasets that we can work with in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNgMojz5qYjs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "directory = \"dataset\"\n",
        "os.makedirs(directory, exist_ok=True)\n",
        "file1, file2 = os.path.join(directory, \"file1.csv\"), os.path.join(directory, \"file2.csv\")\n",
        "\n",
        "def generate_dummy_data(num_samples):\n",
        "    data = [\n",
        "        np.random.normal(50, 10, num_samples),\n",
        "        np.random.randint(1, 100, num_samples),\n",
        "        np.random.choice(['A', 'B', 'C', 'D'], num_samples),\n",
        "        np.random.uniform(0.0, 1.0, num_samples)\n",
        "    ]\n",
        "    return zip(*data)\n",
        "\n",
        "def save_to_csv(file, data):\n",
        "    with open(file, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['feature1', 'feature2', 'feature3', 'feature4'])\n",
        "        writer.writerows(data)\n",
        "\n",
        "num_samples = 100\n",
        "save_to_csv(file1, generate_dummy_data(num_samples))\n",
        "save_to_csv(file2, generate_dummy_data(num_samples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkP8o870qYjt"
      },
      "source": [
        "## Create An Artifact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvDQcwb0qYjt"
      },
      "source": [
        "The general workflow for creating an Artifact is:\n",
        "\n",
        "\n",
        "1.   Intialize a run.\n",
        "2.   Create an Artifact.\n",
        "3.   Add a any files or directories to the new Artifact that you want to track and version.\n",
        "4.   Log the artifact in the W&B platform.\n",
        "\n",
        "The most straightforward way of accomplishing this is the second line of code in the example below, which will log, track and version a new dataset (i.e. do points 2, 3, and 4 above in one step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymc0pv8HqYjt"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT)\n",
        "run.log_artifact(artifact_or_path=f\"{directory}/file1.csv\", name=f\"my_first_artifact_{YOUR_NAME}\", type=\"dataset\")\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYBtUb_sqYjt"
      },
      "source": [
        "In the second line we log the Artifact with [`run.log_artifact()`](https://docs.wandb.ai/ref/python/public-api/run#log_artifact). In this example, we use three common arguments to the function.\n",
        "1. With `artifact_or_path` we specifiy the path to where the data we want to version exists. Any file or directory can be added here.\n",
        "2. with `name` we give the artifact a name within Weights & Biases that we will use to access it.\n",
        "3. With `type` we give the artifact a higher level grouping. For example, we may have multiple artifacts of type data, and multiple artifacts of type model.\n",
        "\n",
        "\n",
        "See the [Artifacts Reference](https://docs.wandb.ai/ref/python/artifact) guide for more information and other commonly used arguments, including how to store additional metadata.\n",
        "\n",
        "Each time the above `log_artifact` is executed, wandb will create a new version of the Artifact within Weights & Biases if the underlying data has changed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lFKSmLQqYjt"
      },
      "source": [
        "An alternative approach that offers more control (at the expense of more lines of code) can be seen below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SXySBUUqYjt"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT)\n",
        "\n",
        "artifact = wandb.Artifact(f\"my_first_artifact_{YOUR_NAME}\", type=\"dataset\")\n",
        "# the below will add two individual files to the artifact.\n",
        "artifact.add_file(local_path=f\"{directory}/file1.csv\")\n",
        "artifact.add_file(local_path=f\"{directory}/file2.csv\")\n",
        "# or the below if you wanted to add the entire directory contents.\n",
        "artifact.add_dir(local_path=f\"{directory}\")\n",
        "# explictly log the artifact to Weights & Biases.\n",
        "run.log_artifact(artifact)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-Pkqhj-qYjt"
      },
      "source": [
        "In the above example, lines 3-5 will create a new Artifact within your Weights & Biases project. With the resulting artifact object, you can call the [`artifact.add_file`](https://docs.wandb.ai/ref/python/artifact#add_file) or [`artifact.add_dir`](https://docs.wandb.ai/ref/python/artifact#add_dir) functions in order to add as many files and directories to the Artifact as you want. Once added, the artifact must then be explictly logged to Weights & Biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKHU2t0LqYjt"
      },
      "source": [
        "## Use an Artifact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg_rpCU6qYjt"
      },
      "source": [
        "When you want to use a specific version of an Artifact in a downstream task, you can specify the specific version you would like to use via either `v0`, `v1`, `v2` and so on, or via specific aliases you may have added. The `latest` alias always refers to the most recent version of the Artifact logged.\n",
        "\n",
        "The proceeding code snippet specifies that the W&B Run will use an artifact called `my_first_artifact` with the alias `latest`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJXibo-wqYjt"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT)\n",
        "artifact = run.use_artifact(artifact_or_name=f\"my_first_artifact_{YOUR_NAME}:latest\") # this creates a reference within Weights & Biases that this artifact was used by this run.\n",
        "path = artifact.download() # this downloads the artifact from Weights & Biases to your local system where the code is executing.\n",
        "print(f\"Data directory located at {path}\")\n",
        "#run training with the downloaded artifact\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X89IpGGZqYjt"
      },
      "source": [
        "For more information on ways to customize your Artifact download, including via the command line, see the [Download and Usage guide](https://docs.wandb.ai/guides/artifacts/download-and-use-an-artifact)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNMS3N_CqYjt"
      },
      "source": [
        "## Create a new Artifact version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBYMZ3pPqYjt"
      },
      "source": [
        "Let's say we want to modify our dataset while also tracking and versioning these changes. In the below example we will subsample our dataset and save it as a new file. We will use the [Pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) library to read our CSV file.\n",
        "\n",
        "In the second block of code we will log it to Weights & Biases under the same Artifact name (*my_first_artifact*) so that Weights & Biases knows that this is a new version of an existing artifact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3BJ_DK_qYjt"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "df = pandas.read_csv(f\"{directory}/file1.csv\")\n",
        "# subsample to 50% of the original size\n",
        "df_subsampled = df.sample(frac=0.5, random_state=1)\n",
        "# save the subsampled dataframe to a new file.\n",
        "df_subsampled.to_csv(f\"{directory}/file1.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPa9RtKnqYjt"
      },
      "source": [
        "Now we have a new subsampled version of our dataset locally, we can log the new version to Weights & Biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmo35xGJqYjt"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT)\n",
        "run.log_artifact(artifact_or_path=f\"{directory}/file1.csv\", name=f\"my_first_artifact_{YOUR_NAME}\", type=\"dataset\", aliases =[\"subsampled\"])\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dns2gOUqYjt"
      },
      "source": [
        "Now the sampled dataset will be logged to the `my_first_artifact_<name>` Artifact as a new version.\n",
        "\n",
        "The Artifact has also been given a custom `alias`, which is a unique label for this Artifact version. While the `alias` is currently `subsampled`, the default aliases is `vN`, where `N` is the number of versions the Artifact has. This increments automatically. You can always access specific versions of an Artifact by using an alias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC4MsIZ6qYjt"
      },
      "source": [
        "## Update Artifact version metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lIkduClqYjt"
      },
      "source": [
        "You can update the `description`, `metadata`, and `alias` of an artifact on the W&B platform during or outside a W&B Run.\n",
        "\n",
        "\n",
        "This example changes the `description` of the `my_first_artifact` artifact inside a run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COJ9dfI0qYjt"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT)\n",
        "artifact = run.use_artifact(artifact_or_name=f\"my_first_artifact_{YOUR_NAME}:subsampled\")\n",
        "artifact.description = \"This is an edited description.\"\n",
        "artifact.metadata = {\"source\": \"local disk\", \"internal data owner\": \"platform team\"}\n",
        "artifact.save()  # persists changes to an Artifact's properties\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDAWR5HoqYjt"
      },
      "source": [
        "## Use the Artifact within your pipelines\n",
        "Once the artifact is tracked and versioned within Weights & Biases it's now easy to integrate it into your ML workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3z5P5g5qYju"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT)\n",
        "artifact = run.use_artifact(artifact_or_name=f\"my_first_artifact_{YOUR_NAME}:latest\")\n",
        "# the below is left as an exercise to the reader\n",
        "# train model\n",
        "# log model as artifact\n",
        "run.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5EuJrAaqYju"
      },
      "source": [
        "## Navigate the Artifacts UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF7Ob0lYqYju"
      },
      "source": [
        "You can also manage your Artifacts via the W&B platform. This can give you insight into your model's performance or dataset versioning. To navigate to the relevant information, click this [link](https://wandb.ai/wandb-smle/artifact_workflow/overview), then click on the **Artifacts** tab.\n",
        "\n",
        "Navigating to the **Lineage** section in the tab will show the dependency graph formed by calling `run.use_artifact()` when an Artifact is an input to a run, and `run.log_artifact()` when an Artifact is output to a run. This helps visualize the relationship between different model versions and other objects like datasets and jobs in your project. Click [this](https://wandb.ai/wandb-smle/artifact_workflow/artifacts/dataset/preprocessed/v6/lineage) link to navigate to the project's lineage page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWklVJlPwdFy"
      },
      "source": [
        "## **Artifacts Time-to-live (TTL)**\n",
        "\n",
        "W&B Artifacts supports setting time-to-live policies on each version of an Artifact. The following examples show the use TTL policy in a common Artifact logging workflow. We'll cover:\n",
        "\n",
        "* Setting a TTL policy when creating an Artifact\n",
        "* Retroactively setting TTL for a specific Artifact aliases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqlxi0eQxT_-"
      },
      "source": [
        "## Setting TTL on New Artifacts\n",
        "Below we create two new Artifacts from the colab provided sample_data\n",
        "- mnist_test.csv\n",
        "- mnist_train_small.csv\n",
        "\n",
        "Upload them as artifacts files to artifact of type `mnist_dataset` and assign them a TTL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKZiPHW2xR90"
      },
      "outputs": [],
      "source": [
        "from datetime import timedelta\n",
        "\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                job_type=\"raw-data\")\n",
        "\n",
        "raw_mnist_train = wandb.Artifact(\n",
        "    f\"mnist_train_small_{YOUR_NAME}\",\n",
        "    type=\"mnist_dataset\",\n",
        "    description=\"Small MNIST Training Set\"\n",
        ")\n",
        "\n",
        "raw_mnist_train.add_file(\"sample_data/mnist_train_small.csv\")\n",
        "raw_mnist_train.ttl = timedelta(days=10)\n",
        "run.log_artifact(raw_mnist_train, aliases=[\"small\", \"mnist\", \"train\"])\n",
        "\n",
        "raw_mnist_test = wandb.Artifact(\n",
        "    f\"mnist_test_small_{YOUR_NAME}\",\n",
        "    type=\"mnist_dataset\",\n",
        "    description=\"Small MNIST Test Set\"\n",
        ")\n",
        "\n",
        "raw_mnist_test.add_file(\"sample_data/mnist_test.csv\")\n",
        "raw_mnist_test.ttl = timedelta(days=10)\n",
        "run.log_artifact(raw_mnist_test, aliases=[\"small\", \"mnist\", \"test\"])\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6P3Mu2T6qdq"
      },
      "source": [
        "Retroactively setting TTL for a specific Artifact aliases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq3g9aSfxqDM"
      },
      "outputs": [],
      "source": [
        "from datetime import timedelta\n",
        "\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                job_type=\"modify-ttl\")\n",
        "\n",
        "test_art = run.use_artifact(f\"{WANDB_ENTITY}/{WANDB_PROJECT}/mnist_test_small_{YOUR_NAME}:latest\")\n",
        "test_art.ttl = timedelta(days=365)  # Delete in a year\n",
        "test_art.save()\n",
        "\n",
        "train_art = run.use_artifact(f\"{WANDB_ENTITY}/{WANDB_PROJECT}/mnist_train_small_{YOUR_NAME}:latest\")\n",
        "train_art.ttl = timedelta(days=2)  # Delete in 2 days\n",
        "train_art.save()\n",
        "\n",
        "print(test_art.ttl)\n",
        "print(train_art.ttl)\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1Tx26C7yCMH"
      },
      "source": [
        "## Artifact References\n",
        "\n",
        "Artifacts currently support the following URI schemes:\n",
        "\n",
        "* **http(s)://:** A path to a file accessible over HTTP. The artifact will track checksums in the form of etags and size metadata if the HTTP server supports the ETag and Content-Length response headers.\n",
        "* **s3://:** A path to an object or object prefix in S3. The artifact will track checksums and versioning information (if the bucket has object versioning enabled) for the referenced objects. Object prefixes are expanded to include the objects under the prefix, default up to 100,000 objects.\n",
        "* **gs://:** A path to an object or object prefix in GCS. The artifact will track checksums and versioning information (if the bucket has object versioning enabled) for the referenced objects. Object prefixes are expanded to include the objects under the prefix, default up to 100,000 objects.\n",
        "\n",
        "See below for an example of reference local files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1r2g-m1zyBD9"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                job_type=\"upload-references\")\n",
        "artifact = wandb.Artifact(name=f\"local-file-references_{YOUR_NAME}\", type=\"reference-dataset\")\n",
        "artifact.add_reference(\"file:///content/sample_data\", checksum=True)\n",
        "run.log_artifact(artifact)\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSy2nDlgsriJ"
      },
      "source": [
        "## **Consideration Artifacts Caching and Staging Directories**\n",
        "\n",
        "By default Wandb artifacts requires 2x their size in storage availability due to the following workflows\n",
        "\n",
        "When youâ€™re uploading artifacts, wandb creates two copies locally, one in the .`cache directory` (used for faster retrievals to fetch files when `use_artifacts` is called instead of downloading them) and another in `.local/share/wandb/artifacts/staging` where files are duplicated in order to avoid issues if the files are being updated by th during uploads. The reason staging exists is to prevent modifications of files you added before you upload it. For example, if you call `artifact.add(file.txt)` then, you modify `file.txt` later in your code, W&B currently guarantees that we will upload the original content of `file.txt` when you added it.\n",
        "\n",
        "How to set where artifacts are cached?\n",
        "- Set the `WANDB_CACHE_DIR` env var to directory where users has read/write access\n",
        "\n",
        "How to set where artifacts are staged?\n",
        "- Set the `WANDB_DATA_DIR` env var to directory where users has read/write access\n",
        "\n",
        "Caching and Stagings can be controled by the user directly in `add_dir` and `add_file`methods by settings `skip_cache` and `policy`\n",
        "\n",
        "Here's a list with all the possible variations:\n",
        "\n",
        "- with `skip_cache=True` and `policy=mutable`, only staging are created.\n",
        "- with `skip_cache=False` and `policy=mutable`, staging and cache files are created, but staging files are deleted while caching.\n",
        "- with `skip_cache=True` and `policy=immutable`, neither staging nor cache files are created.\n",
        "- with `skip_cache=False` and `policy=immutable`, only cache files are created."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VglLxRq4ter3"
      },
      "source": [
        "# Artifact Training Pipeline Example\n",
        "\n",
        "The below pipeline includes:\n",
        "\n",
        "* Data Versioning: The Heart Disease dataset is split into training, validation, and test sets, each logged as a W&B artifact for easy tracking and reproducibility.\n",
        "\n",
        "* Model Training: A neural network is trained on the training set, with performance monitored on the validation set. The best model version is saved and versioned as a W&B artifact.\n",
        "\n",
        "* Model Evaluation: The best model is retrieved from W&B and evaluated on the test set to demonstrate how W&B ensures reproducibility and traceability throughout the ML lifecycle.\n",
        "\n",
        "* What It Showcases:\n",
        "How to use W&B for seamless data and model versioning.\n",
        "Best practices for tracking model performance during training and evaluation.\n",
        "Efficient and reproducible ML workflows in a production-ready environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB2-t6VsiLoS"
      },
      "source": [
        "### Data Preperation and uploading to wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei7EAuNBiAHT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Load the Heart Disease dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
        "           \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
        "data = pd.read_csv(url, header=None, names=columns)\n",
        "\n",
        "# Replace missing values ('?') with NaN and drop rows with NaN values\n",
        "data.replace('?', np.nan, inplace=True)\n",
        "data = data.dropna().astype(float)\n",
        "\n",
        "# Convert target variable: 0 = no heart disease, 1 = presence of heart disease\n",
        "data['target'] = (data['target'] > 0).astype(int)\n",
        "\n",
        "# Shuffle the dataset to ensure random distribution\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Perform a train/validation/test split (60/20/20)\n",
        "train_size = int(0.6 * len(data))\n",
        "val_size = int(0.2 * len(data))\n",
        "test_size = len(data) - train_size - val_size\n",
        "\n",
        "train_data = data[:train_size]\n",
        "val_data = data[train_size:train_size + val_size]\n",
        "test_data = data[train_size + val_size:]\n",
        "\n",
        "# Save the entire dataset as a CSV file\n",
        "data.to_csv(\"heart_disease_full_dataset.csv\", index=False)\n",
        "\n",
        "#simple function to save log dataset artifacts\n",
        "def save_and_log_dataset(data, filename, artifact_name, aliases):\n",
        "    # Save the dataset as a CSV file\n",
        "    data.to_csv(filename, index=False)\n",
        "\n",
        "    # Create and log the dataset artifact\n",
        "    dataset_artifact = wandb.Artifact(name=artifact_name, type='dataset')\n",
        "    dataset_artifact.add_file(filename)\n",
        "    wandb.log_artifact(dataset_artifact, aliases=aliases)\n",
        "\n",
        "#Upload data to wandb\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                group = YOUR_NAME,\n",
        "                job_type=\"heart-disease-data-uploads\",\n",
        "                name = f\"heart_disease_data_uploads_{YOUR_NAME}\",\n",
        "                tags = [\"data-upload\"]\n",
        "                )\n",
        "\n",
        "# Save and log the entire dataset\n",
        "save_and_log_dataset(data, \"heart_disease_full_dataset.csv\", f'heart_disease_full_dataset_{YOUR_NAME}', [\"initial_commit\", \"complete_dataset\"])\n",
        "\n",
        "# Save and log the training dataset\n",
        "save_and_log_dataset(train_data, \"heart_disease_train_dataset.csv\", f'heart_disease_train_dataset_{YOUR_NAME}', [\"initial_commit\", \"train_split\"])\n",
        "\n",
        "# Save and log the validation dataset\n",
        "save_and_log_dataset(val_data, \"heart_disease_val_dataset.csv\", f'heart_disease_validation_dataset_{YOUR_NAME}', [\"initial_commit\", \"validation_split\"])\n",
        "\n",
        "# Save and log the test dataset\n",
        "save_and_log_dataset(test_data, \"heart_disease_test_dataset.csv\", f'heart_disease_test_dataset_{YOUR_NAME}', [\"initial_commit\", \"test_split\"])\n",
        "\n",
        "\n",
        "#Log all dataset to W&B tables for visual analysis\n",
        "wandb.log({f\"train_data_table_{YOUR_NAME}\": wandb.Table(dataframe=train_data),\n",
        "           f\"test_data_table_{YOUR_NAME}\": wandb.Table(dataframe=test_data),\n",
        "           f\"validation_data_table_{YOUR_NAME}\": wandb.Table(dataframe=val_data)})\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk_MBJ3NiP5Y"
      },
      "source": [
        "### Model Training and Logging Artifacts\n",
        "In the below example we are\n",
        "\n",
        "1. Downloading the entire training and validation datasets using artifact.download() inside `load_data`\n",
        "2. Using the training data to execute a training run and saving best version of the model based on the condition if current `val_loss < best_performance`, a new model artifact will be created and logged with the `best` alias\n",
        "3. There after we will validate the `best` model  against the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur7ou-dNiTWd"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#simple function for loading artifacts from wandb\n",
        "def load_data(entity, project, artifact_name, your_name, split_name):\n",
        "\n",
        "    artifact_full_name = f'{entity}/{project}/{artifact_name}_{your_name}:latest'\n",
        "    artifact = wandb.use_artifact(artifact_full_name, type='dataset')\n",
        "    artifact_dir = artifact.download()#If you are using local version of artifact, you can simple utilize wandb.use_artifact() only instead of downloading to associated lineage to the artifact\n",
        "\n",
        "    data = pd.read_csv(f\"{artifact_dir}/heart_disease_{split_name}_dataset.csv\")\n",
        "\n",
        "    X = torch.tensor(data.drop(\"target\", axis=1).values, dtype=torch.float32)\n",
        "    y = torch.tensor(data[\"target\"].values, dtype=torch.float32)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Initialize training run\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                group = YOUR_NAME,\n",
        "                job_type=\"heart-disease-training\",\n",
        "                name = f\"heart_disease_training_validation_{YOUR_NAME}\"\n",
        "                )\n",
        "\n",
        "# Load training data\n",
        "X_train, y_train = load_data(WANDB_ENTITY, WANDB_PROJECT, 'heart_disease_train_dataset', YOUR_NAME, 'train')\n",
        "\n",
        "# Load validation data\n",
        "X_val, y_val = load_data(WANDB_ENTITY, WANDB_PROJECT, 'heart_disease_validation_dataset', YOUR_NAME, 'val')\n",
        "\n",
        "# Define a simple neural network model\n",
        "class HeartDiseaseModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(HeartDiseaseModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.fc4 = nn.Linear(32, 16)\n",
        "        self.fc5 = nn.Linear(16, 1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = torch.sigmoid(self.fc5(x))\n",
        "        return x\n",
        "\n",
        "model = HeartDiseaseModel(input_size=X_train.shape[1])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "best_performance = float('inf')\n",
        "version = 1\n",
        "\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train).squeeze()\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate and log training accuracy\n",
        "    predictions = (outputs >= 0.5).float()\n",
        "    train_accuracy = (predictions == y_train).float().mean().item()\n",
        "\n",
        "    wandb.log({\"train/epoch\": epoch, \"train/train_loss\": loss.item(), \"train/train_accuracy\": train_accuracy})\n",
        "\n",
        "    # Evaluate the model on validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val).squeeze()\n",
        "        val_loss = criterion(val_outputs, y_val).item()\n",
        "\n",
        "        # Calculate and log validation accuracy\n",
        "        val_predictions = (val_outputs >= 0.5).float()\n",
        "        val_accuracy = (val_predictions == y_val).float().mean().item()\n",
        "\n",
        "        wandb.log({\"val/val_loss\": val_loss, \"val/val_accuracy\": val_accuracy})\n",
        "\n",
        "        if val_loss < best_performance:\n",
        "            best_performance = val_loss\n",
        "            model_path = f\"heart_disease_model_v{version}.pth\"\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            artifact = wandb.Artifact(name=f'heart_disease_model_{YOUR_NAME}', type='model')\n",
        "            artifact.add_file(model_path)\n",
        "            wandb.log_artifact(artifact, aliases=[f\"v{version}\", \"best\"])\n",
        "            version += 1\n",
        "\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrhd_DGJigKl"
      },
      "source": [
        "### Evaluation Phase Using the Versioned Model Artifact\n",
        "We will now use the best model logged in our training loop and test against our test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcizXfhkifFA"
      },
      "outputs": [],
      "source": [
        "# Start a new W&B run for evaluation\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                group=YOUR_NAME,\n",
        "                job_type=\"heart-disease-testing\",\n",
        "                name=f\"heart_disease_testing_{YOUR_NAME}\"\n",
        "                )\n",
        "\n",
        "# Load test data\n",
        "X_test, y_test = load_data(WANDB_ENTITY, WANDB_PROJECT, 'heart_disease_test_dataset', YOUR_NAME, 'test')\n",
        "\n",
        "# Perform evaluation on the test set\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test).squeeze()\n",
        "    test_loss = criterion(outputs, y_test).item()\n",
        "    wandb.log({\"test/final_test_loss\": test_loss})\n",
        "\n",
        "wandb.finish()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL6GiFNWMhJG"
      },
      "source": [
        "## **Automations**\n",
        "\n",
        "Automations: Trigger automatic workflows via webhooks\n",
        "\n",
        "Create an automation to trigger workflow steps, such as automated model testing and deployment when a new version of an artifact is added to a registry or an alias is added to a linked version more on this [here](https://docs.wandb.ai/guides/model_registry/model-registry-automations)\n",
        "\n",
        "## **Example**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1yG_cO2chVw8vN0snZn5lcEs95Mx_LcwZ\" alt=\"Automation\" width=\"700\" height=\"400\">\n",
        "\n",
        "**Demo Workflow: Automations to open a \"dataset merge request\" issue in GH**\n",
        "\n",
        "**Context:** An approval workflow process is required (i.e like approving PRs in github) when a new dataset is logged to W&B\n",
        "\n",
        "**Workflow:**\n",
        "\n",
        "âž• Log new dataset version: A review requestor logs a new version to a dataset artifact inside a project, using wandb.log_artifact()\n",
        "\n",
        "ðŸš€ Automation to open GH issue: new dataset version (event) triggers GitHub Actions to create a GitHub Issue (action) for a reviewer to assess.\n",
        "\n",
        "âœ… Review, Approval & Linking: The reviewer can discuss the request with the requestor via the GH issue comment thread. Once approved, the reviewer links the logged dataset to the \"approved\" Dataset Registry in W&B (it is now an approved dataset version usable by others).\n",
        "\n",
        "ðŸ“¦ [Optional] Aggregation: new version in \"approved\" Dataset Registry triggers a second data merge workflow back on GH Actions, which may log a new version of the merged & approved dataset to a separate \"golden\" Dataset Registry\n",
        "\n",
        "Workflow Resources\n",
        "\n",
        "* github repo with GHA (yaml [link text](https://github.com/ijdoc/wandb-recipes/blob/aa787b0613089021b6e1567e15c78b775b5cfee0/.github/workflows/data-pr.yml))\n",
        "\n",
        "* W&B project - [PR creation automation](https://wandb.ai/smle-reg-team-1/demo-data-pr/automations) - view details through options menu to see payload example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHL7By8SiyXL"
      },
      "source": [
        "#Registry\n",
        "\n",
        "W&B Registry is a curated central repository that stores and provides versioning, aliases, lineage tracking, and governance of assets. Registry allows individuals and teams across the entire organization to share and collaboratively manage the lifecycle of all models, datasets and other artifacts. The registry can be access directly in SaaS by visiting https://wandb.ai/registry or on your private instance through `https://<host-url>/registry`\n",
        "\n",
        "The Registry has 2 core registeries, `Model` & `Datasets`, users can also create their own custom registeries, see docs above for more details.\n",
        "\n",
        "Some usecases for registry\n",
        "\n",
        "- A registry for non-model objects like forecasts or datasets that need to be surfaced as valid for production runs, even if models are published in different projects.\n",
        "- Comprehensive tracking beyond models and experiments, including datasets and other elements like prompts.\n",
        "- Enables the handoff of datasets from the â€˜dataset manufacturingâ€™ team to a centralized, discoverable list. Once promoted to the Registry, these datasets can be used by any team with the proper approvals for model training.\n",
        "- Allows sharing of models across the organization without granting access to the project/team where the model was developed.\n",
        "- The model registry supports teams in logging and consuming models with traceability, but larger organizations need enhanced collaboration and sharing across different roles.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=10qkxTZza7kg3j7cYESNfH5Lrw4y8Kv5V\" alt=\"Registry\" width=\"700\" height=\"300\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puhPb0X_jK6j"
      },
      "source": [
        "## Track and Publish dataset\n",
        "\n",
        "Within the core Dataset registry we will create a collection called `heart_disease` to promote our entire `heart disease dataset` to production.  A collection is a set of linked artifact versions in a registry.  \n",
        "\n",
        "To create a collection we need to do two things:\n",
        "1. Specify the collection and registry we want to link our artifact version to. To do this, we specify a \"target path\" for our artifact version.\n",
        "2. Use the `run.link_artifact` method and pass our artifact object and the target path.\n",
        "\n",
        "The target path consists of the name of the organization your team belongs to, the name of the registry, and the name of the collection. There are two ways to get the target path, [interatively with the W&B App UI](https://docs.wandb.ai/guides/registry/link_version#confirm-the-path-of-a-registry-in-the-wb-app-ui) or programmatically with the W&B Python SDK.\n",
        "\n",
        "For this example, we will programmatically create the collection target path:\n",
        "\n",
        "<!-- #### Interactively get target path of a collection\n",
        "\n",
        "1. Navigate to the Registry app at https://wandb.ai/registry/\n",
        "2. Click on the registry you want to link your artifact version to.\n",
        "3. At the top of the page, you will see an autogenerated code snippet. Copy the string next to the `target_path` parameter in `run.link_artifact()`. -->\n",
        "\n",
        "\n",
        "#### Programmatically make the collection target path\n",
        "\n",
        "The target path of a collectin consists of three parts:\n",
        "* The name of the organization\n",
        "* The name of the registry\n",
        "* The name of the collection within the registry\n",
        "\n",
        "If you know these three fields, you can create the full name yourself with string concatanation, f-strings, and so forth:\n",
        "```python\n",
        "target_path = f\"{ORG_NAME}/wandb-registry-{REGISTRY_NAME}/{COLLECTION_NAME}\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_pq9zxUivKO"
      },
      "outputs": [],
      "source": [
        "#uncomment the below when ready to run\n",
        "%%script false --no-raise-error\n",
        "\n",
        "ORG_NAME = \"moe_SAMSBNIZPAIR4\" #see top right of <host_url>/registry  for org name in example path run.link_artifact(artifact, f\"<org-name>/wandb-registry-model/{<insert_collection_name>}\")\n",
        "DATASET_REGISTRY_NAME = \"dataset\"\n",
        "DATASET_COLLECTION_NAME = \"heart_disease_production\"\n",
        "\n",
        "# Path to link the artifact to a collection\n",
        "dataset_target_path = f\"{ORG_NAME}/wandb-registry-{DATASET_REGISTRY_NAME}/{DATASET_COLLECTION_NAME}\"\n",
        "\n",
        "\n",
        "dataset_artifact_name = f\"{WANDB_ENTITY}/{WANDB_PROJECT}/heart_disease_train_dataset_{YOUR_NAME}:latest\"\n",
        "\n",
        "#Promote your <entity>/wandb-intro-session/heart_disease_full_dataset_<your_name>:v0 dataset to the registry\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                group=YOUR_NAME,\n",
        "                name=f\"dataset_registry_promotion_{YOUR_NAME}\"\n",
        "                )\n",
        "\n",
        "model_artifact = run.use_artifact(artifact_or_name=dataset_artifact_name, type=\"dataset\")\n",
        "\n",
        "run.link_artifact(artifact=model_artifact,\n",
        "                  target_path=dataset_target_path,\n",
        "                  aliases=[\"production\"],\n",
        "                  )\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_SBaXXgrS4s"
      },
      "source": [
        "## Track and Publish Model\n",
        "\n",
        "We will now follow a similar approach and instead promote our best model to the `Model` registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_IaVtb_rj_w"
      },
      "outputs": [],
      "source": [
        "#uncomment the below when ready to run\n",
        "%%script false --no-raise-error\n",
        "\n",
        "ORG_NAME = \"moe_SAMSBNIZPAIR4\"\n",
        "MODEL_REGISTRY_NAME = \"models\"\n",
        "MODEL_COLLECTION_NAME = \"heart_disease_production\"\n",
        "\n",
        "# Path to link the artifact to a collection\n",
        "dataset_target_path = f\"{ORG_NAME}/wandb-registry-{MODEL_REGISTRY_NAME}/{MODEL_COLLECTION_NAME}\"\n",
        "\n",
        "\n",
        "model_artifact_name = f\"{WANDB_ENTITY}/{WANDB_PROJECT}/heart_disease_model_{YOUR_NAME}:latest\"\n",
        "\n",
        "#Promote your <entity>/wandb-intro-session/heart_disease_model_<your_name>:v0 model to the registry\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                group=YOUR_NAME,\n",
        "                name=f\"model_registry_promotion_{YOUR_NAME}\"\n",
        "                )\n",
        "\n",
        "model_artifact = run.use_artifact(artifact_or_name=model_artifact_name, type=\"model\")\n",
        "\n",
        "run.link_artifact(artifact=model_artifact,\n",
        "                  target_path=dataset_target_path,\n",
        "                  aliases=[\"production\"],\n",
        "                  )\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anL24j_hWJ9Y"
      },
      "source": [
        "## Unlinking an Artifact from a collection\n",
        "\n",
        "There are time when you want to unlink and artifact from a collection. This can be done via two methods\n",
        "\n",
        "1. Directly in Association with a run\n",
        "2. Using the W&B API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryv5Q3FYWH1T"
      },
      "outputs": [],
      "source": [
        "#uncomment the below when ready to run\n",
        "%%script false --no-raise-error\n",
        "#Unlink the dataset artifact from registry via a run\n",
        "dataset_artifact_name = f\"{ORG_NAME}/wandb-registry-{DATASET_REGISTRY_NAME}/{DATASET_COLLECTION_NAME}:latest\"\n",
        "\n",
        "#Promote your <entity>/wandb-intro-session/heart_disease_full_dataset_<your_name>:v0 dataset to the registry\n",
        "run = wandb.init(entity=WANDB_ENTITY,\n",
        "                project=WANDB_PROJECT,\n",
        "                group=YOUR_NAME,\n",
        "                name=f\"dataset_regisbry_unlinking_{YOUR_NAME}\"\n",
        "                )\n",
        "\n",
        "model_artifact = run.use_artifact(artifact_or_name=dataset_artifact_name, type=\"dataset\")\n",
        "\n",
        "model_artifact.unlink()\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoR4wJ3-Wv5p"
      },
      "outputs": [],
      "source": [
        "#comment the below when ready to run\n",
        "%%script false --no-raise-error\n",
        "#Unlink the Model artifact from registry via the wandb api\n",
        "model_artifact_name = f\"{ORG_NAME}/wandb-registry-{MODEL_REGISTRY_NAME}/{MODEL_COLLECTION_NAME}:latest\"\n",
        "\n",
        "api = wandb.Api()\n",
        "artifact = api.artifact(model_artifact_name)\n",
        "artifact.unlink()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m1BgnmtqYju"
      },
      "source": [
        "# Next steps - Resources\n",
        "\n",
        "**Artifacts**\n",
        "1. [Artifacts Python reference documentation](https://docs.wandb.ai/ref/python/artifact): Deep dive into artifact parameters and advanced methods.\n",
        "2. [Lineage](https://docs.wandb.ai/guides/artifacts/explore-and-traverse-an-artifact-graph): View lineage graphs, which are automatically built when using W&B artifact system, providing an auditable visual overview of the relationships between specific artifact versions, datasets models and runs.\n",
        "3. [Artifact Automations](https://docs.wandb.ai/guides/artifacts/project-scoped-automations): Automatically run specific Weights & Biases jobs based on changes to your artifacts, such as automatically training a new model each time a new version of the training data is logged.\n",
        "4. [Reference Artifacts](https://docs.wandb.ai/guides/artifacts/track-external-files#download-a-reference-artifact): Track files saved outside the W&B server, like Amazon S3 buckets, GCS buckets, Azure blobs, and more.\n",
        "5. [Artifact TTL](https://docs.wandb.ai/guides/artifacts/ttl): Schedule when artifacts are deleted from W&B with W&B Artifact time-to-live (TTL) policy.\n",
        "\n",
        "**Registry**\n",
        "6. [Registry](https://docs.wandb.ai/guides/registry): Learn how to centralize your best artifact versions in a shared registry across your organization.\n",
        "7. [Registry Zoo Example](https://colab.research.google.com/drive/1RTUJgmJqqAElW0n7uy6NRQQQgjPHIcXr?usp=sharing): A secondary registry example representing the following scenario\n",
        "\n",
        "  Team 1 / Project 1:  Creating datasets, publishing them to datasets type registry. Then training a model against those datasets and promoting it to the model registry\n",
        "\n",
        "  Team 2 / Project 2:  Pull the dataset and model above to perform an inference from a completely different team. Would require multi-team membership to test\n",
        "\n",
        "## **Indepth Resources on CI/CD with webhook automations:**\n",
        "\n",
        "10. [Model CI/CD Course: Enterprise Model Management features](https://www.youtube.com/watch?v=VWdRQL0CsAk&list=PLD80i8An1OEGECFPgY-HPCNjXgGu-qGO6&index=1)\n",
        "\n",
        "11. [Model CI/CD with webhook automations on Weights & Biases Report](https://wandb.ai/wandb/wandb-model-cicd/reports/Model-CI-CD-with-W-B--Vmlldzo0OTcwNDQw#using-webhook-automations-in-weights-&-biases)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}